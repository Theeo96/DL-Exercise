{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\workspace\\DL-Exercise\\notebooks\\DL3_20241006\n",
            "c:\\workspace\\DL-Exercise\\notebooks\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "print(os.getcwd())\n",
        "current_dir = os.path.dirname(os.getcwd())\n",
        "print(current_dir)\n",
        "os.chdir(current_dir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch06.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.3022313506631646\n",
            "=== epoch:1, train acc:0.15, test acc:0.129 ===\n",
            "train loss:2.302001429039035\n",
            "train loss:2.301909334224525\n",
            "train loss:2.301738127388035\n",
            "train loss:2.3014175486926116\n",
            "train loss:2.3008670397064255\n",
            "train loss:2.301110290858141\n",
            "train loss:2.3008346697574606\n",
            "train loss:2.3007906239412153\n",
            "train loss:2.300292352238972\n",
            "train loss:2.2998516126328075\n",
            "train loss:2.2992727535058965\n",
            "train loss:2.29958364208738\n",
            "train loss:2.2991474797537688\n",
            "train loss:2.299123412939048\n",
            "train loss:2.2985242129472003\n",
            "train loss:2.298974686109814\n",
            "train loss:2.296830992353713\n",
            "train loss:2.2982395867354155\n",
            "train loss:2.2963203065450113\n",
            "train loss:2.294642276635326\n",
            "train loss:2.295014833385165\n",
            "train loss:2.2927400222533234\n",
            "train loss:2.2921885699059685\n",
            "train loss:2.292468454781651\n",
            "train loss:2.2909034157474455\n",
            "train loss:2.292036868705457\n",
            "train loss:2.2888154043184605\n",
            "train loss:2.287331477965001\n",
            "train loss:2.2869651479721815\n",
            "train loss:2.2859149880372573\n",
            "train loss:2.2836041546742765\n",
            "train loss:2.283548038182918\n",
            "train loss:2.281686822529394\n",
            "train loss:2.2820381219781534\n",
            "train loss:2.2777136497328603\n",
            "train loss:2.2793553162945654\n",
            "train loss:2.269845155232094\n",
            "train loss:2.2746059565256043\n",
            "train loss:2.273575193824721\n",
            "train loss:2.27233463413331\n",
            "train loss:2.273267164351358\n",
            "train loss:2.265541648744408\n",
            "train loss:2.264229535403304\n",
            "train loss:2.253718779177215\n",
            "train loss:2.2642689782518963\n",
            "train loss:2.2582108883294616\n",
            "train loss:2.2625750515112393\n",
            "train loss:2.2484593208382004\n",
            "train loss:2.2516125396768576\n",
            "train loss:2.2480770850174396\n",
            "=== epoch:2, train acc:0.625, test acc:0.588 ===\n",
            "train loss:2.2425336451057607\n",
            "train loss:2.232368105567926\n",
            "train loss:2.241978064874334\n",
            "train loss:2.236619431207273\n",
            "train loss:2.226945238922959\n",
            "train loss:2.232368926727413\n",
            "train loss:2.2190032687094496\n",
            "train loss:2.2173736422060966\n",
            "train loss:2.2173639110569554\n",
            "train loss:2.208028596139509\n",
            "train loss:2.2037462800469476\n",
            "train loss:2.194573331216663\n",
            "train loss:2.1875343269250695\n",
            "train loss:2.1898135279160003\n",
            "train loss:2.1803769924087435\n",
            "train loss:2.1690589147440478\n",
            "train loss:2.1536497254360967\n",
            "train loss:2.1964863545534965\n",
            "train loss:2.1698066712481836\n",
            "train loss:2.14846812845615\n",
            "train loss:2.171658360738385\n",
            "train loss:2.1394368116384155\n",
            "train loss:2.138997359779314\n",
            "train loss:2.1168259157087452\n",
            "train loss:2.1273041636153716\n",
            "train loss:2.113641444065582\n",
            "train loss:2.1044827715910377\n",
            "train loss:2.1173796165767054\n",
            "train loss:2.1047382634948932\n",
            "train loss:2.0485700092099233\n",
            "train loss:2.0756884325316842\n",
            "train loss:2.0680171729295336\n",
            "train loss:2.071368542107092\n",
            "train loss:2.0580085170080107\n",
            "train loss:2.0454550792823962\n",
            "train loss:2.0031513331509614\n",
            "train loss:1.9704787944517599\n",
            "train loss:2.0103130730989434\n",
            "train loss:1.9918011590532096\n",
            "train loss:1.9673281157182143\n",
            "train loss:1.9546213071253595\n",
            "train loss:1.974545705959529\n",
            "train loss:1.9423265206488616\n",
            "train loss:1.896969489521029\n",
            "train loss:1.9041507682154326\n",
            "train loss:1.8888190952098634\n",
            "train loss:1.8551443764322624\n",
            "train loss:1.8056960911402544\n",
            "train loss:1.7757504599079887\n",
            "train loss:1.880779930275579\n",
            "=== epoch:3, train acc:0.658, test acc:0.63 ===\n",
            "train loss:1.794329171882643\n",
            "train loss:1.7950065544257185\n",
            "train loss:1.7874792630402596\n",
            "train loss:1.8229315221033795\n",
            "train loss:1.7585690735489024\n",
            "train loss:1.791041226164112\n",
            "train loss:1.7758513912162752\n",
            "train loss:1.7883299574683043\n",
            "train loss:1.7174788820822284\n",
            "train loss:1.6991568763375116\n",
            "train loss:1.729600258330417\n",
            "train loss:1.7098410313239318\n",
            "train loss:1.6626716889567452\n",
            "train loss:1.6718233911705669\n",
            "train loss:1.6066199359390223\n",
            "train loss:1.5923317313964458\n",
            "train loss:1.5714864646345312\n",
            "train loss:1.5768755993854087\n",
            "train loss:1.5389479212285633\n",
            "train loss:1.476640142461289\n",
            "train loss:1.613159441942373\n",
            "train loss:1.4846913554390897\n",
            "train loss:1.5122594120366801\n",
            "train loss:1.5722736890786269\n",
            "train loss:1.608084001294864\n",
            "train loss:1.4200107280092527\n",
            "train loss:1.516213883237764\n",
            "train loss:1.4419687852567062\n",
            "train loss:1.3706411915054844\n",
            "train loss:1.4234120933390972\n",
            "train loss:1.4333810580259052\n",
            "train loss:1.4488479064535076\n",
            "train loss:1.3879665462286286\n",
            "train loss:1.3735313521645858\n",
            "train loss:1.400940237573608\n",
            "train loss:1.3331406353350377\n",
            "train loss:1.3175612518831086\n",
            "train loss:1.2459364649140874\n",
            "train loss:1.2946302486515469\n",
            "train loss:1.2937449083942\n",
            "train loss:1.2838989801471101\n",
            "train loss:1.2447321371250033\n",
            "train loss:1.2942242791095453\n",
            "train loss:1.296436810817273\n",
            "train loss:1.221723294975591\n",
            "train loss:1.1482774883535485\n",
            "train loss:1.1173544995032123\n",
            "train loss:1.3285658503934625\n",
            "train loss:1.180887922053818\n",
            "train loss:1.2094581858977604\n",
            "=== epoch:4, train acc:0.742, test acc:0.695 ===\n",
            "train loss:1.1211402483222657\n",
            "train loss:1.0674200221020143\n",
            "train loss:1.1640599319529963\n",
            "train loss:1.1020348895492804\n",
            "train loss:1.1557141873029495\n",
            "train loss:1.0252284544713763\n",
            "train loss:1.0959964617752116\n",
            "train loss:1.116224576602434\n",
            "train loss:1.078688973930083\n",
            "train loss:1.048362450021323\n",
            "train loss:1.108289959422972\n",
            "train loss:1.027317230248852\n",
            "train loss:1.1176686504699547\n",
            "train loss:0.9824858140054993\n",
            "train loss:1.0604639642932423\n",
            "train loss:1.0280921353217025\n",
            "train loss:1.038055749642769\n",
            "train loss:1.0784719635050006\n",
            "train loss:1.0013103993087882\n",
            "train loss:0.940544744321434\n",
            "train loss:0.9207133872221293\n",
            "train loss:1.05540336380122\n",
            "train loss:1.0230868463919005\n",
            "train loss:1.0522980307614989\n",
            "train loss:0.8923783055434461\n",
            "train loss:0.8916426962207328\n",
            "train loss:1.0494898660344096\n",
            "train loss:1.0074235026345661\n",
            "train loss:0.9595006980760584\n",
            "train loss:0.9545947109750597\n",
            "train loss:0.8891458471614422\n",
            "train loss:0.924696101187157\n",
            "train loss:0.940123076323762\n",
            "train loss:1.0797429521442086\n",
            "train loss:0.8823235771020186\n",
            "train loss:0.8380137014641533\n",
            "train loss:0.9235189753622123\n",
            "train loss:0.8830546481159878\n",
            "train loss:0.8595659971377462\n",
            "train loss:0.7691351462036589\n",
            "train loss:0.8061384532481862\n",
            "train loss:0.9395206472774176\n",
            "train loss:0.7486655436863758\n",
            "train loss:0.9275770204403436\n",
            "train loss:0.8490747891287553\n",
            "train loss:0.7937196190645092\n",
            "train loss:0.757070139023571\n",
            "train loss:0.8000675834027656\n",
            "train loss:0.8167216074971618\n",
            "train loss:0.814056399255607\n",
            "=== epoch:5, train acc:0.777, test acc:0.75 ===\n",
            "train loss:0.7541801200503003\n",
            "train loss:0.889769876268971\n",
            "train loss:0.9185550621827774\n",
            "train loss:0.8077271883292869\n",
            "train loss:0.6675429463755609\n",
            "train loss:0.7858078852254892\n",
            "train loss:0.8856979971311202\n",
            "train loss:0.705885892690365\n",
            "train loss:0.784826972408984\n",
            "train loss:0.7245262557675758\n",
            "train loss:0.6620166793247226\n",
            "train loss:0.690811103482842\n",
            "train loss:0.8200036969309046\n",
            "train loss:0.8145886737615904\n",
            "train loss:0.6603363924107607\n",
            "train loss:0.8268482172448915\n",
            "train loss:0.6928344768238928\n",
            "train loss:0.8159587851533178\n",
            "train loss:0.7571863129742821\n",
            "train loss:0.807515022303695\n",
            "train loss:0.696793888540308\n",
            "train loss:0.7939221679426997\n",
            "train loss:0.7329608027723457\n",
            "train loss:0.6959816211393784\n",
            "train loss:0.6712523538856626\n",
            "train loss:0.8541018119282713\n",
            "train loss:0.6958640102179338\n",
            "train loss:0.6791102036069965\n",
            "train loss:0.6154662497621631\n",
            "train loss:0.6345825621881936\n",
            "train loss:0.7075995504157025\n",
            "train loss:0.7515752740122857\n",
            "train loss:0.778295865589789\n",
            "train loss:0.7518095459207416\n",
            "train loss:0.7809231618574861\n",
            "train loss:0.6167178624599814\n",
            "train loss:0.6622456323239604\n",
            "train loss:0.588583791408945\n",
            "train loss:0.6940469010567797\n",
            "train loss:0.6937839645058064\n",
            "train loss:0.6837608512044585\n",
            "train loss:0.5997454844632155\n",
            "train loss:0.5246498912592318\n",
            "train loss:0.7278401867197486\n",
            "train loss:0.7029003760143364\n",
            "train loss:0.7477250175008465\n",
            "train loss:0.7423376769147538\n",
            "train loss:0.6564068265192159\n",
            "train loss:0.6079426471944268\n",
            "train loss:0.6602550699465869\n",
            "=== epoch:6, train acc:0.805, test acc:0.792 ===\n",
            "train loss:0.6442867187016184\n",
            "train loss:0.640140848683573\n",
            "train loss:0.5183005452668182\n",
            "train loss:0.7459483537018606\n",
            "train loss:0.7341974286547138\n",
            "train loss:0.7607440220943776\n",
            "train loss:0.6006539212107465\n",
            "train loss:0.646969530270721\n",
            "train loss:0.7539751709627639\n",
            "train loss:0.5435191691184806\n",
            "train loss:0.5915766178851469\n",
            "train loss:0.6371904844842367\n",
            "train loss:0.5440246762589889\n",
            "train loss:0.5849394753737994\n",
            "train loss:0.6155786751757701\n",
            "train loss:0.6801229588160583\n",
            "train loss:0.699486186630868\n",
            "train loss:0.6775305568070125\n",
            "train loss:0.5962248234067675\n",
            "train loss:0.6415857997492759\n",
            "train loss:0.5759524909289225\n",
            "train loss:0.615043049035505\n",
            "train loss:0.7386277285031995\n",
            "train loss:0.6306570679717738\n",
            "train loss:0.545405074770828\n",
            "train loss:0.7569559739856555\n",
            "train loss:0.6529631726046334\n",
            "train loss:0.6133679517523263\n",
            "train loss:0.48321911214206537\n",
            "train loss:0.5544218160648478\n",
            "train loss:0.6394459261221149\n",
            "train loss:0.7079450248330178\n",
            "train loss:0.6483742258616839\n",
            "train loss:0.5705843190712815\n",
            "train loss:0.5819385571871928\n",
            "train loss:0.7936157829019241\n",
            "train loss:0.6750772378813648\n",
            "train loss:0.5660989478917191\n",
            "train loss:0.6674569754017251\n",
            "train loss:0.557318284705216\n",
            "train loss:0.5119611004445306\n",
            "train loss:0.5955919739933129\n",
            "train loss:0.561262166764124\n",
            "train loss:0.5807968088982367\n",
            "train loss:0.580698405795796\n",
            "train loss:0.6221658048247647\n",
            "train loss:0.539130124057593\n",
            "train loss:0.5800716114203716\n",
            "train loss:0.56968779186851\n",
            "train loss:0.5623283353367274\n",
            "=== epoch:7, train acc:0.832, test acc:0.814 ===\n",
            "train loss:0.5486610613127201\n",
            "train loss:0.48739867790887226\n",
            "train loss:0.608459334623287\n",
            "train loss:0.5619770088861719\n",
            "train loss:0.5171412196547278\n",
            "train loss:0.49077256006719183\n",
            "train loss:0.47512731219417254\n",
            "train loss:0.5594627851790759\n",
            "train loss:0.6906647191414134\n",
            "train loss:0.5319363181245033\n",
            "train loss:0.5656165778751466\n",
            "train loss:0.5001397663150977\n",
            "train loss:0.6593207683237047\n",
            "train loss:0.48088500464395645\n",
            "train loss:0.5598188420079694\n",
            "train loss:0.5753532950019382\n",
            "train loss:0.5973221656165412\n",
            "train loss:0.4794305442775206\n",
            "train loss:0.5261488921522658\n",
            "train loss:0.4704900957812363\n",
            "train loss:0.6718748736544262\n",
            "train loss:0.5188952364115569\n",
            "train loss:0.5712435696432612\n",
            "train loss:0.6093225543742543\n",
            "train loss:0.6696310472581398\n",
            "train loss:0.48438557027955637\n",
            "train loss:0.5514414051033915\n",
            "train loss:0.6192277141250108\n",
            "train loss:0.3229514768515639\n",
            "train loss:0.4542420225286402\n",
            "train loss:0.49887318249135765\n",
            "train loss:0.6079336861694589\n",
            "train loss:0.5384761390615046\n",
            "train loss:0.6915734879273765\n",
            "train loss:0.527971076504422\n",
            "train loss:0.6128243263733484\n",
            "train loss:0.5031600311003304\n",
            "train loss:0.3878884789398447\n",
            "train loss:0.4518601319587306\n",
            "train loss:0.3647879010551653\n",
            "train loss:0.5439511863474091\n",
            "train loss:0.4984977964199038\n",
            "train loss:0.3851193035277859\n",
            "train loss:0.5833066098778519\n",
            "train loss:0.39624777448641113\n",
            "train loss:0.49183874726321825\n",
            "train loss:0.5588382894207966\n",
            "train loss:0.377018221859114\n",
            "train loss:0.7876534525173292\n",
            "train loss:0.4433023185495892\n",
            "=== epoch:8, train acc:0.846, test acc:0.832 ===\n",
            "train loss:0.4118557518822555\n",
            "train loss:0.5243094498059411\n",
            "train loss:0.49380892800345255\n",
            "train loss:0.5190471529779103\n",
            "train loss:0.48237688966841297\n",
            "train loss:0.5449189191954753\n",
            "train loss:0.46277753550886075\n",
            "train loss:0.3908437712638752\n",
            "train loss:0.5570970145660619\n",
            "train loss:0.6213501217598767\n",
            "train loss:0.6008913965946815\n",
            "train loss:0.658443289390918\n",
            "train loss:0.4763864067587318\n",
            "train loss:0.48869296285297914\n",
            "train loss:0.48738835068366315\n",
            "train loss:0.5356114533818365\n",
            "train loss:0.4737690625148376\n",
            "train loss:0.4084078074320833\n",
            "train loss:0.48798201948287045\n",
            "train loss:0.4610805561912245\n",
            "train loss:0.5921937550665509\n",
            "train loss:0.4065087686190518\n",
            "train loss:0.3948056766239776\n",
            "train loss:0.49664522106122233\n",
            "train loss:0.5568798804926895\n",
            "train loss:0.4985198277505352\n",
            "train loss:0.3808940114939594\n",
            "train loss:0.48994579436810193\n",
            "train loss:0.4426471987311904\n",
            "train loss:0.553481402685689\n",
            "train loss:0.3808357855723483\n",
            "train loss:0.4480022169944148\n",
            "train loss:0.41623181801175363\n",
            "train loss:0.5270534794598365\n",
            "train loss:0.44714795434303334\n",
            "train loss:0.4321391046985212\n",
            "train loss:0.3465020510409262\n",
            "train loss:0.37524266272126666\n",
            "train loss:0.5245359729219717\n",
            "train loss:0.5605212433885076\n",
            "train loss:0.3667620277165789\n",
            "train loss:0.6775635978400654\n",
            "train loss:0.5047539850272548\n",
            "train loss:0.3731884557655085\n",
            "train loss:0.47714433199563316\n",
            "train loss:0.3890748849739398\n",
            "train loss:0.4812591088162767\n",
            "train loss:0.6447144086694371\n",
            "train loss:0.5260204282311921\n",
            "train loss:0.538530519641421\n",
            "=== epoch:9, train acc:0.853, test acc:0.84 ===\n",
            "train loss:0.6326890782131143\n",
            "train loss:0.4571681756809396\n",
            "train loss:0.4439120143085427\n",
            "train loss:0.4927877380267312\n",
            "train loss:0.3814054958464039\n",
            "train loss:0.4787843167871689\n",
            "train loss:0.413966860434485\n",
            "train loss:0.40889967385649\n",
            "train loss:0.6107470550289658\n",
            "train loss:0.5767867734234647\n",
            "train loss:0.4135350843407387\n",
            "train loss:0.5380192810219079\n",
            "train loss:0.3457950687051508\n",
            "train loss:0.37926090549728597\n",
            "train loss:0.4046412946475106\n",
            "train loss:0.3873134671794967\n",
            "train loss:0.4824725288436072\n",
            "train loss:0.383514179400922\n",
            "train loss:0.5436753538581731\n",
            "train loss:0.3979083341208214\n",
            "train loss:0.38279090328057075\n",
            "train loss:0.4029309105316522\n",
            "train loss:0.5427321787379565\n",
            "train loss:0.4336712463214031\n",
            "train loss:0.4139373497410475\n",
            "train loss:0.4967980190046294\n",
            "train loss:0.49713137774116434\n",
            "train loss:0.46403642096037984\n",
            "train loss:0.38058155385026393\n",
            "train loss:0.5029575224049588\n",
            "train loss:0.4348514199413805\n",
            "train loss:0.47136902639040634\n",
            "train loss:0.4015934428669855\n",
            "train loss:0.4301673193280065\n",
            "train loss:0.47841981703438896\n",
            "train loss:0.5496585601171666\n",
            "train loss:0.4064963741990551\n",
            "train loss:0.4763365618254543\n",
            "train loss:0.41772179480522936\n",
            "train loss:0.4630695212913632\n",
            "train loss:0.5000699562455566\n",
            "train loss:0.46383871132080884\n",
            "train loss:0.5638350508191596\n",
            "train loss:0.3076638161134228\n",
            "train loss:0.43630393114796656\n",
            "train loss:0.40316814809513374\n",
            "train loss:0.4426375228563106\n",
            "train loss:0.47390286138648163\n",
            "train loss:0.5172664589657144\n",
            "train loss:0.37513388385330365\n",
            "=== epoch:10, train acc:0.858, test acc:0.846 ===\n",
            "train loss:0.4268218089953756\n",
            "train loss:0.3652702000381136\n",
            "train loss:0.391871140567379\n",
            "train loss:0.4549714061798844\n",
            "train loss:0.29770569236723615\n",
            "train loss:0.4501921848887869\n",
            "train loss:0.45759090671272945\n",
            "train loss:0.29984065398295423\n",
            "train loss:0.5377088832573871\n",
            "train loss:0.45627083310978395\n",
            "train loss:0.3725130719045381\n",
            "train loss:0.5515042661123721\n",
            "train loss:0.4330110292908593\n",
            "train loss:0.4120871184094932\n",
            "train loss:0.42028012575844687\n",
            "train loss:0.3457920413253481\n",
            "train loss:0.3624348756140366\n",
            "train loss:0.4187787802514127\n",
            "train loss:0.42317105565342494\n",
            "train loss:0.3261669832232099\n",
            "train loss:0.4198413709229603\n",
            "train loss:0.3054326348043471\n",
            "train loss:0.5777548645996832\n",
            "train loss:0.23149709614132621\n",
            "train loss:0.43592346923586256\n",
            "train loss:0.3608097489882451\n",
            "train loss:0.4006051404024612\n",
            "train loss:0.30060336541950666\n",
            "train loss:0.502613166759823\n",
            "train loss:0.4224975922983163\n",
            "train loss:0.397409038048478\n",
            "train loss:0.3905009167210125\n",
            "train loss:0.3624141219956644\n",
            "train loss:0.5319463655091069\n",
            "train loss:0.3948815074124844\n",
            "train loss:0.47985475995413374\n",
            "train loss:0.5133126778496377\n",
            "train loss:0.41334131636548066\n",
            "train loss:0.47925001171272996\n",
            "train loss:0.28098699833872487\n",
            "train loss:0.2949725717834442\n",
            "train loss:0.27226229647670275\n",
            "train loss:0.5127952170996711\n",
            "train loss:0.4760631258437051\n",
            "train loss:0.35901850867668883\n",
            "train loss:0.4368135717454253\n",
            "train loss:0.3505334365992994\n",
            "train loss:0.48745794538437415\n",
            "train loss:0.5720179268185493\n",
            "train loss:0.2684525344125652\n",
            "=== epoch:11, train acc:0.863, test acc:0.857 ===\n",
            "train loss:0.35343230076373894\n",
            "train loss:0.47707972712502206\n",
            "train loss:0.4444312557043929\n",
            "train loss:0.29890154107029987\n",
            "train loss:0.3761178353536714\n",
            "train loss:0.2263780757250202\n",
            "train loss:0.4173061833950551\n",
            "train loss:0.4181400155091821\n",
            "train loss:0.3617553557748735\n",
            "train loss:0.31809619957978774\n",
            "train loss:0.46387132579619644\n",
            "train loss:0.3929083311450709\n",
            "train loss:0.3615821709884384\n",
            "train loss:0.4137233076273807\n",
            "train loss:0.5315784398509967\n",
            "train loss:0.31085946810022436\n",
            "train loss:0.3578631155737202\n",
            "train loss:0.43865974972083854\n",
            "train loss:0.5168763502497248\n",
            "train loss:0.4161856097048728\n",
            "train loss:0.44613756418438205\n",
            "train loss:0.3111170623933807\n",
            "train loss:0.37512418454759633\n",
            "train loss:0.3877517508983443\n",
            "train loss:0.3324029872276424\n",
            "train loss:0.24837379669410847\n",
            "train loss:0.4470836730609454\n",
            "train loss:0.39572943749140016\n",
            "train loss:0.35903288208177153\n",
            "train loss:0.43364454579174283\n",
            "train loss:0.39940333923358295\n",
            "train loss:0.32636037289530084\n",
            "train loss:0.2468193342080386\n",
            "train loss:0.344393795667762\n",
            "train loss:0.35902581830433516\n",
            "train loss:0.37874883518310043\n",
            "train loss:0.32173079797505694\n",
            "train loss:0.38651642134204905\n",
            "train loss:0.4529417006301641\n",
            "train loss:0.47555443638554906\n",
            "train loss:0.28475704095249893\n",
            "train loss:0.3033689070018272\n",
            "train loss:0.43265595909616006\n",
            "train loss:0.5055776208360946\n",
            "train loss:0.257881442808771\n",
            "train loss:0.39341375019232216\n",
            "train loss:0.592240402218265\n",
            "train loss:0.42064193253407794\n",
            "train loss:0.40852730735546977\n",
            "train loss:0.3475205853503406\n",
            "=== epoch:12, train acc:0.869, test acc:0.853 ===\n",
            "train loss:0.34493278158507634\n",
            "train loss:0.45419072062291327\n",
            "train loss:0.3463788956919293\n",
            "train loss:0.3095933306457707\n",
            "train loss:0.3798367693254923\n",
            "train loss:0.617862936943823\n",
            "train loss:0.3999013807795611\n",
            "train loss:0.2769313828358992\n",
            "train loss:0.3589926812042272\n",
            "train loss:0.3905959842094623\n",
            "train loss:0.4192561898334425\n",
            "train loss:0.39571433837775766\n",
            "train loss:0.167499672736741\n",
            "train loss:0.3685533502529717\n",
            "train loss:0.3736060881136616\n",
            "train loss:0.3944749416793062\n",
            "train loss:0.4536229738088444\n",
            "train loss:0.4244433020810868\n",
            "train loss:0.5333820851412407\n",
            "train loss:0.3160069528730766\n",
            "train loss:0.38291263357178723\n",
            "train loss:0.4635338823102863\n",
            "train loss:0.3533476582700439\n",
            "train loss:0.46796264916786584\n",
            "train loss:0.3267015298636357\n",
            "train loss:0.4268874680139977\n",
            "train loss:0.33290817410801177\n",
            "train loss:0.32906709979988896\n",
            "train loss:0.282424504441563\n",
            "train loss:0.27498413968554886\n",
            "train loss:0.3535386865927094\n",
            "train loss:0.4660960539881991\n",
            "train loss:0.4919766979304077\n",
            "train loss:0.5204961210060002\n",
            "train loss:0.33774954603807905\n",
            "train loss:0.42364228434445406\n",
            "train loss:0.4274402833921933\n",
            "train loss:0.3532124568725732\n",
            "train loss:0.38442790504512364\n",
            "train loss:0.5245359812359509\n",
            "train loss:0.25837443044642355\n",
            "train loss:0.3018249089335092\n",
            "train loss:0.2807345068809022\n",
            "train loss:0.3213311565708114\n",
            "train loss:0.4548300631018233\n",
            "train loss:0.39252505297202533\n",
            "train loss:0.34748777452038665\n",
            "train loss:0.4021331267906615\n",
            "train loss:0.24268198655908285\n",
            "train loss:0.4541428808489667\n",
            "=== epoch:13, train acc:0.868, test acc:0.866 ===\n",
            "train loss:0.5176474203696273\n",
            "train loss:0.4668431796949157\n",
            "train loss:0.43451193951964784\n",
            "train loss:0.29483814842691797\n",
            "train loss:0.4364450710525242\n",
            "train loss:0.30195155528911344\n",
            "train loss:0.3989043836993428\n",
            "train loss:0.4058235562506116\n",
            "train loss:0.3166736987725387\n",
            "train loss:0.33297734762521536\n",
            "train loss:0.2749519637454156\n",
            "train loss:0.3109871393607542\n",
            "train loss:0.2560739757287988\n",
            "train loss:0.42471778486947265\n",
            "train loss:0.26246600912388296\n",
            "train loss:0.3130388105588448\n",
            "train loss:0.43643682431268216\n",
            "train loss:0.43880921436084397\n",
            "train loss:0.331711113407373\n",
            "train loss:0.3285441979023651\n",
            "train loss:0.5249747503906459\n",
            "train loss:0.4186978614680556\n",
            "train loss:0.3353377619062872\n",
            "train loss:0.42181813091139153\n",
            "train loss:0.4378817134018461\n",
            "train loss:0.6075867591749747\n",
            "train loss:0.4278514833913553\n",
            "train loss:0.29010728673881353\n",
            "train loss:0.4464064215013284\n",
            "train loss:0.27421834488280966\n",
            "train loss:0.4193012576341361\n",
            "train loss:0.44820714441185994\n",
            "train loss:0.3742714677554776\n",
            "train loss:0.44239218574370814\n",
            "train loss:0.44358735518883\n",
            "train loss:0.22882600940177236\n",
            "train loss:0.4715436046955758\n",
            "train loss:0.2700715389487771\n",
            "train loss:0.1931181020087576\n",
            "train loss:0.3799802834381859\n",
            "train loss:0.4204020872126014\n",
            "train loss:0.369334470883058\n",
            "train loss:0.2890495440401923\n",
            "train loss:0.31753856956412435\n",
            "train loss:0.33746638330443823\n",
            "train loss:0.32106119478548373\n",
            "train loss:0.3529753348812794\n",
            "train loss:0.3487143655957405\n",
            "train loss:0.34934661672082135\n",
            "train loss:0.33824085611221144\n",
            "=== epoch:14, train acc:0.879, test acc:0.865 ===\n",
            "train loss:0.26713753108450716\n",
            "train loss:0.5125686281503896\n",
            "train loss:0.2249733460862166\n",
            "train loss:0.5203081033730682\n",
            "train loss:0.3738192129966982\n",
            "train loss:0.4061870041548969\n",
            "train loss:0.33900905506329226\n",
            "train loss:0.3035239890290375\n",
            "train loss:0.2712574475307528\n",
            "train loss:0.35402973241051733\n",
            "train loss:0.3156982360109132\n",
            "train loss:0.39531764883142684\n",
            "train loss:0.3102809588578076\n",
            "train loss:0.6005007208048029\n",
            "train loss:0.2939495629975454\n",
            "train loss:0.23992990471944928\n",
            "train loss:0.36676784716133676\n",
            "train loss:0.3560917511385172\n",
            "train loss:0.3838549833206659\n",
            "train loss:0.4113243074038399\n",
            "train loss:0.36540503173061173\n",
            "train loss:0.3421437479479954\n",
            "train loss:0.38885257893944614\n",
            "train loss:0.3735264431722122\n",
            "train loss:0.3480946545456518\n",
            "train loss:0.395676133059362\n",
            "train loss:0.3126599354269709\n",
            "train loss:0.268671681182787\n",
            "train loss:0.35674301898816163\n",
            "train loss:0.33688418417726007\n",
            "train loss:0.2922897477891885\n",
            "train loss:0.21022712977462196\n",
            "train loss:0.3274306560340909\n",
            "train loss:0.5125342710938939\n",
            "train loss:0.46846594015098847\n",
            "train loss:0.6648892483844403\n",
            "train loss:0.2230777408786138\n",
            "train loss:0.2466155078556827\n",
            "train loss:0.2812599369445573\n",
            "train loss:0.3663532029155373\n",
            "train loss:0.37743288416658116\n",
            "train loss:0.35860700557905295\n",
            "train loss:0.2482881959568029\n",
            "train loss:0.31588643621675505\n",
            "train loss:0.43212184862622693\n",
            "train loss:0.34277740488819874\n",
            "train loss:0.3259953273736451\n",
            "train loss:0.3293435229618482\n",
            "train loss:0.4182928662416148\n",
            "train loss:0.2815908216983919\n",
            "=== epoch:15, train acc:0.884, test acc:0.865 ===\n",
            "train loss:0.3531726522164525\n",
            "train loss:0.47371564162385\n",
            "train loss:0.44424608095373147\n",
            "train loss:0.33697387448005856\n",
            "train loss:0.23946986982579738\n",
            "train loss:0.241561361634412\n",
            "train loss:0.4202208189615778\n",
            "train loss:0.4309937739892127\n",
            "train loss:0.2385559668946285\n",
            "train loss:0.40609099125310066\n",
            "train loss:0.2300436973400324\n",
            "train loss:0.47238181602239665\n",
            "train loss:0.2849401880722141\n",
            "train loss:0.3024144787192043\n",
            "train loss:0.37335744766528167\n",
            "train loss:0.32176587946782276\n",
            "train loss:0.34025667131350734\n",
            "train loss:0.30981441197418663\n",
            "train loss:0.3402030476865858\n",
            "train loss:0.3405524519236494\n",
            "train loss:0.37156714911366717\n",
            "train loss:0.31904535523899924\n",
            "train loss:0.24663879842472458\n",
            "train loss:0.3069642383748251\n",
            "train loss:0.3915191088756289\n",
            "train loss:0.43885598190975084\n",
            "train loss:0.31441636662041056\n",
            "train loss:0.3429017781597146\n",
            "train loss:0.46842710120684655\n",
            "train loss:0.3994759163378566\n",
            "train loss:0.17055809923479617\n",
            "train loss:0.3577145310963181\n",
            "train loss:0.2549263157666924\n",
            "train loss:0.30873043852756504\n",
            "train loss:0.33062309035281096\n",
            "train loss:0.3109893723478972\n",
            "train loss:0.4303117679622368\n",
            "train loss:0.2995596852891444\n",
            "train loss:0.31536042185093405\n",
            "train loss:0.32200187932167235\n",
            "train loss:0.5247727520640666\n",
            "train loss:0.4393810029083673\n",
            "train loss:0.36071912971249803\n",
            "train loss:0.24279102486432008\n",
            "train loss:0.33859721723994446\n",
            "train loss:0.4753540736431275\n",
            "train loss:0.3001014040996763\n",
            "train loss:0.4401920053636287\n",
            "train loss:0.3292714678356276\n",
            "train loss:0.38833314159188886\n",
            "=== epoch:16, train acc:0.879, test acc:0.87 ===\n",
            "train loss:0.3114644364142634\n",
            "train loss:0.2650647999166889\n",
            "train loss:0.27315807359668787\n",
            "train loss:0.21283551129918238\n",
            "train loss:0.3237027968375623\n",
            "train loss:0.19925088986251388\n",
            "train loss:0.24461416483525394\n",
            "train loss:0.2362319389047832\n",
            "train loss:0.5462444048890851\n",
            "train loss:0.2733295099039104\n",
            "train loss:0.36737167626527445\n",
            "train loss:0.5236376894520403\n",
            "train loss:0.2794014806075201\n",
            "train loss:0.3753589393481729\n",
            "train loss:0.47512015192478757\n",
            "train loss:0.2470448375187978\n",
            "train loss:0.3263787652474976\n",
            "train loss:0.30483959578098224\n",
            "train loss:0.3213023672547372\n",
            "train loss:0.29981229873231535\n",
            "train loss:0.4977309990528556\n",
            "train loss:0.5389812995203966\n",
            "train loss:0.3317795662201652\n",
            "train loss:0.44254182675327125\n",
            "train loss:0.2910661542122814\n",
            "train loss:0.28419836164013046\n",
            "train loss:0.27877006640978574\n",
            "train loss:0.32842677630286204\n",
            "train loss:0.48449945682106516\n",
            "train loss:0.21624097148532642\n",
            "train loss:0.30067158320159576\n",
            "train loss:0.2986406079815141\n",
            "train loss:0.32660340247631614\n",
            "train loss:0.25996446695758824\n",
            "train loss:0.3698737057480204\n",
            "train loss:0.2947440921639666\n",
            "train loss:0.2590769417843366\n",
            "train loss:0.2734691190763734\n",
            "train loss:0.43189605353965965\n",
            "train loss:0.2571449377896088\n",
            "train loss:0.3015941201219074\n",
            "train loss:0.21392859309168302\n",
            "train loss:0.2304027889479513\n",
            "train loss:0.21066250669156214\n",
            "train loss:0.292323347483391\n",
            "train loss:0.507977454005387\n",
            "train loss:0.2511856590446845\n",
            "train loss:0.2896884512177557\n",
            "train loss:0.30342030935059694\n",
            "train loss:0.17995184603338665\n",
            "=== epoch:17, train acc:0.875, test acc:0.876 ===\n",
            "train loss:0.33123535527559844\n",
            "train loss:0.4040630541073449\n",
            "train loss:0.2669491661815093\n",
            "train loss:0.2074907487693002\n",
            "train loss:0.23877269963221104\n",
            "train loss:0.3017787404348241\n",
            "train loss:0.2840811585643297\n",
            "train loss:0.7357005697205227\n",
            "train loss:0.2758729522257627\n",
            "train loss:0.421920987722242\n",
            "train loss:0.1898789098002414\n",
            "train loss:0.4783930648034571\n",
            "train loss:0.3727418525004858\n",
            "train loss:0.39904415134443705\n",
            "train loss:0.2427884716819576\n",
            "train loss:0.2922370004954255\n",
            "train loss:0.2788585556257724\n",
            "train loss:0.25680931158190357\n",
            "train loss:0.2602596731752412\n",
            "train loss:0.33178860296353746\n",
            "train loss:0.37637419992986987\n",
            "train loss:0.2645752125626586\n",
            "train loss:0.24568369291559147\n",
            "train loss:0.29000465855007074\n",
            "train loss:0.31135349927614236\n",
            "train loss:0.3058738761992514\n",
            "train loss:0.3122128764775115\n",
            "train loss:0.4361453262724162\n",
            "train loss:0.26648883868804735\n",
            "train loss:0.2329943213502098\n",
            "train loss:0.24695802884909632\n",
            "train loss:0.3097170048392675\n",
            "train loss:0.26104018570955706\n",
            "train loss:0.252192846834899\n",
            "train loss:0.47551728429679424\n",
            "train loss:0.26353002245496127\n",
            "train loss:0.30370596132658817\n",
            "train loss:0.2529580901318038\n",
            "train loss:0.20105456068348562\n",
            "train loss:0.2926310928122145\n",
            "train loss:0.3127986982234702\n",
            "train loss:0.18197129156128597\n",
            "train loss:0.24783004838423042\n",
            "train loss:0.475681952741651\n",
            "train loss:0.36511362868518327\n",
            "train loss:0.23177087820902922\n",
            "train loss:0.3591752049909101\n",
            "train loss:0.32506999075925064\n",
            "train loss:0.24668437753101777\n",
            "train loss:0.2738386432044291\n",
            "=== epoch:18, train acc:0.886, test acc:0.878 ===\n",
            "train loss:0.3532813153581246\n",
            "train loss:0.3673120231679679\n",
            "train loss:0.2969644958021898\n",
            "train loss:0.49200743807244485\n",
            "train loss:0.41761145629720964\n",
            "train loss:0.4124487211259107\n",
            "train loss:0.3710954535242971\n",
            "train loss:0.3554962794635761\n",
            "train loss:0.19656014915202405\n",
            "train loss:0.24963090132814297\n",
            "train loss:0.5254528640602233\n",
            "train loss:0.3405846455436221\n",
            "train loss:0.21114773310588753\n",
            "train loss:0.2483119514433515\n",
            "train loss:0.3594913670586824\n",
            "train loss:0.33655655425551606\n",
            "train loss:0.31644206820305265\n",
            "train loss:0.3728565704277225\n",
            "train loss:0.3924151830619384\n",
            "train loss:0.3356271600639931\n",
            "train loss:0.2245715869829568\n",
            "train loss:0.26690257621112584\n",
            "train loss:0.4385129778147224\n",
            "train loss:0.21934547798050436\n",
            "train loss:0.35599107908185273\n",
            "train loss:0.30937517329243136\n",
            "train loss:0.30859704169445606\n",
            "train loss:0.326888042199081\n",
            "train loss:0.19236370343053086\n",
            "train loss:0.3811791379110272\n",
            "train loss:0.19905668481023978\n",
            "train loss:0.2285307281844019\n",
            "train loss:0.24994262610469517\n",
            "train loss:0.39716834396517725\n",
            "train loss:0.24677227667879287\n",
            "train loss:0.3452002908672698\n",
            "train loss:0.25642137821406963\n",
            "train loss:0.40069836008694154\n",
            "train loss:0.36476367638119145\n",
            "train loss:0.3083287074477775\n",
            "train loss:0.2941676755271342\n",
            "train loss:0.2845745245849269\n",
            "train loss:0.28630909298700347\n",
            "train loss:0.2614969330486771\n",
            "train loss:0.23110518609933972\n",
            "train loss:0.3775126112876255\n",
            "train loss:0.298694729630522\n",
            "train loss:0.28974903850787337\n",
            "train loss:0.46679044134750514\n",
            "train loss:0.23917493638774176\n",
            "=== epoch:19, train acc:0.879, test acc:0.88 ===\n",
            "train loss:0.4297274737299845\n",
            "train loss:0.3698817172497717\n",
            "train loss:0.4244009254190083\n",
            "train loss:0.33742218389808104\n",
            "train loss:0.3698738989761893\n",
            "train loss:0.277162353181774\n",
            "train loss:0.3037801104502645\n",
            "train loss:0.3472542439100658\n",
            "train loss:0.3326995695745056\n",
            "train loss:0.25396641750583876\n",
            "train loss:0.18297025572756043\n",
            "train loss:0.2813483151924133\n",
            "train loss:0.380175310016702\n",
            "train loss:0.24625889189118294\n",
            "train loss:0.3480866016774929\n",
            "train loss:0.5338369938573729\n",
            "train loss:0.3308945609474382\n",
            "train loss:0.3449010216793627\n",
            "train loss:0.2067741175742277\n",
            "train loss:0.27908381645816815\n",
            "train loss:0.2138325164168315\n",
            "train loss:0.29864853143888764\n",
            "train loss:0.21729937602874816\n",
            "train loss:0.26922091922494357\n",
            "train loss:0.3160960669355298\n",
            "train loss:0.1824837444985227\n",
            "train loss:0.26806111631765045\n",
            "train loss:0.3403185251161759\n",
            "train loss:0.45228695302658734\n",
            "train loss:0.23889502005355565\n",
            "train loss:0.36029175472194763\n",
            "train loss:0.35563188450186695\n",
            "train loss:0.26421649719242946\n",
            "train loss:0.4430760734058064\n",
            "train loss:0.31444173556546506\n",
            "train loss:0.2747895414255515\n",
            "train loss:0.26397314458787213\n",
            "train loss:0.6433037560366225\n",
            "train loss:0.46184376726229737\n",
            "train loss:0.31947743062924955\n",
            "train loss:0.4994912426474091\n",
            "train loss:0.15504162531951204\n",
            "train loss:0.264534603827981\n",
            "train loss:0.2668807021219788\n",
            "train loss:0.18897904252699793\n",
            "train loss:0.2906425394459731\n",
            "train loss:0.3495880003556931\n",
            "train loss:0.20325215780080563\n",
            "train loss:0.22311311929636254\n",
            "train loss:0.3109575739455416\n",
            "=== epoch:20, train acc:0.895, test acc:0.878 ===\n",
            "train loss:0.30056157605290534\n",
            "train loss:0.21525593070587232\n",
            "train loss:0.48448718485255016\n",
            "train loss:0.33038172104739305\n",
            "train loss:0.2573067460027232\n",
            "train loss:0.2873106820924926\n",
            "train loss:0.2587483516138451\n",
            "train loss:0.39996401532904585\n",
            "train loss:0.40914035126042003\n",
            "train loss:0.36290782492417933\n",
            "train loss:0.33797896784217385\n",
            "train loss:0.3172874704037059\n",
            "train loss:0.34171178243040673\n",
            "train loss:0.270081022876961\n",
            "train loss:0.2199914327074225\n",
            "train loss:0.290138403736255\n",
            "train loss:0.29901085334683136\n",
            "train loss:0.3492158592710568\n",
            "train loss:0.3991561404204087\n",
            "train loss:0.1675154410924537\n",
            "train loss:0.35437350746818747\n",
            "train loss:0.22176419312841614\n",
            "train loss:0.3259501711324292\n",
            "train loss:0.29696380198218414\n",
            "train loss:0.24567453892837698\n",
            "train loss:0.27379216896534225\n",
            "train loss:0.24571975231199816\n",
            "train loss:0.274552314821239\n",
            "train loss:0.304558847789832\n",
            "train loss:0.21493371335488176\n",
            "train loss:0.22012146991169657\n",
            "train loss:0.25132185301960214\n",
            "train loss:0.19351535657260388\n",
            "train loss:0.3866424571769184\n",
            "train loss:0.30508055563678455\n",
            "train loss:0.29682821905792506\n",
            "train loss:0.3041462355090415\n",
            "train loss:0.264782449575601\n",
            "train loss:0.23315185709232847\n",
            "train loss:0.23212214107032397\n",
            "train loss:0.27841420148836626\n",
            "train loss:0.3364827106868081\n",
            "train loss:0.2462186823781564\n",
            "train loss:0.2358517515036356\n",
            "train loss:0.38890471720207287\n",
            "train loss:0.3278810321217426\n",
            "train loss:0.3662665320484507\n",
            "train loss:0.2675843398208816\n",
            "train loss:0.2299820892941239\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.884\n",
            "Saved network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
        "                        conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs,\n",
        "                  mini_batch_size=100, optimizer='Adam', optimizer_param={'lr' : 0.0001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved network Parameters!\")\n",
        "\n",
        "markers = {'train' : 'o', 'test' : 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.2987775551304295\n",
            "=== epoch:1, train acc:0.263, test acc:0.289 ===\n",
            "train loss:2.2975510801994914\n",
            "train loss:2.2932098068031634\n",
            "train loss:2.2833649441840227\n",
            "train loss:2.277984813264986\n",
            "train loss:2.2642844046421033\n",
            "train loss:2.2597900130384176\n",
            "train loss:2.2385259252407046\n",
            "train loss:2.1957188728428103\n",
            "train loss:2.1590683072240258\n",
            "train loss:2.1579232167794387\n",
            "train loss:2.111745196626186\n",
            "train loss:2.0801959609446463\n",
            "train loss:1.9867361351064163\n",
            "train loss:1.9761498933533457\n",
            "train loss:1.8719183686015188\n",
            "train loss:1.915768715154602\n",
            "train loss:1.6970724292853487\n",
            "train loss:1.6769588029449465\n",
            "train loss:1.6471800065008046\n",
            "train loss:1.5629728590828083\n",
            "train loss:1.3524138733836242\n",
            "train loss:1.3508277504968695\n",
            "train loss:1.2681630174737877\n",
            "train loss:1.2904878695717905\n",
            "train loss:1.0724207905021397\n",
            "train loss:1.0651780892540301\n",
            "train loss:0.9328315081757853\n",
            "train loss:0.9625629368188755\n",
            "train loss:0.9240916437173274\n",
            "train loss:0.8414903578990557\n",
            "train loss:0.7439814277835292\n",
            "train loss:0.8817783830188172\n",
            "train loss:0.7390253829201933\n",
            "train loss:0.6701261827343824\n",
            "train loss:0.7261855249992211\n",
            "train loss:0.5785216825752058\n",
            "train loss:0.6488713054530413\n",
            "train loss:0.8057215790592857\n",
            "train loss:0.7092308863071769\n",
            "train loss:0.6214177026658121\n",
            "train loss:0.5055372727986152\n",
            "train loss:0.5190429792129306\n",
            "train loss:0.45789322648095987\n",
            "train loss:0.6525790379568551\n",
            "train loss:0.5981853218316386\n",
            "train loss:0.5927079725291222\n",
            "train loss:0.6212464063715825\n",
            "train loss:0.45598721693558353\n",
            "train loss:0.44776323089494885\n",
            "train loss:0.37037125526164066\n",
            "=== epoch:2, train acc:0.804, test acc:0.794 ===\n",
            "train loss:0.40914490878797866\n",
            "train loss:0.43937365246969934\n",
            "train loss:0.5372574619455627\n",
            "train loss:0.7442948406825471\n",
            "train loss:0.4756761898961189\n",
            "train loss:0.522353415634762\n",
            "train loss:0.7102793424843532\n",
            "train loss:0.4114087054113212\n",
            "train loss:0.4175285122579314\n",
            "train loss:0.2919182308407634\n",
            "train loss:0.552497481450084\n",
            "train loss:0.540510129917822\n",
            "train loss:0.3605853598717399\n",
            "train loss:0.4251493161692892\n",
            "train loss:0.3519765563304069\n",
            "train loss:0.3524060633738707\n",
            "train loss:0.30575348845819955\n",
            "train loss:0.33224663498526363\n",
            "train loss:0.2938018735079436\n",
            "train loss:0.41372910835462356\n",
            "train loss:0.4728560746300674\n",
            "train loss:0.37975727674664483\n",
            "train loss:0.3622697979925442\n",
            "train loss:0.44032353559202386\n",
            "train loss:0.4076697851395471\n",
            "train loss:0.3660274480412049\n",
            "train loss:0.4654305689634944\n",
            "train loss:0.4034901945696649\n",
            "train loss:0.3263179689388376\n",
            "train loss:0.3291898343007786\n",
            "train loss:0.45075327957691136\n",
            "train loss:0.26133627333284976\n",
            "train loss:0.3805779458601328\n",
            "train loss:0.3887390856106848\n",
            "train loss:0.3245508715359087\n",
            "train loss:0.43576866298220707\n",
            "train loss:0.3083484256922895\n",
            "train loss:0.366516448449413\n",
            "train loss:0.4706299445776467\n",
            "train loss:0.2322662670908467\n",
            "train loss:0.2269343469088249\n",
            "train loss:0.3924190284607799\n",
            "train loss:0.42559541764908104\n",
            "train loss:0.32903837663295327\n",
            "train loss:0.19897701520532618\n",
            "train loss:0.2624617477648178\n",
            "train loss:0.23740459008972206\n",
            "train loss:0.2959532998689703\n",
            "train loss:0.374289562952648\n",
            "train loss:0.29452764003797754\n",
            "=== epoch:3, train acc:0.87, test acc:0.857 ===\n",
            "train loss:0.24529421946877078\n",
            "train loss:0.4508790606103739\n",
            "train loss:0.3458764445670154\n",
            "train loss:0.2764969703751014\n",
            "train loss:0.412483746349479\n",
            "train loss:0.3269948217692903\n",
            "train loss:0.21682407395291253\n",
            "train loss:0.22064732272168616\n",
            "train loss:0.2962559447055737\n",
            "train loss:0.22927169579302994\n",
            "train loss:0.3666307360221623\n",
            "train loss:0.21148862140446859\n",
            "train loss:0.20817287408021137\n",
            "train loss:0.4005982244115463\n",
            "train loss:0.22442827401482254\n",
            "train loss:0.1721936071459929\n",
            "train loss:0.5636193801080868\n",
            "train loss:0.20963899018564505\n",
            "train loss:0.5170294346435278\n",
            "train loss:0.18376751478548997\n",
            "train loss:0.1782080515327914\n",
            "train loss:0.3029264620541744\n",
            "train loss:0.502348372527478\n",
            "train loss:0.27317999344150307\n",
            "train loss:0.3791220256180018\n",
            "train loss:0.31829772797562245\n",
            "train loss:0.25144707967905827\n",
            "train loss:0.2911430210452708\n",
            "train loss:0.3294795484006513\n",
            "train loss:0.22976508300578366\n",
            "train loss:0.354282865503527\n",
            "train loss:0.28882708764031223\n",
            "train loss:0.3749376080102846\n",
            "train loss:0.3247204810608391\n",
            "train loss:0.2841937071607507\n",
            "train loss:0.19902169764875172\n",
            "train loss:0.2767750460432035\n",
            "train loss:0.45313978192603854\n",
            "train loss:0.28307366796309497\n",
            "train loss:0.17573706121634483\n",
            "train loss:0.24036484595444751\n",
            "train loss:0.2188724186852158\n",
            "train loss:0.34831329769586106\n",
            "train loss:0.17277100085179492\n",
            "train loss:0.2674373987998156\n",
            "train loss:0.37783694247139415\n",
            "train loss:0.24429276010025697\n",
            "train loss:0.30512948427302455\n",
            "train loss:0.3388454762103911\n",
            "train loss:0.326233009230721\n",
            "=== epoch:4, train acc:0.894, test acc:0.886 ===\n",
            "train loss:0.3215696932368673\n",
            "train loss:0.21965892213972393\n",
            "train loss:0.31632926792487465\n",
            "train loss:0.38722826492288187\n",
            "train loss:0.12243053432657028\n",
            "train loss:0.14761646255727204\n",
            "train loss:0.2019490128740108\n",
            "train loss:0.2609362644861332\n",
            "train loss:0.19899607435711203\n",
            "train loss:0.109186139819423\n",
            "train loss:0.27477787414011867\n",
            "train loss:0.3088839313265964\n",
            "train loss:0.17544945738864176\n",
            "train loss:0.3318746661091506\n",
            "train loss:0.27289185007042854\n",
            "train loss:0.3324981284872623\n",
            "train loss:0.34907164808326563\n",
            "train loss:0.1727805026616544\n",
            "train loss:0.3479636957586218\n",
            "train loss:0.2927167181958488\n",
            "train loss:0.2342226336273022\n",
            "train loss:0.3841879203245348\n",
            "train loss:0.2989485194360811\n",
            "train loss:0.16932749182119608\n",
            "train loss:0.23384251398653855\n",
            "train loss:0.22704014166585001\n",
            "train loss:0.19450404272892158\n",
            "train loss:0.3537243262210898\n",
            "train loss:0.24668011558992614\n",
            "train loss:0.23344795011449407\n",
            "train loss:0.2556763973699385\n",
            "train loss:0.17686857809339807\n",
            "train loss:0.24487265341878214\n",
            "train loss:0.18493386028757158\n",
            "train loss:0.2943555567069565\n",
            "train loss:0.23863012775767545\n",
            "train loss:0.2747397845906748\n",
            "train loss:0.16159119757741994\n",
            "train loss:0.2709078558232061\n",
            "train loss:0.21694209046028856\n",
            "train loss:0.17584344892518483\n",
            "train loss:0.3194651964632636\n",
            "train loss:0.17945121993095015\n",
            "train loss:0.341502759340688\n",
            "train loss:0.22995929382063016\n",
            "train loss:0.2691021397207536\n",
            "train loss:0.3421091377151316\n",
            "train loss:0.16141440085240624\n",
            "train loss:0.17499033739438205\n",
            "train loss:0.327070623125466\n",
            "=== epoch:5, train acc:0.913, test acc:0.883 ===\n",
            "train loss:0.3233722514678089\n",
            "train loss:0.24370253158437208\n",
            "train loss:0.2588687679726374\n",
            "train loss:0.17863314084009335\n",
            "train loss:0.25212280719722047\n",
            "train loss:0.2665559078875957\n",
            "train loss:0.2029523180457467\n",
            "train loss:0.2656120298520384\n",
            "train loss:0.3584859274159394\n",
            "train loss:0.21979053064563153\n",
            "train loss:0.16361779186743275\n",
            "train loss:0.1899210601031461\n",
            "train loss:0.26378033918800153\n",
            "train loss:0.3052516595100555\n",
            "train loss:0.24013606003415863\n",
            "train loss:0.21031395648189716\n",
            "train loss:0.13417231625147447\n",
            "train loss:0.17505935997955444\n",
            "train loss:0.15362938080003016\n",
            "train loss:0.12100429432747604\n",
            "train loss:0.08753217640443416\n",
            "train loss:0.27405920343411916\n",
            "train loss:0.308780818809449\n",
            "train loss:0.1752684193571762\n",
            "train loss:0.29233605790637646\n",
            "train loss:0.1423120206431865\n",
            "train loss:0.21019710228181537\n",
            "train loss:0.23237617582737508\n",
            "train loss:0.20030619009641207\n",
            "train loss:0.15110971805469492\n",
            "train loss:0.2373964872822127\n",
            "train loss:0.2021812982465613\n",
            "train loss:0.11136247075406165\n",
            "train loss:0.173133362468595\n",
            "train loss:0.19957837216732688\n",
            "train loss:0.21400364298220884\n",
            "train loss:0.3721112735504045\n",
            "train loss:0.2768448973167009\n",
            "train loss:0.27299016070649756\n",
            "train loss:0.24598584653905214\n",
            "train loss:0.1566181629460214\n",
            "train loss:0.16034532195266002\n",
            "train loss:0.18875323197645866\n",
            "train loss:0.2719153787881136\n",
            "train loss:0.20215955430980248\n",
            "train loss:0.2882801840121166\n",
            "train loss:0.2502511274173857\n",
            "train loss:0.30349925452199894\n",
            "train loss:0.13952415185121883\n",
            "train loss:0.2808339805438681\n",
            "=== epoch:6, train acc:0.912, test acc:0.903 ===\n",
            "train loss:0.16422258661950562\n",
            "train loss:0.14062271968047915\n",
            "train loss:0.27655611393337787\n",
            "train loss:0.29630164129983816\n",
            "train loss:0.34339317279431675\n",
            "train loss:0.17836849754710785\n",
            "train loss:0.2989237818383328\n",
            "train loss:0.28125813932434623\n",
            "train loss:0.12572327156674962\n",
            "train loss:0.1624995504072025\n",
            "train loss:0.17819127459955006\n",
            "train loss:0.24029955891699636\n",
            "train loss:0.20923452257167274\n",
            "train loss:0.18797455407924638\n",
            "train loss:0.13173973331930572\n",
            "train loss:0.17625002088231145\n",
            "train loss:0.20642287011062366\n",
            "train loss:0.23939270976561533\n",
            "train loss:0.15352490576341576\n",
            "train loss:0.10299124579545248\n",
            "train loss:0.07865899291763645\n",
            "train loss:0.17538430151207823\n",
            "train loss:0.12021698013955197\n",
            "train loss:0.19582470868570387\n",
            "train loss:0.20516650688152296\n",
            "train loss:0.16076045751545542\n",
            "train loss:0.2206105992807907\n",
            "train loss:0.2514748192295943\n",
            "train loss:0.26690742732112016\n",
            "train loss:0.05759520660139548\n",
            "train loss:0.12958163427249753\n",
            "train loss:0.0938535049353957\n",
            "train loss:0.20488013002878622\n",
            "train loss:0.1524245513384388\n",
            "train loss:0.3126709666882219\n",
            "train loss:0.2004292767124376\n",
            "train loss:0.30485240389471024\n",
            "train loss:0.23617791208558309\n",
            "train loss:0.21406856198036084\n",
            "train loss:0.13133149715667466\n",
            "train loss:0.12491964979724784\n",
            "train loss:0.13450579241206015\n",
            "train loss:0.09211402837501018\n",
            "train loss:0.23124315204567444\n",
            "train loss:0.1474680418891978\n",
            "train loss:0.24554344238887196\n",
            "train loss:0.1560836544214616\n",
            "train loss:0.0895810181113112\n",
            "train loss:0.1070640203972202\n",
            "train loss:0.14876856193741622\n",
            "=== epoch:7, train acc:0.941, test acc:0.92 ===\n",
            "train loss:0.14863011313573313\n",
            "train loss:0.11002707304842783\n",
            "train loss:0.14040619325424544\n",
            "train loss:0.07252789713337719\n",
            "train loss:0.0678809022170467\n",
            "train loss:0.1385997805181664\n",
            "train loss:0.30989864196483824\n",
            "train loss:0.22987093419924767\n",
            "train loss:0.1588123945985719\n",
            "train loss:0.1690048875819367\n",
            "train loss:0.21935121990841253\n",
            "train loss:0.26724776755973323\n",
            "train loss:0.1329229835510505\n",
            "train loss:0.13578742737767274\n",
            "train loss:0.17799196858652006\n",
            "train loss:0.21592088664194475\n",
            "train loss:0.2024387067487092\n",
            "train loss:0.1990147745892328\n",
            "train loss:0.21177837480867084\n",
            "train loss:0.3186987667814812\n",
            "train loss:0.09805972634303695\n",
            "train loss:0.16526942046505855\n",
            "train loss:0.26166311305057677\n",
            "train loss:0.20309290711971006\n",
            "train loss:0.3092316796413863\n",
            "train loss:0.14740314631302184\n",
            "train loss:0.19506687672031997\n",
            "train loss:0.22615939371299554\n",
            "train loss:0.24643888726098553\n",
            "train loss:0.19557563912757917\n",
            "train loss:0.33724268873748947\n",
            "train loss:0.10478372931313673\n",
            "train loss:0.15006348067592268\n",
            "train loss:0.16573713392591102\n",
            "train loss:0.29523002212589816\n",
            "train loss:0.13904473095424705\n",
            "train loss:0.12258132464839239\n",
            "train loss:0.2665668977565554\n",
            "train loss:0.22209967559160507\n",
            "train loss:0.12730029262084608\n",
            "train loss:0.22948577938502235\n",
            "train loss:0.13092080294582292\n",
            "train loss:0.1598672311135854\n",
            "train loss:0.24756475997811228\n",
            "train loss:0.2534385355129467\n",
            "train loss:0.11056179000434739\n",
            "train loss:0.08004390458828112\n",
            "train loss:0.12104006360906175\n",
            "train loss:0.09604506102358623\n",
            "train loss:0.21842183038576682\n",
            "=== epoch:8, train acc:0.946, test acc:0.922 ===\n",
            "train loss:0.13426646197173656\n",
            "train loss:0.22496250866408446\n",
            "train loss:0.1093630257715872\n",
            "train loss:0.22037278141078523\n",
            "train loss:0.16362923193641266\n",
            "train loss:0.2047858073592318\n",
            "train loss:0.20369742695635615\n",
            "train loss:0.19631461514705856\n",
            "train loss:0.12719156552289343\n",
            "train loss:0.13154378287271504\n",
            "train loss:0.22035200871441724\n",
            "train loss:0.11719242216674809\n",
            "train loss:0.1273206775327203\n",
            "train loss:0.10983682027675665\n",
            "train loss:0.21146166134506225\n",
            "train loss:0.19679018270702164\n",
            "train loss:0.2286869986165192\n",
            "train loss:0.18223327516421584\n",
            "train loss:0.16647288481213507\n",
            "train loss:0.12584578538761101\n",
            "train loss:0.1768187255876631\n",
            "train loss:0.15127898932116746\n",
            "train loss:0.10437499920568383\n",
            "train loss:0.14619246691493037\n",
            "train loss:0.1092598346728796\n",
            "train loss:0.21142212798475998\n",
            "train loss:0.20223808905251953\n",
            "train loss:0.1612980375365744\n",
            "train loss:0.14664084952888645\n",
            "train loss:0.1089811429401755\n",
            "train loss:0.09043836071527891\n",
            "train loss:0.15501771576221635\n",
            "train loss:0.1743805454076016\n",
            "train loss:0.15159264028282493\n",
            "train loss:0.13993315153951127\n",
            "train loss:0.1691337453378576\n",
            "train loss:0.19721052875240005\n",
            "train loss:0.1326016448822714\n",
            "train loss:0.11069776451156954\n",
            "train loss:0.2229319684983158\n",
            "train loss:0.15540993543361256\n",
            "train loss:0.13081145028371\n",
            "train loss:0.20939523047924108\n",
            "train loss:0.07067086710125892\n",
            "train loss:0.1314889752025228\n",
            "train loss:0.07446054251404072\n",
            "train loss:0.10734326406235832\n",
            "train loss:0.1060521594212918\n",
            "train loss:0.24118892342629134\n",
            "train loss:0.101341142099829\n",
            "=== epoch:9, train acc:0.953, test acc:0.934 ===\n",
            "train loss:0.21100137835704214\n",
            "train loss:0.2440036938564237\n",
            "train loss:0.09443562500703591\n",
            "train loss:0.18044080831953732\n",
            "train loss:0.11515294561531536\n",
            "train loss:0.1231324651373616\n",
            "train loss:0.05144981519962078\n",
            "train loss:0.09468506596191265\n",
            "train loss:0.11770479658754844\n",
            "train loss:0.08023040612943574\n",
            "train loss:0.0786660655572339\n",
            "train loss:0.11207654804358701\n",
            "train loss:0.11405110088782576\n",
            "train loss:0.2060368654583615\n",
            "train loss:0.13477912867160835\n",
            "train loss:0.15554135253680873\n",
            "train loss:0.18345737174801513\n",
            "train loss:0.12119085246460495\n",
            "train loss:0.10943658641246885\n",
            "train loss:0.07779409530445376\n",
            "train loss:0.1124427372264274\n",
            "train loss:0.1154772077400714\n",
            "train loss:0.10177644509865073\n",
            "train loss:0.3639624936515852\n",
            "train loss:0.10174652329113415\n",
            "train loss:0.13456492951278917\n",
            "train loss:0.05296707597039578\n",
            "train loss:0.07958700701791041\n",
            "train loss:0.0597909366090159\n",
            "train loss:0.08935933905249936\n",
            "train loss:0.1112206452225968\n",
            "train loss:0.08824049528866394\n",
            "train loss:0.06566773182625951\n",
            "train loss:0.15451536813150477\n",
            "train loss:0.06952889646046857\n",
            "train loss:0.06086904489610036\n",
            "train loss:0.1249003430136893\n",
            "train loss:0.13759674874080602\n",
            "train loss:0.11145412271654831\n",
            "train loss:0.07618426054323858\n",
            "train loss:0.10919189447002887\n",
            "train loss:0.0726377497373824\n",
            "train loss:0.11253687551460136\n",
            "train loss:0.1334616504688191\n",
            "train loss:0.07406603720823318\n",
            "train loss:0.19252069559993135\n",
            "train loss:0.12895324298780142\n",
            "train loss:0.13082270188487646\n",
            "train loss:0.10096599138362303\n",
            "train loss:0.16022489730314796\n",
            "=== epoch:10, train acc:0.957, test acc:0.945 ===\n",
            "train loss:0.10111845868386073\n",
            "train loss:0.12282803533537519\n",
            "train loss:0.12269170228016205\n",
            "train loss:0.07877675735100576\n",
            "train loss:0.080234897710769\n",
            "train loss:0.08220467676748269\n",
            "train loss:0.15230447263778135\n",
            "train loss:0.1601829387970275\n",
            "train loss:0.10245755792855384\n",
            "train loss:0.12844811480342277\n",
            "train loss:0.11128108684887117\n",
            "train loss:0.1483397685075181\n",
            "train loss:0.09366310908954105\n",
            "train loss:0.05245174750474066\n",
            "train loss:0.08124868750901557\n",
            "train loss:0.11631526040429906\n",
            "train loss:0.10345760436695867\n",
            "train loss:0.11490806051181184\n",
            "train loss:0.11850221662008811\n",
            "train loss:0.042912281899470314\n",
            "train loss:0.11186528712059897\n",
            "train loss:0.08287130638879381\n",
            "train loss:0.06631038742662819\n",
            "train loss:0.14564742215504428\n",
            "train loss:0.08017636212491362\n",
            "train loss:0.05967233307240128\n",
            "train loss:0.06489860522673063\n",
            "train loss:0.060034240921982354\n",
            "train loss:0.0986133126603424\n",
            "train loss:0.10879742185225488\n",
            "train loss:0.054495759150504586\n",
            "train loss:0.03648127127331221\n",
            "train loss:0.052852465008274015\n",
            "train loss:0.09767997512773757\n",
            "train loss:0.044175401715476736\n",
            "train loss:0.08167935508309741\n",
            "train loss:0.09793395811502713\n",
            "train loss:0.10967294549915906\n",
            "train loss:0.09898049617011619\n",
            "train loss:0.10757307772455534\n",
            "train loss:0.1121180595203301\n",
            "train loss:0.11850857081288182\n",
            "train loss:0.11012581883946654\n",
            "train loss:0.048011803536531665\n",
            "train loss:0.13446480364187388\n",
            "train loss:0.10675728387295642\n",
            "train loss:0.06439675724674614\n",
            "train loss:0.13467539354811672\n",
            "train loss:0.0337256636894221\n",
            "train loss:0.06947170767109571\n",
            "=== epoch:11, train acc:0.968, test acc:0.941 ===\n",
            "train loss:0.093061651406507\n",
            "train loss:0.1948613688657452\n",
            "train loss:0.1463362874346004\n",
            "train loss:0.11474090137103468\n",
            "train loss:0.04909267764394667\n",
            "train loss:0.18108803385412686\n",
            "train loss:0.0816863151662875\n",
            "train loss:0.0630428691954778\n",
            "train loss:0.0955746239393499\n",
            "train loss:0.05574779026948663\n",
            "train loss:0.1329779510467743\n",
            "train loss:0.08195453534752285\n",
            "train loss:0.060222568879441996\n",
            "train loss:0.11072836028440237\n",
            "train loss:0.13627678862969314\n",
            "train loss:0.07441160404664404\n",
            "train loss:0.056031859730918114\n",
            "train loss:0.08706621871290596\n",
            "train loss:0.031052090720960834\n",
            "train loss:0.0573276245910002\n",
            "train loss:0.06625744499149229\n",
            "train loss:0.034506014110705995\n",
            "train loss:0.06454615275505142\n",
            "train loss:0.04402780061961767\n",
            "train loss:0.03150593756609575\n",
            "train loss:0.030474413979555456\n",
            "train loss:0.061348042796075836\n",
            "train loss:0.030392308185268772\n",
            "train loss:0.09218075280913574\n",
            "train loss:0.05637112591204932\n",
            "train loss:0.07515266346829633\n",
            "train loss:0.0736243911730625\n",
            "train loss:0.01862411029599115\n",
            "train loss:0.15371756309698925\n",
            "train loss:0.03250712731356027\n",
            "train loss:0.1167618152601611\n",
            "train loss:0.04306388835169448\n",
            "train loss:0.04911051889482634\n",
            "train loss:0.12170302248237805\n",
            "train loss:0.07741682544341766\n",
            "train loss:0.0636445574523391\n",
            "train loss:0.10108709615271484\n",
            "train loss:0.16498916686280501\n",
            "train loss:0.0943853814159114\n",
            "train loss:0.08708161814343938\n",
            "train loss:0.06801743827017255\n",
            "train loss:0.05601738067248669\n",
            "train loss:0.20086713747113727\n",
            "train loss:0.12656569726910782\n",
            "train loss:0.0904604815780575\n",
            "=== epoch:12, train acc:0.967, test acc:0.946 ===\n",
            "train loss:0.062141638451394365\n",
            "train loss:0.09003513007978031\n",
            "train loss:0.047806641344674966\n",
            "train loss:0.15822681432056707\n",
            "train loss:0.06316830225611816\n",
            "train loss:0.07831725696962603\n",
            "train loss:0.09294588832296537\n",
            "train loss:0.09448456820376631\n",
            "train loss:0.05744944718678211\n",
            "train loss:0.06034901893293819\n",
            "train loss:0.0770406319009347\n",
            "train loss:0.0588106784653274\n",
            "train loss:0.05661386260529986\n",
            "train loss:0.03944733982189772\n",
            "train loss:0.05000942463624572\n",
            "train loss:0.04646568562435811\n",
            "train loss:0.05565393641146638\n",
            "train loss:0.05525091283506565\n",
            "train loss:0.05459209529557419\n",
            "train loss:0.12803237286526892\n",
            "train loss:0.09856046262592805\n",
            "train loss:0.09881503791399389\n",
            "train loss:0.046224045705723665\n",
            "train loss:0.19112082637342862\n",
            "train loss:0.07050847876046192\n",
            "train loss:0.09402748988327056\n",
            "train loss:0.06526464502962352\n",
            "train loss:0.12077938452002407\n",
            "train loss:0.18570252284294228\n",
            "train loss:0.12573030334358667\n",
            "train loss:0.03911740669137081\n",
            "train loss:0.045105636633890854\n",
            "train loss:0.055265422913248825\n",
            "train loss:0.14127288877258717\n",
            "train loss:0.034517066236901026\n",
            "train loss:0.06334856817392316\n",
            "train loss:0.16150958004595173\n",
            "train loss:0.07634171651867178\n",
            "train loss:0.10785676954509955\n",
            "train loss:0.13918654491750718\n",
            "train loss:0.09088227625666156\n",
            "train loss:0.07718950383100279\n",
            "train loss:0.057020796348199794\n",
            "train loss:0.09443644949154334\n",
            "train loss:0.07585788651463304\n",
            "train loss:0.06496950406835066\n",
            "train loss:0.0252635351752637\n",
            "train loss:0.053478545982990136\n",
            "train loss:0.026716388606428918\n",
            "train loss:0.06301695352952909\n",
            "=== epoch:13, train acc:0.969, test acc:0.95 ===\n",
            "train loss:0.053876284546076236\n",
            "train loss:0.13155872209667382\n",
            "train loss:0.05899361480469637\n",
            "train loss:0.0783240034159949\n",
            "train loss:0.02585165677560242\n",
            "train loss:0.04202460369952676\n",
            "train loss:0.07775611637864545\n",
            "train loss:0.06224691175019057\n",
            "train loss:0.028330993332571167\n",
            "train loss:0.02431280466018865\n",
            "train loss:0.05201481046918526\n",
            "train loss:0.07053074296778988\n",
            "train loss:0.06895289210541353\n",
            "train loss:0.06202817380480361\n",
            "train loss:0.07477502976584156\n",
            "train loss:0.05424690259829525\n",
            "train loss:0.1045103145618262\n",
            "train loss:0.06424982378970162\n",
            "train loss:0.042102453872288594\n",
            "train loss:0.022859414984211193\n",
            "train loss:0.04544405746822232\n",
            "train loss:0.08839041779646709\n",
            "train loss:0.04419516582447724\n",
            "train loss:0.050027731343962295\n",
            "train loss:0.0777601928215215\n",
            "train loss:0.053315291143555335\n",
            "train loss:0.10183000603774216\n",
            "train loss:0.03168970100250919\n",
            "train loss:0.10777734270799966\n",
            "train loss:0.059771665462972745\n",
            "train loss:0.0727164307199308\n",
            "train loss:0.09357074094400086\n",
            "train loss:0.10407719033572188\n",
            "train loss:0.053086100801617506\n",
            "train loss:0.05916658188302244\n",
            "train loss:0.0678517422417389\n",
            "train loss:0.025034386080357286\n",
            "train loss:0.06907946110849487\n",
            "train loss:0.025297753722465354\n",
            "train loss:0.020648367892380778\n",
            "train loss:0.11679244273058771\n",
            "train loss:0.0301348196874744\n",
            "train loss:0.0876736618182245\n",
            "train loss:0.044283825027808685\n",
            "train loss:0.0464272224465629\n",
            "train loss:0.050341086041443425\n",
            "train loss:0.035137366414635336\n",
            "train loss:0.10577792002086023\n",
            "train loss:0.033822247083039374\n",
            "train loss:0.026292466282001145\n",
            "=== epoch:14, train acc:0.973, test acc:0.948 ===\n",
            "train loss:0.05315469131253026\n",
            "train loss:0.09369061448241829\n",
            "train loss:0.03457324131233714\n",
            "train loss:0.0780307199278477\n",
            "train loss:0.039268028021645494\n",
            "train loss:0.06842529450306467\n",
            "train loss:0.02970727286501865\n",
            "train loss:0.05270898128069253\n",
            "train loss:0.018233310786436405\n",
            "train loss:0.07485271092554072\n",
            "train loss:0.048836405285646076\n",
            "train loss:0.12909327774608154\n",
            "train loss:0.06565035934098364\n",
            "train loss:0.0657401888841402\n",
            "train loss:0.03899119195271484\n",
            "train loss:0.030232258081832053\n",
            "train loss:0.07512798908423898\n",
            "train loss:0.050860457914062865\n",
            "train loss:0.09274883036651765\n",
            "train loss:0.03355213529765954\n",
            "train loss:0.041400769611680766\n",
            "train loss:0.05646053548035698\n",
            "train loss:0.03230777651831217\n",
            "train loss:0.03526996272046043\n",
            "train loss:0.06533434207042585\n",
            "train loss:0.028387518987842683\n",
            "train loss:0.02842992774857966\n",
            "train loss:0.034717947618034095\n",
            "train loss:0.06967172657104208\n",
            "train loss:0.03601990057985483\n",
            "train loss:0.05160992289819404\n",
            "train loss:0.06874103819261179\n",
            "train loss:0.027407314631216657\n",
            "train loss:0.045023879032039575\n",
            "train loss:0.04656926645842284\n",
            "train loss:0.027875576736635113\n",
            "train loss:0.021359812220164996\n",
            "train loss:0.03190197915042844\n",
            "train loss:0.015369263257433898\n",
            "train loss:0.09569165313586989\n",
            "train loss:0.060194521085556944\n",
            "train loss:0.04790750727518999\n",
            "train loss:0.03794874191354468\n",
            "train loss:0.09486167787929906\n",
            "train loss:0.047707480222865035\n",
            "train loss:0.019048910413954\n",
            "train loss:0.0450275174412937\n",
            "train loss:0.026995602855196282\n",
            "train loss:0.08366189656292461\n",
            "train loss:0.022517703601720677\n",
            "=== epoch:15, train acc:0.974, test acc:0.949 ===\n",
            "train loss:0.027928333531311727\n",
            "train loss:0.06086187017947085\n",
            "train loss:0.035500770609710665\n",
            "train loss:0.09368702104157652\n",
            "train loss:0.0355692574919111\n",
            "train loss:0.04604096300179422\n",
            "train loss:0.04518029790297691\n",
            "train loss:0.0255418568957277\n",
            "train loss:0.07591864846232518\n",
            "train loss:0.06177550377532828\n",
            "train loss:0.01811691919207717\n",
            "train loss:0.027836691235562348\n",
            "train loss:0.035564016740686204\n",
            "train loss:0.05173848700253186\n",
            "train loss:0.06419891427084734\n",
            "train loss:0.054207057095138914\n",
            "train loss:0.022058513599981917\n",
            "train loss:0.019696154880797335\n",
            "train loss:0.038355851505953636\n",
            "train loss:0.0956051995754688\n",
            "train loss:0.056301699115509835\n",
            "train loss:0.053155246840013604\n",
            "train loss:0.02655487567770689\n",
            "train loss:0.11848017911785469\n",
            "train loss:0.029380963938745463\n",
            "train loss:0.04439317810594012\n",
            "train loss:0.03467149936358569\n",
            "train loss:0.040142612752784565\n",
            "train loss:0.04751099240260331\n",
            "train loss:0.04184221212603216\n",
            "train loss:0.03223088295035419\n",
            "train loss:0.07312756258038912\n",
            "train loss:0.021741127043794904\n",
            "train loss:0.03105345491850532\n",
            "train loss:0.04917730046733644\n",
            "train loss:0.042559043110757665\n",
            "train loss:0.09209451485468712\n",
            "train loss:0.053251450697384\n",
            "train loss:0.02890563875407478\n",
            "train loss:0.026969236551575956\n",
            "train loss:0.05224872303408185\n",
            "train loss:0.025328773099563547\n",
            "train loss:0.021211009655843983\n",
            "train loss:0.15789804452184614\n",
            "train loss:0.07796615377374656\n",
            "train loss:0.04480414582954978\n",
            "train loss:0.12032689005853701\n",
            "train loss:0.06495359816146688\n",
            "train loss:0.08214628036463303\n",
            "train loss:0.045983400659110434\n",
            "=== epoch:16, train acc:0.983, test acc:0.951 ===\n",
            "train loss:0.048899278856787076\n",
            "train loss:0.037699182333385234\n",
            "train loss:0.11166423049336575\n",
            "train loss:0.052709724617261304\n",
            "train loss:0.029048107549644223\n",
            "train loss:0.10307636948311986\n",
            "train loss:0.03645176541427404\n",
            "train loss:0.0892310299726806\n",
            "train loss:0.02897536938138466\n",
            "train loss:0.020871278468272268\n",
            "train loss:0.03878429379030554\n",
            "train loss:0.048258314783766604\n",
            "train loss:0.02261488004226123\n",
            "train loss:0.03953845066573863\n",
            "train loss:0.030766238315956818\n",
            "train loss:0.030928743483360513\n",
            "train loss:0.0905422740484835\n",
            "train loss:0.03233655531516103\n",
            "train loss:0.013510295927178919\n",
            "train loss:0.03391443073765746\n",
            "train loss:0.03629923778513107\n",
            "train loss:0.03509285650305145\n",
            "train loss:0.044752498543980467\n",
            "train loss:0.07217882537797213\n",
            "train loss:0.0401098660840011\n",
            "train loss:0.0669575841050751\n",
            "train loss:0.10026878923061497\n",
            "train loss:0.0438678907080765\n",
            "train loss:0.0740911616080524\n",
            "train loss:0.0526023524474191\n",
            "train loss:0.08249202824919866\n",
            "train loss:0.028046336371083273\n",
            "train loss:0.049371571293212846\n",
            "train loss:0.026135969137420043\n",
            "train loss:0.040634418875136705\n",
            "train loss:0.03883773353115614\n",
            "train loss:0.020553619107728908\n",
            "train loss:0.07902857597458299\n",
            "train loss:0.03951394343516139\n",
            "train loss:0.01946195830508002\n",
            "train loss:0.03001707206324042\n",
            "train loss:0.047687945937833015\n",
            "train loss:0.036326715480511834\n",
            "train loss:0.022282727323752267\n",
            "train loss:0.07274860723387509\n",
            "train loss:0.05190692367842491\n",
            "train loss:0.02281772786479524\n",
            "train loss:0.036617238631946054\n",
            "train loss:0.03582849770172056\n",
            "train loss:0.04228273380983442\n",
            "=== epoch:17, train acc:0.984, test acc:0.963 ===\n",
            "train loss:0.071300626618226\n",
            "train loss:0.04894673534913082\n",
            "train loss:0.06680274817148336\n",
            "train loss:0.01647306524748788\n",
            "train loss:0.017960206642000556\n",
            "train loss:0.030140133120126894\n",
            "train loss:0.04796609448970233\n",
            "train loss:0.03924902493851904\n",
            "train loss:0.042435184711736344\n",
            "train loss:0.04887628540826078\n",
            "train loss:0.021435549720705178\n",
            "train loss:0.03074388144342253\n",
            "train loss:0.05016394589949573\n",
            "train loss:0.02559791407874067\n",
            "train loss:0.02855555830787174\n",
            "train loss:0.014518536937115918\n",
            "train loss:0.040657927610232104\n",
            "train loss:0.029999539035216446\n",
            "train loss:0.015654920483608465\n",
            "train loss:0.05602859953282466\n",
            "train loss:0.013762059243402522\n",
            "train loss:0.02806851505295918\n",
            "train loss:0.02965544165797927\n",
            "train loss:0.047632561088196114\n",
            "train loss:0.0690679893879588\n",
            "train loss:0.027897881671075538\n",
            "train loss:0.018038904414201255\n",
            "train loss:0.0247404546899151\n",
            "train loss:0.021723049578921697\n",
            "train loss:0.025026086940461406\n",
            "train loss:0.033920076635227496\n",
            "train loss:0.011189936999717695\n",
            "train loss:0.070640022253749\n",
            "train loss:0.021617164460586685\n",
            "train loss:0.02191475149428184\n",
            "train loss:0.019860300048590317\n",
            "train loss:0.030086292181385196\n",
            "train loss:0.036158692721822365\n",
            "train loss:0.022259552424669166\n",
            "train loss:0.0530828084701659\n",
            "train loss:0.0887254071099062\n",
            "train loss:0.02374487784209004\n",
            "train loss:0.03262920450592744\n",
            "train loss:0.030968615586164233\n",
            "train loss:0.0125087476914724\n",
            "train loss:0.018220584500219357\n",
            "train loss:0.043431603939958305\n",
            "train loss:0.016887765811246645\n",
            "train loss:0.018294976514172792\n",
            "train loss:0.0183183602703805\n",
            "=== epoch:18, train acc:0.989, test acc:0.956 ===\n",
            "train loss:0.02769186607143338\n",
            "train loss:0.0411756664612236\n",
            "train loss:0.02495516200173118\n",
            "train loss:0.03599398162566707\n",
            "train loss:0.08608931664137379\n",
            "train loss:0.029765711518076155\n",
            "train loss:0.047249519391053874\n",
            "train loss:0.02169208789801581\n",
            "train loss:0.019454715644658428\n",
            "train loss:0.038558165439367374\n",
            "train loss:0.052946687905856535\n",
            "train loss:0.03096465251480375\n",
            "train loss:0.01790914832961546\n",
            "train loss:0.02604745835742257\n",
            "train loss:0.01605629163767283\n",
            "train loss:0.016988312642339874\n",
            "train loss:0.018186366505754895\n",
            "train loss:0.042614078697982345\n",
            "train loss:0.014445677409309035\n",
            "train loss:0.01351065755015672\n",
            "train loss:0.020575334487122766\n",
            "train loss:0.0520310230503708\n",
            "train loss:0.04852080073080457\n",
            "train loss:0.019756752773498726\n",
            "train loss:0.020483699901350735\n",
            "train loss:0.020736018857276228\n",
            "train loss:0.028143696621055567\n",
            "train loss:0.026513956088007815\n",
            "train loss:0.031022856777716344\n",
            "train loss:0.06576762720279404\n",
            "train loss:0.023611642791296256\n",
            "train loss:0.03225196237051522\n",
            "train loss:0.005160484999645546\n",
            "train loss:0.014335625060660238\n",
            "train loss:0.02182106365477434\n",
            "train loss:0.033380082668131965\n",
            "train loss:0.03012332304338865\n",
            "train loss:0.017289504677146014\n",
            "train loss:0.02587090369229022\n",
            "train loss:0.03365782586645946\n",
            "train loss:0.03166606537264454\n",
            "train loss:0.02275907319349972\n",
            "train loss:0.0627211497203795\n",
            "train loss:0.030006518208245306\n",
            "train loss:0.040495384416166395\n",
            "train loss:0.02495483752618117\n",
            "train loss:0.02877438424653343\n",
            "train loss:0.017237411703765783\n",
            "train loss:0.03542499990279667\n",
            "train loss:0.01678391021730214\n",
            "=== epoch:19, train acc:0.991, test acc:0.958 ===\n",
            "train loss:0.03012821124007118\n",
            "train loss:0.030830295202397905\n",
            "train loss:0.016664659723558788\n",
            "train loss:0.022921126753387214\n",
            "train loss:0.019127588754632982\n",
            "train loss:0.09020609895126804\n",
            "train loss:0.019420057823545486\n",
            "train loss:0.01585264566835146\n",
            "train loss:0.025467624835729584\n",
            "train loss:0.026525346613425236\n",
            "train loss:0.014976264979276796\n",
            "train loss:0.04037014024241649\n",
            "train loss:0.019426034911963912\n",
            "train loss:0.04740013037385785\n",
            "train loss:0.036254702747503795\n",
            "train loss:0.01826244424407691\n",
            "train loss:0.021946761086852607\n",
            "train loss:0.03688055964328147\n",
            "train loss:0.019711411200606814\n",
            "train loss:0.013265108363624305\n",
            "train loss:0.012006778616506874\n",
            "train loss:0.031668198492883215\n",
            "train loss:0.03355142343783189\n",
            "train loss:0.010722737836735835\n",
            "train loss:0.01350474753618159\n",
            "train loss:0.018961869138731578\n",
            "train loss:0.03961244158701794\n",
            "train loss:0.015445021687629212\n",
            "train loss:0.009747027211418152\n",
            "train loss:0.021571036896877498\n",
            "train loss:0.019238479153190196\n",
            "train loss:0.026234674950824256\n",
            "train loss:0.007951571954640461\n",
            "train loss:0.01956874178628387\n",
            "train loss:0.02317037139216489\n",
            "train loss:0.01867099445597426\n",
            "train loss:0.04341198935841033\n",
            "train loss:0.00783293488195545\n",
            "train loss:0.01269861976665121\n",
            "train loss:0.015027164719816157\n",
            "train loss:0.012978686365883113\n",
            "train loss:0.01631733952960059\n",
            "train loss:0.007928798697192107\n",
            "train loss:0.02420614086660043\n",
            "train loss:0.024783984227123083\n",
            "train loss:0.011632618773698153\n",
            "train loss:0.018443711458112466\n",
            "train loss:0.03151347509948013\n",
            "train loss:0.015897591949999554\n",
            "train loss:0.031175095510839138\n",
            "=== epoch:20, train acc:0.988, test acc:0.954 ===\n",
            "train loss:0.0212272631045521\n",
            "train loss:0.017747200689668712\n",
            "train loss:0.009080330643529215\n",
            "train loss:0.0189921253560958\n",
            "train loss:0.01810929822972063\n",
            "train loss:0.018412465239420507\n",
            "train loss:0.032120348554687296\n",
            "train loss:0.06947962972862722\n",
            "train loss:0.021944299317354345\n",
            "train loss:0.03327452116427103\n",
            "train loss:0.008638761776537052\n",
            "train loss:0.017975357627821747\n",
            "train loss:0.006184586372946012\n",
            "train loss:0.00882519887631386\n",
            "train loss:0.04597718996853662\n",
            "train loss:0.03799211013215714\n",
            "train loss:0.011328493076366138\n",
            "train loss:0.019268203222333698\n",
            "train loss:0.01178286914615287\n",
            "train loss:0.01916336661431734\n",
            "train loss:0.020506056998320105\n",
            "train loss:0.02930661554784696\n",
            "train loss:0.016206800392495398\n",
            "train loss:0.024429968753106254\n",
            "train loss:0.007437177220657109\n",
            "train loss:0.009568793830054247\n",
            "train loss:0.022050974761390826\n",
            "train loss:0.03435450218014759\n",
            "train loss:0.013443285702155933\n",
            "train loss:0.01518623755400233\n",
            "train loss:0.03518684530875871\n",
            "train loss:0.012639283487905306\n",
            "train loss:0.016408353667468355\n",
            "train loss:0.04531910311496689\n",
            "train loss:0.03727456881952073\n",
            "train loss:0.02393677783078891\n",
            "train loss:0.01195055457962103\n",
            "train loss:0.02778046568885076\n",
            "train loss:0.03296355081020754\n",
            "train loss:0.012706289051308836\n",
            "train loss:0.028826183509409707\n",
            "train loss:0.016522913505903566\n",
            "train loss:0.013821477172869119\n",
            "train loss:0.03402619360590668\n",
            "train loss:0.008839164937706465\n",
            "train loss:0.008735552856070996\n",
            "train loss:0.04382963773965724\n",
            "train loss:0.012047307713373319\n",
            "train loss:0.01001998376570563\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.952\n",
            "Saved network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
        "                        conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs,\n",
        "                  mini_batch_size=100, optimizer='Adam', optimizer_param={'lr' : 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved network Parameters!\")\n",
        "\n",
        "markers = {'train' : 'o', 'test' : 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.2892322805503977\n",
            "=== epoch:1, train acc:0.189, test acc:0.198 ===\n",
            "train loss:2.2633234997158254\n",
            "train loss:2.185256761849971\n",
            "train loss:2.086769555574799\n",
            "train loss:1.9676354103353082\n",
            "train loss:1.8162689439554296\n",
            "train loss:1.5762707116117958\n",
            "train loss:1.4893813890248728\n",
            "train loss:1.2395212242932023\n",
            "train loss:1.1008796919389956\n",
            "train loss:0.8692132915503186\n",
            "train loss:0.8376156634040839\n",
            "train loss:0.7329172571167342\n",
            "train loss:0.779764007087104\n",
            "train loss:0.7158267603965018\n",
            "train loss:0.6553507883064837\n",
            "train loss:0.7078478338911784\n",
            "train loss:0.8501822703833668\n",
            "train loss:0.3401416058741205\n",
            "train loss:0.7502891445901301\n",
            "train loss:0.816735165185908\n",
            "train loss:0.39700997986762476\n",
            "train loss:0.4944889749910261\n",
            "train loss:0.49299976482446967\n",
            "train loss:0.5672405026312419\n",
            "train loss:0.4720623249190548\n",
            "train loss:0.5090058175973006\n",
            "train loss:0.5157768168802114\n",
            "train loss:0.452309663715701\n",
            "train loss:0.3109690809664818\n",
            "train loss:0.4332288971324001\n",
            "train loss:0.4196784086688678\n",
            "train loss:0.4415180533839534\n",
            "train loss:0.41896420108389326\n",
            "train loss:0.4784228444822902\n",
            "train loss:0.47270191771002074\n",
            "train loss:0.3063108660828016\n",
            "train loss:0.36621136577591995\n",
            "train loss:0.538563436046082\n",
            "train loss:0.3516471061646408\n",
            "train loss:0.2930870030137627\n",
            "train loss:0.2631278989646426\n",
            "train loss:0.2678975780296267\n",
            "train loss:0.3433421009567407\n",
            "train loss:0.2964150478929113\n",
            "train loss:0.40951816896182064\n",
            "train loss:0.41014745444972484\n",
            "train loss:0.30383805696298816\n",
            "train loss:0.3004534956440371\n",
            "train loss:0.34181129380078423\n",
            "train loss:0.3154652059758785\n",
            "=== epoch:2, train acc:0.91, test acc:0.88 ===\n",
            "train loss:0.2600894710638362\n",
            "train loss:0.4043902831929394\n",
            "train loss:0.22368183255763519\n",
            "train loss:0.24138479467135013\n",
            "train loss:0.3282234863647897\n",
            "train loss:0.20007439646201028\n",
            "train loss:0.1842567452832862\n",
            "train loss:0.29181823792848244\n",
            "train loss:0.2617945834572093\n",
            "train loss:0.2659785833609497\n",
            "train loss:0.31152400173620814\n",
            "train loss:0.27715483737043944\n",
            "train loss:0.21059819807331032\n",
            "train loss:0.34890324926584365\n",
            "train loss:0.23431645756770114\n",
            "train loss:0.24583534104882496\n",
            "train loss:0.10620907827505746\n",
            "train loss:0.1493794062631322\n",
            "train loss:0.17693263832649456\n",
            "train loss:0.18864721923665914\n",
            "train loss:0.32130984477881575\n",
            "train loss:0.09147869201004528\n",
            "train loss:0.16562100593809706\n",
            "train loss:0.1905022121262695\n",
            "train loss:0.21671048905412607\n",
            "train loss:0.18542590353188587\n",
            "train loss:0.15604652825396434\n",
            "train loss:0.22555899754255548\n",
            "train loss:0.09687691574925791\n",
            "train loss:0.13771614421191128\n",
            "train loss:0.24803004245713858\n",
            "train loss:0.16324291366868682\n",
            "train loss:0.23164645502095632\n",
            "train loss:0.22613867171402868\n",
            "train loss:0.15308521082364607\n",
            "train loss:0.204705159721984\n",
            "train loss:0.11927790013391752\n",
            "train loss:0.2071293529353369\n",
            "train loss:0.09759588297210971\n",
            "train loss:0.23429302651010986\n",
            "train loss:0.09825814264028791\n",
            "train loss:0.12669736013541077\n",
            "train loss:0.09676347520454291\n",
            "train loss:0.22956260618315025\n",
            "train loss:0.17724109340191785\n",
            "train loss:0.15631636551756542\n",
            "train loss:0.08086923380339883\n",
            "train loss:0.21539658181770335\n",
            "train loss:0.08055805613566941\n",
            "train loss:0.08754379954335936\n",
            "=== epoch:3, train acc:0.95, test acc:0.928 ===\n",
            "train loss:0.09815424133443042\n",
            "train loss:0.1265790839546621\n",
            "train loss:0.09016294044698521\n",
            "train loss:0.14700969562293512\n",
            "train loss:0.14812725012199246\n",
            "train loss:0.3035832112016538\n",
            "train loss:0.26669044454890983\n",
            "train loss:0.17792484770780528\n",
            "train loss:0.09267645281310713\n",
            "train loss:0.1045977595017536\n",
            "train loss:0.11711703325135078\n",
            "train loss:0.11309053697983716\n",
            "train loss:0.05972441826088154\n",
            "train loss:0.13163826883040217\n",
            "train loss:0.1353180095151493\n",
            "train loss:0.19712115809005465\n",
            "train loss:0.0916163792573213\n",
            "train loss:0.09098119106095867\n",
            "train loss:0.08738887652288145\n",
            "train loss:0.10237178165283145\n",
            "train loss:0.12266764072115446\n",
            "train loss:0.09289060199349447\n",
            "train loss:0.17388348831642844\n",
            "train loss:0.09787805949400198\n",
            "train loss:0.10204557128275898\n",
            "train loss:0.09168505865554391\n",
            "train loss:0.10536947861913752\n",
            "train loss:0.13587315039947342\n",
            "train loss:0.2581663235982354\n",
            "train loss:0.1350783327430574\n",
            "train loss:0.07217687788242545\n",
            "train loss:0.20441963157776286\n",
            "train loss:0.0920829320515832\n",
            "train loss:0.16807654341984798\n",
            "train loss:0.11361659005518701\n",
            "train loss:0.06551793032219214\n",
            "train loss:0.09252471874274776\n",
            "train loss:0.09220832942021648\n",
            "train loss:0.11979152754270156\n",
            "train loss:0.08885379817681732\n",
            "train loss:0.13054062285769874\n",
            "train loss:0.05351647461428887\n",
            "train loss:0.08157967537190375\n",
            "train loss:0.04264216740076393\n",
            "train loss:0.11824005935239278\n",
            "train loss:0.0423035980019947\n",
            "train loss:0.09104931499355573\n",
            "train loss:0.12897661026513013\n",
            "train loss:0.10064492114910852\n",
            "train loss:0.051192893150238346\n",
            "=== epoch:4, train acc:0.971, test acc:0.943 ===\n",
            "train loss:0.0724426008073751\n",
            "train loss:0.12681160079851417\n",
            "train loss:0.05115338514067207\n",
            "train loss:0.12388516841408961\n",
            "train loss:0.13894277608638403\n",
            "train loss:0.05739682237612729\n",
            "train loss:0.09141976910818136\n",
            "train loss:0.11994194328426147\n",
            "train loss:0.03321819254130483\n",
            "train loss:0.18691796817176495\n",
            "train loss:0.03499069732305562\n",
            "train loss:0.03511039980137985\n",
            "train loss:0.06913414367241169\n",
            "train loss:0.04416914730214101\n",
            "train loss:0.03389265576770526\n",
            "train loss:0.06965178933004093\n",
            "train loss:0.06285133529677292\n",
            "train loss:0.023407052767161665\n",
            "train loss:0.14195394974133915\n",
            "train loss:0.04140062894716045\n",
            "train loss:0.019174771877008615\n",
            "train loss:0.050139338096579014\n",
            "train loss:0.046696501227995145\n",
            "train loss:0.049761342677871995\n",
            "train loss:0.04869286023709362\n",
            "train loss:0.10145722627085345\n",
            "train loss:0.05237288821950635\n",
            "train loss:0.0577523568486836\n",
            "train loss:0.026109468162227988\n",
            "train loss:0.05302157328136025\n",
            "train loss:0.06077270390421332\n",
            "train loss:0.037496159574887986\n",
            "train loss:0.10425427863285322\n",
            "train loss:0.07223353304967967\n",
            "train loss:0.023089419953840267\n",
            "train loss:0.01953964972058367\n",
            "train loss:0.11424865074437239\n",
            "train loss:0.24644026223258947\n",
            "train loss:0.06690143602473647\n",
            "train loss:0.04287364055004431\n",
            "train loss:0.0327854325862969\n",
            "train loss:0.04033092064324689\n",
            "train loss:0.04988350906161436\n",
            "train loss:0.03948093206635099\n",
            "train loss:0.1016265577165079\n",
            "train loss:0.018348442154328418\n",
            "train loss:0.02564359687301885\n",
            "train loss:0.06765445562326017\n",
            "train loss:0.03414098607835629\n",
            "train loss:0.026080633966405812\n",
            "=== epoch:5, train acc:0.979, test acc:0.956 ===\n",
            "train loss:0.055579822513475284\n",
            "train loss:0.0788615966060038\n",
            "train loss:0.03461349863723493\n",
            "train loss:0.016108743358870744\n",
            "train loss:0.057561362187247445\n",
            "train loss:0.04101997619012812\n",
            "train loss:0.0832857377442569\n",
            "train loss:0.07427652297468701\n",
            "train loss:0.04694501583769626\n",
            "train loss:0.044486736363445765\n",
            "train loss:0.032115548415038934\n",
            "train loss:0.04292567604276714\n",
            "train loss:0.05004228434352692\n",
            "train loss:0.03413976182737912\n",
            "train loss:0.019560529812718178\n",
            "train loss:0.036496966159089095\n",
            "train loss:0.051880552934797895\n",
            "train loss:0.015064510076355604\n",
            "train loss:0.020557901475145145\n",
            "train loss:0.06400275238748568\n",
            "train loss:0.05712246608874102\n",
            "train loss:0.037141663289462105\n",
            "train loss:0.041232281541899986\n",
            "train loss:0.08293213658573739\n",
            "train loss:0.033616673226281375\n",
            "train loss:0.019374265834092894\n",
            "train loss:0.024401651484467955\n",
            "train loss:0.12925857548883424\n",
            "train loss:0.016402810738036913\n",
            "train loss:0.03741013891575617\n",
            "train loss:0.03811026180120268\n",
            "train loss:0.019699337167380707\n",
            "train loss:0.030377770497943216\n",
            "train loss:0.04646780875332299\n",
            "train loss:0.02461540370389153\n",
            "train loss:0.0439759682360283\n",
            "train loss:0.01420885985432423\n",
            "train loss:0.016650105050858212\n",
            "train loss:0.0345481627141443\n",
            "train loss:0.025453678110903457\n",
            "train loss:0.019335590098455094\n",
            "train loss:0.012198332991016298\n",
            "train loss:0.04732263457432313\n",
            "train loss:0.07782091145892368\n",
            "train loss:0.05324816250049118\n",
            "train loss:0.01785871457003486\n",
            "train loss:0.026963019050148532\n",
            "train loss:0.024342048449380135\n",
            "train loss:0.0569956899881027\n",
            "train loss:0.009117076811989096\n",
            "=== epoch:6, train acc:0.979, test acc:0.963 ===\n",
            "train loss:0.03589330096513629\n",
            "train loss:0.029019042246758362\n",
            "train loss:0.028952761947598153\n",
            "train loss:0.05939904924294799\n",
            "train loss:0.017970537896091557\n",
            "train loss:0.046708307210241135\n",
            "train loss:0.059868366518403035\n",
            "train loss:0.06814499701608324\n",
            "train loss:0.014056058768388469\n",
            "train loss:0.10562105785494018\n",
            "train loss:0.06424653609297068\n",
            "train loss:0.015919695208492606\n",
            "train loss:0.023279419092251653\n",
            "train loss:0.036164362574820814\n",
            "train loss:0.029849335118300326\n",
            "train loss:0.023663961317862577\n",
            "train loss:0.01974122063144307\n",
            "train loss:0.039380065230242116\n",
            "train loss:0.01812919275860424\n",
            "train loss:0.014856673349399392\n",
            "train loss:0.015896335547729967\n",
            "train loss:0.004656305214391609\n",
            "train loss:0.042201472585490354\n",
            "train loss:0.02001307645278216\n",
            "train loss:0.029435159750242086\n",
            "train loss:0.041694599391654596\n",
            "train loss:0.008213646973212873\n",
            "train loss:0.05141461035661334\n",
            "train loss:0.0395427952618715\n",
            "train loss:0.04119269758278856\n",
            "train loss:0.02328677045157982\n",
            "train loss:0.03482579010473034\n",
            "train loss:0.047489415245648876\n",
            "train loss:0.014813945912743088\n",
            "train loss:0.030528403786538854\n",
            "train loss:0.022310942540523588\n",
            "train loss:0.006876380449121378\n",
            "train loss:0.03268027360291027\n",
            "train loss:0.055772121359060504\n",
            "train loss:0.02869999811538952\n",
            "train loss:0.0759047557709495\n",
            "train loss:0.023193910397676376\n",
            "train loss:0.02384172867335439\n",
            "train loss:0.015746318438929763\n",
            "train loss:0.02398401011746202\n",
            "train loss:0.034316623949927905\n",
            "train loss:0.03306349980076743\n",
            "train loss:0.02159623236966533\n",
            "train loss:0.03643191812246922\n",
            "train loss:0.024447895464051093\n",
            "=== epoch:7, train acc:0.985, test acc:0.967 ===\n",
            "train loss:0.03241598531685233\n",
            "train loss:0.031034617273391896\n",
            "train loss:0.011122335242286271\n",
            "train loss:0.03679883305798628\n",
            "train loss:0.010050606922165337\n",
            "train loss:0.0177911793615481\n",
            "train loss:0.03091224840786657\n",
            "train loss:0.015795446060489927\n",
            "train loss:0.044260530546871645\n",
            "train loss:0.0456805389279642\n",
            "train loss:0.03291795258549766\n",
            "train loss:0.014130415486001939\n",
            "train loss:0.009178485060592885\n",
            "train loss:0.006978549439168485\n",
            "train loss:0.014782299853434454\n",
            "train loss:0.04229180330977081\n",
            "train loss:0.01702281306575251\n",
            "train loss:0.025886201174106577\n",
            "train loss:0.015224546633744623\n",
            "train loss:0.006583235074140459\n",
            "train loss:0.02353557485901732\n",
            "train loss:0.007255525863472882\n",
            "train loss:0.007509032628531057\n",
            "train loss:0.04765597120710072\n",
            "train loss:0.018638898321546627\n",
            "train loss:0.07309892066992274\n",
            "train loss:0.016418055447287482\n",
            "train loss:0.01021835821526332\n",
            "train loss:0.018693187298513735\n",
            "train loss:0.016224122113242978\n",
            "train loss:0.013138633406001769\n",
            "train loss:0.035367276328641346\n",
            "train loss:0.027988368504987034\n",
            "train loss:0.011419590410267048\n",
            "train loss:0.02969596998832897\n",
            "train loss:0.025728132685557594\n",
            "train loss:0.028570897382312527\n",
            "train loss:0.02075130471368511\n",
            "train loss:0.025567637063903505\n",
            "train loss:0.030664981824906862\n",
            "train loss:0.02563988472314025\n",
            "train loss:0.018703375000571443\n",
            "train loss:0.08015089906006612\n",
            "train loss:0.021480648832853198\n",
            "train loss:0.005120994465101472\n",
            "train loss:0.005809820288766203\n",
            "train loss:0.024495386137012104\n",
            "train loss:0.01399471529065822\n",
            "train loss:0.018549450724883516\n",
            "train loss:0.016429367074529966\n",
            "=== epoch:8, train acc:0.991, test acc:0.966 ===\n",
            "train loss:0.014288906409802772\n",
            "train loss:0.01227746900641345\n",
            "train loss:0.024986337626986897\n",
            "train loss:0.016438253993822746\n",
            "train loss:0.01533518257046861\n",
            "train loss:0.015055831544027438\n",
            "train loss:0.010916936438773697\n",
            "train loss:0.012796083268508906\n",
            "train loss:0.04319637202994381\n",
            "train loss:0.00963156251972656\n",
            "train loss:0.00650604099619059\n",
            "train loss:0.010792941218656895\n",
            "train loss:0.021305990393180416\n",
            "train loss:0.02235863335928066\n",
            "train loss:0.014264078135467317\n",
            "train loss:0.017062313228899385\n",
            "train loss:0.01356195765448618\n",
            "train loss:0.016954019999130383\n",
            "train loss:0.015858260424199678\n",
            "train loss:0.00837483085320181\n",
            "train loss:0.015030896718434335\n",
            "train loss:0.006467055967732458\n",
            "train loss:0.0059050157087021425\n",
            "train loss:0.012139862292248449\n",
            "train loss:0.0039026219775711414\n",
            "train loss:0.023105184056564417\n",
            "train loss:0.01979992369709872\n",
            "train loss:0.028166824217083702\n",
            "train loss:0.005935257426953754\n",
            "train loss:0.012097212399961508\n",
            "train loss:0.004870049151907669\n",
            "train loss:0.011416185221112122\n",
            "train loss:0.013790659412703085\n",
            "train loss:0.024257711176240527\n",
            "train loss:0.02077597332136498\n",
            "train loss:0.013346471168992624\n",
            "train loss:0.043198924097914544\n",
            "train loss:0.00756537566495438\n",
            "train loss:0.0071651030381487514\n",
            "train loss:0.04160776533125188\n",
            "train loss:0.00487394031530391\n",
            "train loss:0.01610096624728862\n",
            "train loss:0.01357118328411219\n",
            "train loss:0.03561613668491312\n",
            "train loss:0.006558512827023299\n",
            "train loss:0.04759874054911947\n",
            "train loss:0.010169197702009376\n",
            "train loss:0.036072708107013324\n",
            "train loss:0.0036845933675818997\n",
            "train loss:0.01542748899405037\n",
            "=== epoch:9, train acc:0.986, test acc:0.963 ===\n",
            "train loss:0.0026375244392331716\n",
            "train loss:0.0023677591279847444\n",
            "train loss:0.007787935364191977\n",
            "train loss:0.01878488971387063\n",
            "train loss:0.01663847047481535\n",
            "train loss:0.005763427421211695\n",
            "train loss:0.023875296699195386\n",
            "train loss:0.022555381668676665\n",
            "train loss:0.012578009820077308\n",
            "train loss:0.004234297745601463\n",
            "train loss:0.006343781327602659\n",
            "train loss:0.013125229329627725\n",
            "train loss:0.020013905519447858\n",
            "train loss:0.006039543833834422\n",
            "train loss:0.004957773772819537\n",
            "train loss:0.009770859150936617\n",
            "train loss:0.01487878225342795\n",
            "train loss:0.005166212198571245\n",
            "train loss:0.005509755021636279\n",
            "train loss:0.015320513581944637\n",
            "train loss:0.006329943392249651\n",
            "train loss:0.016381227804367734\n",
            "train loss:0.017913746186984186\n",
            "train loss:0.015240448606317132\n",
            "train loss:0.003864893776982517\n",
            "train loss:0.009768632565111669\n",
            "train loss:0.02663327062253086\n",
            "train loss:0.008036229743241717\n",
            "train loss:0.01246301101476449\n",
            "train loss:0.009363942846659416\n",
            "train loss:0.01079378810397429\n",
            "train loss:0.0061894848211009705\n",
            "train loss:0.007594617875559943\n",
            "train loss:0.012966702032515294\n",
            "train loss:0.014793876767137852\n",
            "train loss:0.004071653340623708\n",
            "train loss:0.012488271765120771\n",
            "train loss:0.005057727586697975\n",
            "train loss:0.009812608681861122\n",
            "train loss:0.022419555908264034\n",
            "train loss:0.010051747697179782\n",
            "train loss:0.011859951910847336\n",
            "train loss:0.001990651431385907\n",
            "train loss:0.004189530327713891\n",
            "train loss:0.003879989099288348\n",
            "train loss:0.015566306146649434\n",
            "train loss:0.005542504843313709\n",
            "train loss:0.011762394061809976\n",
            "train loss:0.002376091967075027\n",
            "train loss:0.012823702198546895\n",
            "=== epoch:10, train acc:0.993, test acc:0.954 ===\n",
            "train loss:0.009405670931578368\n",
            "train loss:0.008313505490673506\n",
            "train loss:0.008202990329513991\n",
            "train loss:0.005308142022163373\n",
            "train loss:0.0062452282202037305\n",
            "train loss:0.027722266615958783\n",
            "train loss:0.005172642746664041\n",
            "train loss:0.00761258190443078\n",
            "train loss:0.006403687159697257\n",
            "train loss:0.01369272094862359\n",
            "train loss:0.0069541082389368205\n",
            "train loss:0.024878881231573113\n",
            "train loss:0.004580603406871044\n",
            "train loss:0.007727500053863075\n",
            "train loss:0.004724084781910443\n",
            "train loss:0.0034249886731932415\n",
            "train loss:0.002393124826479421\n",
            "train loss:0.020751075088209118\n",
            "train loss:0.019118220511702196\n",
            "train loss:0.009117413095249174\n",
            "train loss:0.005172531301875984\n",
            "train loss:0.0046475729666487905\n",
            "train loss:0.009379464774915337\n",
            "train loss:0.00966791422338759\n",
            "train loss:0.0054314245364684025\n",
            "train loss:0.002471922101970306\n",
            "train loss:0.011123920761373767\n",
            "train loss:0.016131965766248615\n",
            "train loss:0.004251645949560872\n",
            "train loss:0.005898770709428308\n",
            "train loss:0.014185148611312171\n",
            "train loss:0.003924112500837314\n",
            "train loss:0.005169174274036739\n",
            "train loss:0.0028901846904611757\n",
            "train loss:0.011182094733879233\n",
            "train loss:0.005421562120563363\n",
            "train loss:0.027826614338083613\n",
            "train loss:0.01013509067250733\n",
            "train loss:0.026726112443721316\n",
            "train loss:0.010361257978888141\n",
            "train loss:0.011215564630153661\n",
            "train loss:0.005372940950541211\n",
            "train loss:0.005088936010141565\n",
            "train loss:0.009557765567770998\n",
            "train loss:0.012419933965083271\n",
            "train loss:0.012262438412527991\n",
            "train loss:0.008867805055794445\n",
            "train loss:0.010616698115978674\n",
            "train loss:0.025519933371155677\n",
            "train loss:0.011600606049237848\n",
            "=== epoch:11, train acc:0.989, test acc:0.96 ===\n",
            "train loss:0.008394390258042969\n",
            "train loss:0.0015197844601052746\n",
            "train loss:0.006724953410359672\n",
            "train loss:0.02645681857625529\n",
            "train loss:0.016490289527699604\n",
            "train loss:0.003557802013605749\n",
            "train loss:0.005325579159638168\n",
            "train loss:0.011383311683123523\n",
            "train loss:0.012009861867045198\n",
            "train loss:0.02199265205019719\n",
            "train loss:0.017962511298899655\n",
            "train loss:0.007911712525771728\n",
            "train loss:0.021598213906746953\n",
            "train loss:0.00925715441141827\n",
            "train loss:0.0049875420166019275\n",
            "train loss:0.004496291242657421\n",
            "train loss:0.005408642346521818\n",
            "train loss:0.00805055260498052\n",
            "train loss:0.0033898924121469496\n",
            "train loss:0.008079190470174942\n",
            "train loss:0.00442770588021318\n",
            "train loss:0.010146753670822885\n",
            "train loss:0.002581345844436314\n",
            "train loss:0.001849854329772039\n",
            "train loss:0.0015189678356086855\n",
            "train loss:0.004143815232946032\n",
            "train loss:0.0094579666737981\n",
            "train loss:0.006866508542826497\n",
            "train loss:0.003223446087429303\n",
            "train loss:0.006612925145871747\n",
            "train loss:0.0026485140021230446\n",
            "train loss:0.004145388654213259\n",
            "train loss:0.004588067370185861\n",
            "train loss:0.006274349389084532\n",
            "train loss:0.004717212831720479\n",
            "train loss:0.004467435007937867\n",
            "train loss:0.0017508104624466407\n",
            "train loss:0.0020442001520776694\n",
            "train loss:0.003302385805098964\n",
            "train loss:0.00614738326732315\n",
            "train loss:0.02466858885515907\n",
            "train loss:0.002449097113502691\n",
            "train loss:0.023798221053671628\n",
            "train loss:0.011057574413383106\n",
            "train loss:0.003473415849742528\n",
            "train loss:0.012686226432081489\n",
            "train loss:0.005592786891646574\n",
            "train loss:0.004505537891881798\n",
            "train loss:0.003493341473739295\n",
            "train loss:0.008400376319890773\n",
            "=== epoch:12, train acc:0.998, test acc:0.965 ===\n",
            "train loss:0.006389703261011488\n",
            "train loss:0.005433485205418741\n",
            "train loss:0.008331863617828715\n",
            "train loss:0.004682012410458496\n",
            "train loss:0.002042757059690912\n",
            "train loss:0.0014175325180969536\n",
            "train loss:0.012065697112133133\n",
            "train loss:0.0010038983477633997\n",
            "train loss:0.00426782225240218\n",
            "train loss:0.005245719781098484\n",
            "train loss:0.004513660303745899\n",
            "train loss:0.0018641342087209153\n",
            "train loss:0.0014635193240652956\n",
            "train loss:0.006523148537985904\n",
            "train loss:0.006391149945151685\n",
            "train loss:0.0025074196667795113\n",
            "train loss:0.002077976200361439\n",
            "train loss:0.0037941305018910337\n",
            "train loss:0.0018682232079912378\n",
            "train loss:0.004455251356110378\n",
            "train loss:0.00342582802119722\n",
            "train loss:0.0014570063132989647\n",
            "train loss:0.00468656689608317\n",
            "train loss:0.008999785805390454\n",
            "train loss:0.004101439846495593\n",
            "train loss:0.0031184632238099763\n",
            "train loss:0.0012950707793071095\n",
            "train loss:0.0028430570277348595\n",
            "train loss:0.01665675282499084\n",
            "train loss:0.001773664340089525\n",
            "train loss:0.0015146890663755672\n",
            "train loss:0.0009188943757794897\n",
            "train loss:0.002134233832559694\n",
            "train loss:0.0018937314850565876\n",
            "train loss:0.006014044543419481\n",
            "train loss:0.001617169014125012\n",
            "train loss:0.0016777392898953982\n",
            "train loss:0.001983844557993572\n",
            "train loss:0.009865449472388958\n",
            "train loss:0.003587574892628862\n",
            "train loss:0.0029882730978405643\n",
            "train loss:0.0026686736407462454\n",
            "train loss:0.0021225464152294636\n",
            "train loss:0.0009904021645964537\n",
            "train loss:0.009236750406801157\n",
            "train loss:0.0037266780263619553\n",
            "train loss:0.0012390336107690125\n",
            "train loss:0.0010252982846321688\n",
            "train loss:0.003517724457004958\n",
            "train loss:0.0016506670943217546\n",
            "=== epoch:13, train acc:0.997, test acc:0.97 ===\n",
            "train loss:0.0005410642547940142\n",
            "train loss:0.006480292838522594\n",
            "train loss:0.0009368433420369177\n",
            "train loss:0.0006948439028601944\n",
            "train loss:0.002149205442560075\n",
            "train loss:0.0014261209002991797\n",
            "train loss:0.0014377521634034345\n",
            "train loss:0.0014450388954252635\n",
            "train loss:0.00323347915037215\n",
            "train loss:0.0013212521957416503\n",
            "train loss:0.0016347724237536913\n",
            "train loss:0.0032040403884937597\n",
            "train loss:0.0011608322826436438\n",
            "train loss:0.0007491075306902787\n",
            "train loss:0.0009966802283106007\n",
            "train loss:0.0025288406618539784\n",
            "train loss:0.0015423228870469824\n",
            "train loss:0.002325020237050531\n",
            "train loss:0.002028608492873873\n",
            "train loss:0.0018137547309151354\n",
            "train loss:0.0013270708245639961\n",
            "train loss:0.000932862245493215\n",
            "train loss:0.001932819202791863\n",
            "train loss:0.002246798759017265\n",
            "train loss:0.0010063050243949614\n",
            "train loss:0.002469886391813679\n",
            "train loss:0.0034744370082340527\n",
            "train loss:0.0009199693960043124\n",
            "train loss:0.0010542534848841797\n",
            "train loss:0.0005418587065725943\n",
            "train loss:0.0016134172866602197\n",
            "train loss:0.0008819211092295422\n",
            "train loss:0.0013394529644158932\n",
            "train loss:0.0004967751255674123\n",
            "train loss:0.0005956297880369721\n",
            "train loss:0.0033479500576000247\n",
            "train loss:0.0025656292127428783\n",
            "train loss:0.0020181533992593465\n",
            "train loss:0.00441322962386379\n",
            "train loss:0.0004224576668604766\n",
            "train loss:0.0003732761635429595\n",
            "train loss:0.0009609675818240883\n",
            "train loss:0.0006983646270133656\n",
            "train loss:0.000598047504467583\n",
            "train loss:0.0035254855199990083\n",
            "train loss:0.0015885991581386497\n",
            "train loss:0.0013907674337393164\n",
            "train loss:0.002469798618346268\n",
            "train loss:0.0012263223536352205\n",
            "train loss:0.00043373648439122835\n",
            "=== epoch:14, train acc:1.0, test acc:0.97 ===\n",
            "train loss:0.0023998867947289722\n",
            "train loss:0.000966434531725783\n",
            "train loss:0.0008621649730875896\n",
            "train loss:0.0007080145012226179\n",
            "train loss:0.0007269995481290646\n",
            "train loss:0.0011328950570754172\n",
            "train loss:0.0004908321024415134\n",
            "train loss:0.0009337443421832154\n",
            "train loss:0.0011245980025751832\n",
            "train loss:0.0005880344852835262\n",
            "train loss:0.000602856550705126\n",
            "train loss:0.00032687342961338977\n",
            "train loss:0.0001980297639028087\n",
            "train loss:0.0006520465300773077\n",
            "train loss:0.002451575263377531\n",
            "train loss:0.000981733119261745\n",
            "train loss:0.0008050029075819592\n",
            "train loss:0.0016896165878054568\n",
            "train loss:0.001147256387757522\n",
            "train loss:0.000534013726897798\n",
            "train loss:0.0010470694854005829\n",
            "train loss:0.0005251047623412631\n",
            "train loss:0.002387185067665656\n",
            "train loss:0.001021715230025759\n",
            "train loss:0.0005263349878587222\n",
            "train loss:0.0008014944393910387\n",
            "train loss:0.000483221932801331\n",
            "train loss:0.0004132330712354231\n",
            "train loss:0.001465394778897639\n",
            "train loss:0.0009336412244001399\n",
            "train loss:0.000664460237710061\n",
            "train loss:0.0003515456349557606\n",
            "train loss:0.0016006951992475064\n",
            "train loss:0.0008507310535919528\n",
            "train loss:0.0023429939166028074\n",
            "train loss:0.00014205498723453706\n",
            "train loss:0.00043312939573965874\n",
            "train loss:0.0017164782769780547\n",
            "train loss:0.0012635873258211913\n",
            "train loss:0.0016813290803483346\n",
            "train loss:0.0007396599619426032\n",
            "train loss:0.0005483432531918238\n",
            "train loss:0.0007056406447706724\n",
            "train loss:0.0011817442156730757\n",
            "train loss:0.0006234046305418667\n",
            "train loss:0.0004478899182044925\n",
            "train loss:0.0024591186258588055\n",
            "train loss:0.0003272803672401907\n",
            "train loss:0.0006052668612957412\n",
            "train loss:0.0007355457029616524\n",
            "=== epoch:15, train acc:1.0, test acc:0.976 ===\n",
            "train loss:0.0009451047139115577\n",
            "train loss:0.00030353730159888583\n",
            "train loss:0.0005002666524392953\n",
            "train loss:0.0008891941315127164\n",
            "train loss:0.0022290955352805004\n",
            "train loss:0.0008782795979696834\n",
            "train loss:0.0007973043800503659\n",
            "train loss:0.0009146127325152563\n",
            "train loss:0.0011054991184207554\n",
            "train loss:0.0002364161997292906\n",
            "train loss:0.00036901272377112175\n",
            "train loss:0.0003336423499091905\n",
            "train loss:0.0007938904609109928\n",
            "train loss:0.000380418250333531\n",
            "train loss:0.0008290255709357181\n",
            "train loss:4.80877302238005e-05\n",
            "train loss:0.0007664473503301534\n",
            "train loss:0.0016712827408369888\n",
            "train loss:0.00022092024411793918\n",
            "train loss:0.00040874822365549626\n",
            "train loss:0.000559579244005583\n",
            "train loss:0.002268781773937657\n",
            "train loss:0.00016613701370575413\n",
            "train loss:0.0003291560654135457\n",
            "train loss:0.00012170159067046949\n",
            "train loss:0.0001918300511062712\n",
            "train loss:0.0005998191674137603\n",
            "train loss:0.0006472130702981464\n",
            "train loss:0.0008813997915194392\n",
            "train loss:0.00034826353363452407\n",
            "train loss:0.00028009447778416693\n",
            "train loss:0.0019788359214391875\n",
            "train loss:0.00019044079915504517\n",
            "train loss:0.00048686772510781183\n",
            "train loss:0.0008791112086939089\n",
            "train loss:0.0001743189030976541\n",
            "train loss:0.00021596590113577026\n",
            "train loss:0.0014223163036611681\n",
            "train loss:0.0002009745007744671\n",
            "train loss:0.00010644547299940829\n",
            "train loss:0.00019414329678206992\n",
            "train loss:0.0004164463479019956\n",
            "train loss:0.0012804758667907825\n",
            "train loss:0.0014160428131607175\n",
            "train loss:0.0006111814462263266\n",
            "train loss:0.0002804287510718687\n",
            "train loss:0.0008731248068781558\n",
            "train loss:0.00019551513642045694\n",
            "train loss:0.0005651402829186841\n",
            "train loss:0.000284929100285928\n",
            "=== epoch:16, train acc:1.0, test acc:0.976 ===\n",
            "train loss:0.00027860400933947023\n",
            "train loss:0.0010771873622482343\n",
            "train loss:0.00025705814909592224\n",
            "train loss:0.0008724290165111183\n",
            "train loss:0.0005436931494762633\n",
            "train loss:0.0012868612515726547\n",
            "train loss:0.0007006740250770359\n",
            "train loss:0.00031697868630230376\n",
            "train loss:0.00018852061163673206\n",
            "train loss:0.0008825825008104675\n",
            "train loss:0.0015809702170710772\n",
            "train loss:0.00026613840144773304\n",
            "train loss:0.00040820897407554935\n",
            "train loss:0.0007218900299389985\n",
            "train loss:0.0008189041226899\n",
            "train loss:0.00126859199778853\n",
            "train loss:0.0009926634208141934\n",
            "train loss:0.0011352699197221688\n",
            "train loss:0.0016796534218327683\n",
            "train loss:0.000660689390102942\n",
            "train loss:0.0006565568327356501\n",
            "train loss:0.00041369150684274133\n",
            "train loss:0.000654461401129352\n",
            "train loss:0.0016628622067496608\n",
            "train loss:0.0016271982509222665\n",
            "train loss:0.00028742853051363487\n",
            "train loss:0.0011214245482723938\n",
            "train loss:0.00038788883298985415\n",
            "train loss:0.00018296657155096932\n",
            "train loss:0.0003422484614651078\n",
            "train loss:0.0002933040552337884\n",
            "train loss:0.00021260672807424036\n",
            "train loss:0.0002935575566932866\n",
            "train loss:0.001503627234065703\n",
            "train loss:0.0007353103440475531\n",
            "train loss:0.000739354395529639\n",
            "train loss:0.0006550243202276753\n",
            "train loss:0.0011607528902434815\n",
            "train loss:0.0007486311840330011\n",
            "train loss:0.0003319770191456912\n",
            "train loss:0.0002881740685301825\n",
            "train loss:0.000737385993746817\n",
            "train loss:0.00021522745882703132\n",
            "train loss:0.0006581600454524571\n",
            "train loss:0.00033987490489612645\n",
            "train loss:0.00027705542941875556\n",
            "train loss:0.0006818154279674652\n",
            "train loss:0.000677671075000792\n",
            "train loss:0.0001744191477314124\n",
            "train loss:0.0005693163606136668\n",
            "=== epoch:17, train acc:1.0, test acc:0.977 ===\n",
            "train loss:0.00031406436760807177\n",
            "train loss:0.0011744801074133424\n",
            "train loss:0.000451164504269281\n",
            "train loss:0.0006208472555356166\n",
            "train loss:0.0006458892432085018\n",
            "train loss:0.00023146139031117783\n",
            "train loss:0.00020559842482901315\n",
            "train loss:0.0007494687763778551\n",
            "train loss:0.0006559076350591995\n",
            "train loss:0.000478359274410384\n",
            "train loss:0.0003277817842402164\n",
            "train loss:0.00037996975116516416\n",
            "train loss:0.0005287206193290544\n",
            "train loss:9.84076602707563e-05\n",
            "train loss:0.00020323772491586466\n",
            "train loss:0.00022324742460230144\n",
            "train loss:0.0007576627751405471\n",
            "train loss:0.0004206118438823765\n",
            "train loss:0.000495380533519905\n",
            "train loss:0.0004953566662964421\n",
            "train loss:0.0003087948349644503\n",
            "train loss:0.00020920905139376204\n",
            "train loss:0.00014081776855418704\n",
            "train loss:0.00013777956819097342\n",
            "train loss:0.00021577779933792197\n",
            "train loss:0.00016097970397869812\n",
            "train loss:0.0003066295137232535\n",
            "train loss:0.00012521702854184137\n",
            "train loss:0.000470459427552773\n",
            "train loss:0.00022312549661373328\n",
            "train loss:0.0002577396152388781\n",
            "train loss:0.00014544052033481833\n",
            "train loss:0.0001156208038308955\n",
            "train loss:0.00047103375012673486\n",
            "train loss:0.0002223926620598552\n",
            "train loss:0.0003859763333388605\n",
            "train loss:0.0001705259245266429\n",
            "train loss:0.00029205187533660077\n",
            "train loss:0.0005783008911464962\n",
            "train loss:0.0005465025463675029\n",
            "train loss:0.0003536406468709599\n",
            "train loss:0.0003249888230230008\n",
            "train loss:0.00036846971076685185\n",
            "train loss:0.000508527842510998\n",
            "train loss:0.0003445113733462703\n",
            "train loss:0.00016933078607754112\n",
            "train loss:0.00013954367750342682\n",
            "train loss:0.00016802606348478508\n",
            "train loss:0.00024362938009668233\n",
            "train loss:0.00014296284383475538\n",
            "=== epoch:18, train acc:1.0, test acc:0.974 ===\n",
            "train loss:0.00020534986090153657\n",
            "train loss:0.00020597137984901097\n",
            "train loss:0.0003069203160634514\n",
            "train loss:0.00014789992230957325\n",
            "train loss:0.00023824298443838563\n",
            "train loss:0.0013715402933357332\n",
            "train loss:0.0003414018700787247\n",
            "train loss:0.00026274662234582516\n",
            "train loss:8.553623648480632e-05\n",
            "train loss:0.00028036804885419117\n",
            "train loss:7.64779300448002e-05\n",
            "train loss:0.0005268328741708669\n",
            "train loss:0.00014448866964860747\n",
            "train loss:0.00014637280165717866\n",
            "train loss:0.0006769984585795907\n",
            "train loss:0.00020024204350980784\n",
            "train loss:0.0009282514089968989\n",
            "train loss:3.513160596469366e-05\n",
            "train loss:0.0003747187526319078\n",
            "train loss:0.00018854602305974977\n",
            "train loss:0.0002823685160988177\n",
            "train loss:0.00018314990749182576\n",
            "train loss:0.0007251488738323603\n",
            "train loss:0.0006528139591117548\n",
            "train loss:0.00013587569398107777\n",
            "train loss:0.00018082120558989313\n",
            "train loss:0.00021256321571623763\n",
            "train loss:0.00020803287940046497\n",
            "train loss:0.00012706785439053808\n",
            "train loss:0.00036939021576909377\n",
            "train loss:0.00018454188024286916\n",
            "train loss:0.00012042042391120104\n",
            "train loss:0.00020720518736310286\n",
            "train loss:0.0001611481342943732\n",
            "train loss:0.00035484038639424737\n",
            "train loss:9.440046717025909e-05\n",
            "train loss:0.00017484287236621502\n",
            "train loss:0.00012116310859286531\n",
            "train loss:0.00032242630469795004\n",
            "train loss:0.0008392102561374301\n",
            "train loss:0.00042529445132978973\n",
            "train loss:0.00031489693964752846\n",
            "train loss:0.0004255642494230032\n",
            "train loss:0.0006827781204690492\n",
            "train loss:0.00025655735946549825\n",
            "train loss:0.00033966129812041003\n",
            "train loss:0.0002476462122752214\n",
            "train loss:0.0002306479422769696\n",
            "train loss:0.00018512216697342022\n",
            "train loss:0.00043008422806310005\n",
            "=== epoch:19, train acc:1.0, test acc:0.974 ===\n",
            "train loss:0.0004515668154994261\n",
            "train loss:0.00012587360214629193\n",
            "train loss:0.00018108356610836468\n",
            "train loss:0.0004624401828495601\n",
            "train loss:0.00024103917959196193\n",
            "train loss:0.00018362836974842323\n",
            "train loss:0.00011827610813268541\n",
            "train loss:9.116502231568256e-05\n",
            "train loss:0.00011060100550569813\n",
            "train loss:5.009874364046406e-05\n",
            "train loss:8.990603188434352e-05\n",
            "train loss:0.0007676045904168136\n",
            "train loss:0.00019310786277227916\n",
            "train loss:0.00021530022617972624\n",
            "train loss:0.00019190507317439986\n",
            "train loss:0.0001871863575520281\n",
            "train loss:3.2552340349524574e-05\n",
            "train loss:0.00018540854284086985\n",
            "train loss:0.00031081348625953313\n",
            "train loss:0.00021023559386624822\n",
            "train loss:0.00015472834030022385\n",
            "train loss:0.0002926274034796919\n",
            "train loss:8.115291096558683e-05\n",
            "train loss:0.0008271443828602606\n",
            "train loss:0.00011132745236280927\n",
            "train loss:0.00027913211965790134\n",
            "train loss:9.886465897781438e-05\n",
            "train loss:0.00029611226198071513\n",
            "train loss:0.0003437993216708634\n",
            "train loss:0.0001964467990607887\n",
            "train loss:0.0006602077429784173\n",
            "train loss:0.0003438009099386009\n",
            "train loss:0.000280858617747061\n",
            "train loss:0.0002977547486191537\n",
            "train loss:0.00023906530409860692\n",
            "train loss:5.521533178012692e-05\n",
            "train loss:9.183740190160508e-05\n",
            "train loss:8.233608749747703e-05\n",
            "train loss:0.000197857105366343\n",
            "train loss:0.0002829089282275166\n",
            "train loss:0.0002992304626732592\n",
            "train loss:0.0003105954703442832\n",
            "train loss:0.0001548013621679184\n",
            "train loss:0.0001821457765978923\n",
            "train loss:0.0001492350842145167\n",
            "train loss:7.670698473747155e-05\n",
            "train loss:0.00015229932636936141\n",
            "train loss:0.0003451407663056525\n",
            "train loss:0.00016870021140091113\n",
            "train loss:0.00017390818280296045\n",
            "=== epoch:20, train acc:1.0, test acc:0.972 ===\n",
            "train loss:0.00020664644571171488\n",
            "train loss:0.0002925737529383678\n",
            "train loss:0.0001425652924728744\n",
            "train loss:7.983089699916268e-05\n",
            "train loss:0.00027098071030249996\n",
            "train loss:6.520271669622926e-05\n",
            "train loss:0.00021428211124213938\n",
            "train loss:0.000602470525323618\n",
            "train loss:0.00022411624367057805\n",
            "train loss:0.00016804347119411472\n",
            "train loss:0.00019552840704623476\n",
            "train loss:0.00015843568810255788\n",
            "train loss:8.391115414808141e-05\n",
            "train loss:0.00012278113842687438\n",
            "train loss:0.0001131200107963879\n",
            "train loss:0.0003523128938186808\n",
            "train loss:0.00033081746967281856\n",
            "train loss:0.000408944979059756\n",
            "train loss:4.788611526545309e-05\n",
            "train loss:0.00010191929710847497\n",
            "train loss:0.0005226724354726143\n",
            "train loss:9.604850851880848e-05\n",
            "train loss:0.00013452720558016095\n",
            "train loss:0.000242355478605665\n",
            "train loss:0.0001381666791294085\n",
            "train loss:0.0001228707692854438\n",
            "train loss:0.00021732902335728805\n",
            "train loss:0.00010437250991778192\n",
            "train loss:0.00012795101533671658\n",
            "train loss:6.899569807717078e-05\n",
            "train loss:0.00018961329378345724\n",
            "train loss:0.0004578610133815846\n",
            "train loss:0.00012515660428246553\n",
            "train loss:0.0003317120406196656\n",
            "train loss:0.00020798725373674147\n",
            "train loss:0.000400195904226503\n",
            "train loss:0.000257946593199365\n",
            "train loss:0.00033550319663665804\n",
            "train loss:0.0003009911374771434\n",
            "train loss:0.00015590678000398627\n",
            "train loss:0.00017668453772565134\n",
            "train loss:0.00024354419152048371\n",
            "train loss:0.00011112097028212137\n",
            "train loss:0.0002208174625222862\n",
            "train loss:0.00015486770131093575\n",
            "train loss:0.0002805141112062054\n",
            "train loss:0.00024822044454899907\n",
            "train loss:0.0003391381454044046\n",
            "train loss:0.000380193651710335\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.975\n",
            "Saved network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
        "                        conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs,\n",
        "                  mini_batch_size=100, optimizer='Adam', optimizer_param={'lr' : 0.004},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved network Parameters!\")\n",
        "\n",
        "markers = {'train' : 'o', 'test' : 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.2918607493165846\n",
            "=== epoch:1, train acc:0.142, test acc:0.14 ===\n",
            "train loss:2.2692918810752154\n",
            "train loss:2.25144489619273\n",
            "train loss:2.1555144868895284\n",
            "train loss:2.1302221627554463\n",
            "train loss:2.0391117407921944\n",
            "train loss:1.907447090294851\n",
            "train loss:1.7727733485174249\n",
            "train loss:1.584222314407547\n",
            "train loss:1.3690128705797318\n",
            "train loss:1.1917598407468828\n",
            "train loss:1.0348365537869795\n",
            "train loss:1.0339249971214672\n",
            "train loss:0.7922699210689274\n",
            "train loss:0.8322207491738797\n",
            "train loss:0.6338490353506121\n",
            "train loss:0.5801788812558262\n",
            "train loss:0.4907012494250458\n",
            "train loss:0.645747162269722\n",
            "train loss:0.5161284922255357\n",
            "train loss:0.7414527757549986\n",
            "train loss:0.6235161594015104\n",
            "train loss:0.6971575549474889\n",
            "train loss:0.6161317020906548\n",
            "train loss:0.6496934952403702\n",
            "train loss:0.4255287862680282\n",
            "train loss:0.5235911766296583\n",
            "train loss:0.4855767962653704\n",
            "train loss:0.31141174350529893\n",
            "train loss:0.6153310054604175\n",
            "train loss:0.3914782611702542\n",
            "train loss:0.46085685084449124\n",
            "train loss:0.5320342267792942\n",
            "train loss:0.30926373245109284\n",
            "train loss:0.4400057229022589\n",
            "train loss:0.5069823352770156\n",
            "train loss:0.3471749160548416\n",
            "train loss:0.4373053130654292\n",
            "train loss:0.541557868999495\n",
            "train loss:0.4843532052031936\n",
            "train loss:0.3874874258560122\n",
            "train loss:0.48872306157892625\n",
            "train loss:0.5165283740394662\n",
            "train loss:0.4979931729487891\n",
            "train loss:0.5510677378811034\n",
            "train loss:0.40607946263237993\n",
            "train loss:0.3387285539313433\n",
            "train loss:0.3710526345383514\n",
            "train loss:0.501950402963912\n",
            "train loss:0.3342498859067205\n",
            "train loss:0.24662235979810582\n",
            "=== epoch:2, train acc:0.882, test acc:0.882 ===\n",
            "train loss:0.31031782767631855\n",
            "train loss:0.27628795211246243\n",
            "train loss:0.33437048255226354\n",
            "train loss:0.455887905060002\n",
            "train loss:0.4378240193872081\n",
            "train loss:0.24338206859630684\n",
            "train loss:0.4606372304418836\n",
            "train loss:0.2762453161871518\n",
            "train loss:0.22967904348394275\n",
            "train loss:0.26966562092437185\n",
            "train loss:0.31722503369128635\n",
            "train loss:0.4414888658773759\n",
            "train loss:0.25507900285116764\n",
            "train loss:0.2029274258172371\n",
            "train loss:0.2616391684643039\n",
            "train loss:0.1889197071832627\n",
            "train loss:0.2211713709796976\n",
            "train loss:0.39622039094437556\n",
            "train loss:0.2932589724733423\n",
            "train loss:0.2193559830777082\n",
            "train loss:0.3474035851152681\n",
            "train loss:0.22484082186543403\n",
            "train loss:0.1449979546635892\n",
            "train loss:0.18337053327622377\n",
            "train loss:0.3052737191736562\n",
            "train loss:0.31155701891218385\n",
            "train loss:0.28811314203421134\n",
            "train loss:0.3046123270196255\n",
            "train loss:0.1967876828916251\n",
            "train loss:0.19699382776479862\n",
            "train loss:0.15775213989312428\n",
            "train loss:0.20743109812421626\n",
            "train loss:0.249214426030035\n",
            "train loss:0.18667169172495326\n",
            "train loss:0.13137379020526396\n",
            "train loss:0.28230118609714827\n",
            "train loss:0.17206060581819005\n",
            "train loss:0.10525736480471044\n",
            "train loss:0.13942950226517217\n",
            "train loss:0.22981967048585872\n",
            "train loss:0.2286644382840193\n",
            "train loss:0.22105830013540548\n",
            "train loss:0.3344794102872404\n",
            "train loss:0.22226803339601242\n",
            "train loss:0.1993817272703047\n",
            "train loss:0.16264167324437614\n",
            "train loss:0.11836497479879995\n",
            "train loss:0.33366159932732026\n",
            "train loss:0.20886238071462684\n",
            "train loss:0.2841340152216805\n",
            "=== epoch:3, train acc:0.909, test acc:0.908 ===\n",
            "train loss:0.2302917252884051\n",
            "train loss:0.1774669884846359\n",
            "train loss:0.3148297611635479\n",
            "train loss:0.274818320286663\n",
            "train loss:0.267873919413751\n",
            "train loss:0.12974055443659951\n",
            "train loss:0.2548980133974207\n",
            "train loss:0.24454631189079823\n",
            "train loss:0.1853041751957337\n",
            "train loss:0.17549570931805725\n",
            "train loss:0.23576937421317784\n",
            "train loss:0.11379954249641201\n",
            "train loss:0.221177075088262\n",
            "train loss:0.18534620949951586\n",
            "train loss:0.28715524303503187\n",
            "train loss:0.14589613900612275\n",
            "train loss:0.21772641725789374\n",
            "train loss:0.18327675976642305\n",
            "train loss:0.2335288106460199\n",
            "train loss:0.13567742505982525\n",
            "train loss:0.23319546443594175\n",
            "train loss:0.15016149059715794\n",
            "train loss:0.15609336927948683\n",
            "train loss:0.18443708411266158\n",
            "train loss:0.14099127532696804\n",
            "train loss:0.10436636613941624\n",
            "train loss:0.17078804183327423\n",
            "train loss:0.09930976832242815\n",
            "train loss:0.2012864158727279\n",
            "train loss:0.16727583019420045\n",
            "train loss:0.171072327701071\n",
            "train loss:0.1799912196437851\n",
            "train loss:0.13159885864730547\n",
            "train loss:0.1448536800698845\n",
            "train loss:0.13875297046834237\n",
            "train loss:0.06623870873868432\n",
            "train loss:0.1325440580237816\n",
            "train loss:0.08128074548330932\n",
            "train loss:0.16082485814029643\n",
            "train loss:0.13424898722969997\n",
            "train loss:0.24364279709135675\n",
            "train loss:0.13339153310233315\n",
            "train loss:0.0701153938637088\n",
            "train loss:0.11459542158135895\n",
            "train loss:0.1404843450997511\n",
            "train loss:0.16405214321642858\n",
            "train loss:0.06486992819536815\n",
            "train loss:0.11306890734055021\n",
            "train loss:0.1429529641622459\n",
            "train loss:0.0662254196469306\n",
            "=== epoch:4, train acc:0.956, test acc:0.937 ===\n",
            "train loss:0.044104883954636034\n",
            "train loss:0.11918053219405944\n",
            "train loss:0.0871745576949627\n",
            "train loss:0.11085672423354467\n",
            "train loss:0.1313222932747681\n",
            "train loss:0.13749001480542636\n",
            "train loss:0.1092548751617788\n",
            "train loss:0.07294486088262231\n",
            "train loss:0.10056273106518024\n",
            "train loss:0.19055686623710824\n",
            "train loss:0.10872375335966764\n",
            "train loss:0.11007252325098073\n",
            "train loss:0.047982359537369935\n",
            "train loss:0.07129116136974305\n",
            "train loss:0.05638354713579265\n",
            "train loss:0.1573075633193359\n",
            "train loss:0.04801682380578442\n",
            "train loss:0.13028677867258556\n",
            "train loss:0.1856183754235371\n",
            "train loss:0.11449919058437347\n",
            "train loss:0.04893110453538836\n",
            "train loss:0.05704478198992814\n",
            "train loss:0.10246510817362722\n",
            "train loss:0.05648041003192636\n",
            "train loss:0.09808732640647858\n",
            "train loss:0.06614225081244379\n",
            "train loss:0.08344966319218862\n",
            "train loss:0.23211297121175314\n",
            "train loss:0.034710598185680695\n",
            "train loss:0.08840586011541347\n",
            "train loss:0.12018767694174062\n",
            "train loss:0.14280439156394115\n",
            "train loss:0.07879472101275356\n",
            "train loss:0.11996514354850607\n",
            "train loss:0.054040464076040226\n",
            "train loss:0.06821252767826615\n",
            "train loss:0.0983187119851317\n",
            "train loss:0.14278926124689645\n",
            "train loss:0.11834259043875406\n",
            "train loss:0.04665295529251228\n",
            "train loss:0.045581979327245274\n",
            "train loss:0.07037661077655283\n",
            "train loss:0.10148026208415852\n",
            "train loss:0.12224658811390098\n",
            "train loss:0.10363311080865317\n",
            "train loss:0.10590099864627599\n",
            "train loss:0.14412290687077783\n",
            "train loss:0.06613454746850989\n",
            "train loss:0.1287274894747484\n",
            "train loss:0.06691046933445137\n",
            "=== epoch:5, train acc:0.961, test acc:0.942 ===\n",
            "train loss:0.05301608534765112\n",
            "train loss:0.17791615398434119\n",
            "train loss:0.08413197197716654\n",
            "train loss:0.0844225597449752\n",
            "train loss:0.1183201350311993\n",
            "train loss:0.1751305641225246\n",
            "train loss:0.04912543292878875\n",
            "train loss:0.07107500059310572\n",
            "train loss:0.08030009978019796\n",
            "train loss:0.11083881984078602\n",
            "train loss:0.0798935991089091\n",
            "train loss:0.048474581986478256\n",
            "train loss:0.11491586729880089\n",
            "train loss:0.054550293639847446\n",
            "train loss:0.06956902712095434\n",
            "train loss:0.08531824045994414\n",
            "train loss:0.052590895190223884\n",
            "train loss:0.03498016542107605\n",
            "train loss:0.05271129021224241\n",
            "train loss:0.03331119573064121\n",
            "train loss:0.1222180958907956\n",
            "train loss:0.06582664394730646\n",
            "train loss:0.050354748205071784\n",
            "train loss:0.0637041459394761\n",
            "train loss:0.05432144309100982\n",
            "train loss:0.025650411666780264\n",
            "train loss:0.05235588543077919\n",
            "train loss:0.035678777055915434\n",
            "train loss:0.02897286822090363\n",
            "train loss:0.06190440849525628\n",
            "train loss:0.025615353024981743\n",
            "train loss:0.043074916420916265\n",
            "train loss:0.021937157757400953\n",
            "train loss:0.10558207149796464\n",
            "train loss:0.04002622808027325\n",
            "train loss:0.023936973479072598\n",
            "train loss:0.04135193473192671\n",
            "train loss:0.017612286096995097\n",
            "train loss:0.07722298637362661\n",
            "train loss:0.0637249420854232\n",
            "train loss:0.046167691423183305\n",
            "train loss:0.02627944339523196\n",
            "train loss:0.07908698596820957\n",
            "train loss:0.08687205259494292\n",
            "train loss:0.038653712996159384\n",
            "train loss:0.019918230160334102\n",
            "train loss:0.06805448417335447\n",
            "train loss:0.01924664919535239\n",
            "train loss:0.034891264905062676\n",
            "train loss:0.07475813673970994\n",
            "=== epoch:6, train acc:0.981, test acc:0.948 ===\n",
            "train loss:0.05775738899940543\n",
            "train loss:0.0994228564074475\n",
            "train loss:0.03841725507662613\n",
            "train loss:0.04345364017018141\n",
            "train loss:0.03802301544845622\n",
            "train loss:0.06394379929633687\n",
            "train loss:0.0804530864720204\n",
            "train loss:0.05380100829535868\n",
            "train loss:0.043071929581417014\n",
            "train loss:0.017531364628809058\n",
            "train loss:0.06933859776315549\n",
            "train loss:0.04593757453831863\n",
            "train loss:0.01790432936425367\n",
            "train loss:0.042847061242632434\n",
            "train loss:0.01903170771736295\n",
            "train loss:0.0480155365776974\n",
            "train loss:0.033009440703763965\n",
            "train loss:0.2574534352511895\n",
            "train loss:0.018570016740711008\n",
            "train loss:0.08347485942935842\n",
            "train loss:0.07520308729135075\n",
            "train loss:0.03567426033715438\n",
            "train loss:0.043581131508882896\n",
            "train loss:0.01885770498602479\n",
            "train loss:0.0908677906934667\n",
            "train loss:0.015873279985590006\n",
            "train loss:0.12039780962374177\n",
            "train loss:0.035650125631959476\n",
            "train loss:0.028145427626644536\n",
            "train loss:0.026747074963491185\n",
            "train loss:0.10227742723763819\n",
            "train loss:0.048898984180498546\n",
            "train loss:0.032435422444492136\n",
            "train loss:0.045252392874508496\n",
            "train loss:0.04715487524892239\n",
            "train loss:0.05244257262194159\n",
            "train loss:0.06091257738488142\n",
            "train loss:0.06197852551299955\n",
            "train loss:0.016747384972664855\n",
            "train loss:0.056933435285230934\n",
            "train loss:0.020791344337946854\n",
            "train loss:0.056096142665994365\n",
            "train loss:0.04815190889015471\n",
            "train loss:0.04912864491194147\n",
            "train loss:0.020803469894387066\n",
            "train loss:0.023077745811316865\n",
            "train loss:0.08742904021065055\n",
            "train loss:0.015486570184728526\n",
            "train loss:0.021197893191007875\n",
            "train loss:0.02235239558887121\n",
            "=== epoch:7, train acc:0.984, test acc:0.965 ===\n",
            "train loss:0.04321440432896231\n",
            "train loss:0.041076155017543414\n",
            "train loss:0.020131499575298344\n",
            "train loss:0.023613364533350324\n",
            "train loss:0.03778366578582315\n",
            "train loss:0.022220062666236953\n",
            "train loss:0.07517670164717585\n",
            "train loss:0.04808066100184125\n",
            "train loss:0.04347280580548557\n",
            "train loss:0.03735814964753044\n",
            "train loss:0.01388789714230143\n",
            "train loss:0.04914283363688515\n",
            "train loss:0.07242702971827214\n",
            "train loss:0.013560446554713414\n",
            "train loss:0.035366368292124854\n",
            "train loss:0.04881003134695972\n",
            "train loss:0.03244627938417062\n",
            "train loss:0.025587771139171826\n",
            "train loss:0.015341590318379674\n",
            "train loss:0.024208584629838062\n",
            "train loss:0.03980636452776978\n",
            "train loss:0.01834758468548333\n",
            "train loss:0.05402920536754571\n",
            "train loss:0.04714517904224869\n",
            "train loss:0.03843305465892337\n",
            "train loss:0.017678075253020106\n",
            "train loss:0.009966719140764304\n",
            "train loss:0.028611456671982848\n",
            "train loss:0.005818770102671618\n",
            "train loss:0.024414380037721766\n",
            "train loss:0.010886084635013688\n",
            "train loss:0.04172089129922013\n",
            "train loss:0.10037512738950045\n",
            "train loss:0.044774017536096035\n",
            "train loss:0.009960865245400675\n",
            "train loss:0.08815845597008025\n",
            "train loss:0.06318530317393758\n",
            "train loss:0.04017786133858242\n",
            "train loss:0.05014649546064324\n",
            "train loss:0.010459770690907906\n",
            "train loss:0.04538441046091382\n",
            "train loss:0.031160499205059127\n",
            "train loss:0.04067873730801591\n",
            "train loss:0.010668228820842129\n",
            "train loss:0.027650050892316348\n",
            "train loss:0.051203612463570795\n",
            "train loss:0.011316897103887662\n",
            "train loss:0.019407992991188294\n",
            "train loss:0.01609063392561498\n",
            "train loss:0.03171820624272077\n",
            "=== epoch:8, train acc:0.986, test acc:0.96 ===\n",
            "train loss:0.01612522500675659\n",
            "train loss:0.023967788141134306\n",
            "train loss:0.0200830156594748\n",
            "train loss:0.03286968463572126\n",
            "train loss:0.035119629203047245\n",
            "train loss:0.015629296914367683\n",
            "train loss:0.07825208424601864\n",
            "train loss:0.08544079269832121\n",
            "train loss:0.030592558360334948\n",
            "train loss:0.01768267578386986\n",
            "train loss:0.013963045972384357\n",
            "train loss:0.007482406788401642\n",
            "train loss:0.027262720221683415\n",
            "train loss:0.06693241755780982\n",
            "train loss:0.010507405694696966\n",
            "train loss:0.020559613957843823\n",
            "train loss:0.029484375554372645\n",
            "train loss:0.018178027939633357\n",
            "train loss:0.03402224922723094\n",
            "train loss:0.043964944966800486\n",
            "train loss:0.046452504879859254\n",
            "train loss:0.018119899666183187\n",
            "train loss:0.02158401650759452\n",
            "train loss:0.02358985777950928\n",
            "train loss:0.025235083181213428\n",
            "train loss:0.06377789049109146\n",
            "train loss:0.02355554594353082\n",
            "train loss:0.014113545614472488\n",
            "train loss:0.07376714486347016\n",
            "train loss:0.042184682579963974\n",
            "train loss:0.04382400825479315\n",
            "train loss:0.023611629838630452\n",
            "train loss:0.013911571674382806\n",
            "train loss:0.039764854943589265\n",
            "train loss:0.04155520496732686\n",
            "train loss:0.03416348050648102\n",
            "train loss:0.022521335097411766\n",
            "train loss:0.035768971954317615\n",
            "train loss:0.007538130940314163\n",
            "train loss:0.014121848410806239\n",
            "train loss:0.024824559892889987\n",
            "train loss:0.015181529047897551\n",
            "train loss:0.02759430566732768\n",
            "train loss:0.01207462797483677\n",
            "train loss:0.0190211397967635\n",
            "train loss:0.058254761389552503\n",
            "train loss:0.014175844131237516\n",
            "train loss:0.0324771513692127\n",
            "train loss:0.021820984957288153\n",
            "train loss:0.02187334093207441\n",
            "=== epoch:9, train acc:0.987, test acc:0.958 ===\n",
            "train loss:0.030233752546322608\n",
            "train loss:0.01548882184200469\n",
            "train loss:0.011219625510248086\n",
            "train loss:0.005044955604675942\n",
            "train loss:0.10095328199406345\n",
            "train loss:0.016264483186212726\n",
            "train loss:0.03989679171260792\n",
            "train loss:0.029002934852922987\n",
            "train loss:0.012156418281278624\n",
            "train loss:0.12869259980236944\n",
            "train loss:0.03947334583253277\n",
            "train loss:0.020481207815586128\n",
            "train loss:0.032037905953737705\n",
            "train loss:0.03192494805238564\n",
            "train loss:0.01748833758985641\n",
            "train loss:0.014214447186659122\n",
            "train loss:0.021670546482981386\n",
            "train loss:0.026082186037204552\n",
            "train loss:0.04140746034283229\n",
            "train loss:0.008048491982805318\n",
            "train loss:0.0546064825571448\n",
            "train loss:0.031714310771273374\n",
            "train loss:0.04356748833266551\n",
            "train loss:0.005687213888210942\n",
            "train loss:0.04732480212422345\n",
            "train loss:0.016811391812288703\n",
            "train loss:0.014718225594236065\n",
            "train loss:0.01981438470498558\n",
            "train loss:0.01967818792892488\n",
            "train loss:0.02012729446325865\n",
            "train loss:0.010766822814565855\n",
            "train loss:0.017264443583105072\n",
            "train loss:0.05962263475440326\n",
            "train loss:0.020280227875529567\n",
            "train loss:0.0062657438963865135\n",
            "train loss:0.026796670547023237\n",
            "train loss:0.030880881510242736\n",
            "train loss:0.02717561830798675\n",
            "train loss:0.022964841539753426\n",
            "train loss:0.012366419848490066\n",
            "train loss:0.0160918480792981\n",
            "train loss:0.020616279201626123\n",
            "train loss:0.02531780457197966\n",
            "train loss:0.014089718448002298\n",
            "train loss:0.019353326312142143\n",
            "train loss:0.025191750859899283\n",
            "train loss:0.010793941977461325\n",
            "train loss:0.015422029195189162\n",
            "train loss:0.02509746356382914\n",
            "train loss:0.016716971005364026\n",
            "=== epoch:10, train acc:0.994, test acc:0.96 ===\n",
            "train loss:0.03189575400976893\n",
            "train loss:0.01691921748397194\n",
            "train loss:0.016078527684078744\n",
            "train loss:0.008849868068862864\n",
            "train loss:0.010299763281312728\n",
            "train loss:0.009437052891118979\n",
            "train loss:0.019003575503822125\n",
            "train loss:0.012753922534817322\n",
            "train loss:0.021897062726382113\n",
            "train loss:0.009104345626086939\n",
            "train loss:0.0248655329344315\n",
            "train loss:0.01743106153381055\n",
            "train loss:0.011361343260344785\n",
            "train loss:0.012399749334683989\n",
            "train loss:0.006913222733329537\n",
            "train loss:0.010846907230974407\n",
            "train loss:0.01993736451641051\n",
            "train loss:0.003819641164443654\n",
            "train loss:0.013830335981497304\n",
            "train loss:0.01596708706122555\n",
            "train loss:0.053088787915622396\n",
            "train loss:0.0031012416467048913\n",
            "train loss:0.007522253045972534\n",
            "train loss:0.04920927495082835\n",
            "train loss:0.016659999369927313\n",
            "train loss:0.0057169734902181146\n",
            "train loss:0.007122113071392449\n",
            "train loss:0.013984401586904165\n",
            "train loss:0.0045897325899080235\n",
            "train loss:0.012838372806891202\n",
            "train loss:0.0030999734841707827\n",
            "train loss:0.010810086976664055\n",
            "train loss:0.010527812742427691\n",
            "train loss:0.011939947264068733\n",
            "train loss:0.0064936541471746455\n",
            "train loss:0.008066180401297402\n",
            "train loss:0.0049719765865569405\n",
            "train loss:0.009454173906322217\n",
            "train loss:0.018430936719880952\n",
            "train loss:0.008760346734719423\n",
            "train loss:0.003774663897692533\n",
            "train loss:0.006914343749452434\n",
            "train loss:0.011276311107161268\n",
            "train loss:0.004221550620567676\n",
            "train loss:0.004129533744786614\n",
            "train loss:0.030709240282319594\n",
            "train loss:0.008282224109819382\n",
            "train loss:0.003344550692936066\n",
            "train loss:0.007125513855618993\n",
            "train loss:0.00539439698206137\n",
            "=== epoch:11, train acc:0.997, test acc:0.964 ===\n",
            "train loss:0.004106237877768953\n",
            "train loss:0.0045182076018865005\n",
            "train loss:0.006833938076857668\n",
            "train loss:0.00871894633906115\n",
            "train loss:0.010176283200476387\n",
            "train loss:0.014379120590794154\n",
            "train loss:0.011621102444673152\n",
            "train loss:0.006175947765526713\n",
            "train loss:0.010286664501033813\n",
            "train loss:0.01083898460111466\n",
            "train loss:0.005021443193878205\n",
            "train loss:0.003785136032271058\n",
            "train loss:0.0039564832545648555\n",
            "train loss:0.01707965003351741\n",
            "train loss:0.0066244576850073575\n",
            "train loss:0.006835472152666083\n",
            "train loss:0.011216203243149352\n",
            "train loss:0.012673585734217046\n",
            "train loss:0.004507882268318282\n",
            "train loss:0.0029385317520485344\n",
            "train loss:0.02146860885141783\n",
            "train loss:0.012501188250716014\n",
            "train loss:0.004767381184924135\n",
            "train loss:0.0039007296527526136\n",
            "train loss:0.005988948472517181\n",
            "train loss:0.006952771791928857\n",
            "train loss:0.005100548661796496\n",
            "train loss:0.006289296466351873\n",
            "train loss:0.0075810817762618585\n",
            "train loss:0.004160295470730665\n",
            "train loss:0.004792151369922036\n",
            "train loss:0.010468514909969211\n",
            "train loss:0.007131948333674292\n",
            "train loss:0.007061335678812417\n",
            "train loss:0.008501696417757826\n",
            "train loss:0.009550224522464231\n",
            "train loss:0.008303496307323313\n",
            "train loss:0.008296215112851044\n",
            "train loss:0.005789498147151667\n",
            "train loss:0.01816557344315217\n",
            "train loss:0.009474064221242086\n",
            "train loss:0.006282331403083712\n",
            "train loss:0.005799172440962401\n",
            "train loss:0.03721254356510142\n",
            "train loss:0.01036556636943454\n",
            "train loss:0.0034871145403781857\n",
            "train loss:0.020902127369067377\n",
            "train loss:0.006501345277189101\n",
            "train loss:0.01572107840753504\n",
            "train loss:0.01086040656553473\n",
            "=== epoch:12, train acc:0.996, test acc:0.964 ===\n",
            "train loss:0.004816385128838836\n",
            "train loss:0.0052431794535801116\n",
            "train loss:0.0040083563932485355\n",
            "train loss:0.017769236028383248\n",
            "train loss:0.0026000826890754595\n",
            "train loss:0.004281740991840258\n",
            "train loss:0.014721775386355997\n",
            "train loss:0.0060816219492908375\n",
            "train loss:0.010976589268800921\n",
            "train loss:0.013108673635133699\n",
            "train loss:0.02229779271133298\n",
            "train loss:0.004178400169813054\n",
            "train loss:0.005469830783501648\n",
            "train loss:0.009096792678450498\n",
            "train loss:0.012324201309562988\n",
            "train loss:0.007885276700186116\n",
            "train loss:0.005819014382220211\n",
            "train loss:0.0038155093490700597\n",
            "train loss:0.0017376929219827487\n",
            "train loss:0.002827957138982383\n",
            "train loss:0.010937429832632115\n",
            "train loss:0.00819113601454256\n",
            "train loss:0.006141977563356619\n",
            "train loss:0.000883794398609813\n",
            "train loss:0.005291172135669656\n",
            "train loss:0.005857923391124683\n",
            "train loss:0.011196485523623447\n",
            "train loss:0.0043055358682062995\n",
            "train loss:0.00375084787584719\n",
            "train loss:0.005332851289984609\n",
            "train loss:0.006379955033938633\n",
            "train loss:0.01366049785492687\n",
            "train loss:0.011067231387901354\n",
            "train loss:0.07733741539697014\n",
            "train loss:0.006567404756744289\n",
            "train loss:0.00753483213992709\n",
            "train loss:0.008979450885431381\n",
            "train loss:0.006295840260012017\n",
            "train loss:0.006767123698899611\n",
            "train loss:0.004846448456399064\n",
            "train loss:0.005179400250160448\n",
            "train loss:0.022359980166510127\n",
            "train loss:0.011404199296246706\n",
            "train loss:0.004150327887180517\n",
            "train loss:0.006409885015806418\n",
            "train loss:0.010942501268449143\n",
            "train loss:0.0032076114937602336\n",
            "train loss:0.015332182760960555\n",
            "train loss:0.009744376459576492\n",
            "train loss:0.0018013147355843153\n",
            "=== epoch:13, train acc:0.998, test acc:0.962 ===\n",
            "train loss:0.0025333870483185733\n",
            "train loss:0.005493930454776999\n",
            "train loss:0.006060089028615818\n",
            "train loss:0.00310793906114583\n",
            "train loss:0.00698123006134529\n",
            "train loss:0.007525764375136417\n",
            "train loss:0.006638139221487944\n",
            "train loss:0.0027934226460608323\n",
            "train loss:0.029495094386141615\n",
            "train loss:0.014068419514310484\n",
            "train loss:0.0017523709708660922\n",
            "train loss:0.008628452486235595\n",
            "train loss:0.0022741062324298255\n",
            "train loss:0.004268163830562235\n",
            "train loss:0.004252994836909465\n",
            "train loss:0.002142251440437599\n",
            "train loss:0.008977006837775033\n",
            "train loss:0.07990821671763651\n",
            "train loss:0.007799847061947419\n",
            "train loss:0.014744805077913915\n",
            "train loss:0.0057333875075458165\n",
            "train loss:0.005665475830102287\n",
            "train loss:0.012312584511467868\n",
            "train loss:0.006725496505294617\n",
            "train loss:0.004227822772998323\n",
            "train loss:0.006175764921216126\n",
            "train loss:0.016521369941919984\n",
            "train loss:0.011205094706114886\n",
            "train loss:0.013754462328147954\n",
            "train loss:0.007422986860337213\n",
            "train loss:0.007329896431556896\n",
            "train loss:0.002631912148952372\n",
            "train loss:0.005590796243473573\n",
            "train loss:0.014685980030789662\n",
            "train loss:0.006788918323466902\n",
            "train loss:0.008324149093499397\n",
            "train loss:0.008862506776624209\n",
            "train loss:0.011850061084988754\n",
            "train loss:0.016240931950929162\n",
            "train loss:0.014646316749346648\n",
            "train loss:0.006532366588833658\n",
            "train loss:0.013711979212897327\n",
            "train loss:0.0050094717118605026\n",
            "train loss:0.010670866333613676\n",
            "train loss:0.004871470211047335\n",
            "train loss:0.008850197017790846\n",
            "train loss:0.00553592099923776\n",
            "train loss:0.005592192728922447\n",
            "train loss:0.00255993145102462\n",
            "train loss:0.0039562016953307915\n",
            "=== epoch:14, train acc:0.991, test acc:0.96 ===\n",
            "train loss:0.011057287175666801\n",
            "train loss:0.007632264523694004\n",
            "train loss:0.005775712723838366\n",
            "train loss:0.017359514404691165\n",
            "train loss:0.007097052834407872\n",
            "train loss:0.00791290382229107\n",
            "train loss:0.0029264785333650693\n",
            "train loss:0.008165598301812514\n",
            "train loss:0.006657701761577681\n",
            "train loss:0.0021324935095150843\n",
            "train loss:0.005003111841284066\n",
            "train loss:0.01058013829708802\n",
            "train loss:0.004832546769332071\n",
            "train loss:0.01868224125098691\n",
            "train loss:0.009393035968877705\n",
            "train loss:0.004744618886839851\n",
            "train loss:0.001486089784879677\n",
            "train loss:0.011291916051329088\n",
            "train loss:0.021205631949586003\n",
            "train loss:0.005283836370855325\n",
            "train loss:0.005285531532091328\n",
            "train loss:0.005398156694983497\n",
            "train loss:0.002522677158205204\n",
            "train loss:0.0139800454931554\n",
            "train loss:0.0022854799389297746\n",
            "train loss:0.0077237200009141\n",
            "train loss:0.027859624885933028\n",
            "train loss:0.0030763567445124783\n",
            "train loss:0.02449235931430822\n",
            "train loss:0.005488318543537454\n",
            "train loss:0.001566447289298449\n",
            "train loss:0.004632721089047128\n",
            "train loss:0.003128741443779442\n",
            "train loss:0.003604142859603025\n",
            "train loss:0.0027655923784807705\n",
            "train loss:0.006264167387288541\n",
            "train loss:0.015662860605962583\n",
            "train loss:0.008941083693825498\n",
            "train loss:0.00480283153057138\n",
            "train loss:0.0134078782590202\n",
            "train loss:0.0015892216077195256\n",
            "train loss:0.006627103761439491\n",
            "train loss:0.0051598495408194354\n",
            "train loss:0.0021939694798612215\n",
            "train loss:0.0045583911429414314\n",
            "train loss:0.028860009180953727\n",
            "train loss:0.00803235254038371\n",
            "train loss:0.004014377608412684\n",
            "train loss:0.0029100088416352072\n",
            "train loss:0.0008645483199709836\n",
            "=== epoch:15, train acc:0.998, test acc:0.973 ===\n",
            "train loss:0.0064854657274023885\n",
            "train loss:0.019463812483034998\n",
            "train loss:0.0035627020928718236\n",
            "train loss:0.013409699365342637\n",
            "train loss:0.024598686457041007\n",
            "train loss:0.014850973369418074\n",
            "train loss:0.014212622812637047\n",
            "train loss:0.006694614683951519\n",
            "train loss:0.002977846967296928\n",
            "train loss:0.0072286157613832705\n",
            "train loss:0.003159132340439503\n",
            "train loss:0.012034681567425211\n",
            "train loss:0.00496924325376181\n",
            "train loss:0.010345942470228506\n",
            "train loss:0.006113623253967263\n",
            "train loss:0.008858573849080566\n",
            "train loss:0.005651229806724901\n",
            "train loss:0.008074628729302407\n",
            "train loss:0.001547531618248338\n",
            "train loss:0.02373253032508063\n",
            "train loss:0.002550571423614383\n",
            "train loss:0.005236113351879934\n",
            "train loss:0.010172876735690459\n",
            "train loss:0.0013118483644835803\n",
            "train loss:0.016377949547315574\n",
            "train loss:0.0072212260550647426\n",
            "train loss:0.0031259081225720902\n",
            "train loss:0.0018609384114151475\n",
            "train loss:0.0028200427560750503\n",
            "train loss:0.0012451605548740197\n",
            "train loss:0.005290586630417584\n",
            "train loss:0.003296534695607667\n",
            "train loss:0.00331996217754684\n",
            "train loss:0.003376076295104371\n",
            "train loss:0.003649880737007275\n",
            "train loss:0.006387088682599502\n",
            "train loss:0.0027188311674896087\n",
            "train loss:0.007632496003058919\n",
            "train loss:0.0023861788297629173\n",
            "train loss:0.0034379105898986913\n",
            "train loss:0.006868768346638092\n",
            "train loss:0.0007088061553491425\n",
            "train loss:0.002146527593798752\n",
            "train loss:0.0036674439880356154\n",
            "train loss:0.004610007586046765\n",
            "train loss:0.0007331910981101425\n",
            "train loss:0.009078554180850877\n",
            "train loss:0.006496584674950176\n",
            "train loss:0.0006712406123278334\n",
            "train loss:0.004852136998927205\n",
            "=== epoch:16, train acc:0.998, test acc:0.976 ===\n",
            "train loss:0.0034534943769138453\n",
            "train loss:0.0013743387728615277\n",
            "train loss:0.006201504410151062\n",
            "train loss:0.0031173863525481575\n",
            "train loss:0.004883649846008978\n",
            "train loss:0.0006042611411870351\n",
            "train loss:0.0026560975319115964\n",
            "train loss:0.004749473643739447\n",
            "train loss:0.0022738520540685615\n",
            "train loss:0.0038695221835568132\n",
            "train loss:0.002235908157834106\n",
            "train loss:0.0017904157121900522\n",
            "train loss:0.0037162173610070914\n",
            "train loss:0.0014268020371051055\n",
            "train loss:0.0004404072655979641\n",
            "train loss:0.0007952699189860293\n",
            "train loss:0.00199122053864418\n",
            "train loss:0.0017511476146636485\n",
            "train loss:0.005987429172824843\n",
            "train loss:0.0005562699573764295\n",
            "train loss:0.00238772513039129\n",
            "train loss:0.0016475002738095823\n",
            "train loss:0.0023767291928725796\n",
            "train loss:0.0026851664978608187\n",
            "train loss:0.0010899379952458996\n",
            "train loss:0.0039041036046021317\n",
            "train loss:0.0009737610445566967\n",
            "train loss:0.008002698709164636\n",
            "train loss:0.007142684843691184\n",
            "train loss:0.003413983981198703\n",
            "train loss:0.0033311021878586478\n",
            "train loss:0.0017056248441639746\n",
            "train loss:0.00650243407343267\n",
            "train loss:0.0013054558867069519\n",
            "train loss:0.0044100692492418355\n",
            "train loss:0.0021515562058088715\n",
            "train loss:0.0015347586999306909\n",
            "train loss:0.011652800345348449\n",
            "train loss:0.0008998008967353614\n",
            "train loss:0.0007409957558156461\n",
            "train loss:0.00265209701375564\n",
            "train loss:0.0035985757846838705\n",
            "train loss:0.002845097692071799\n",
            "train loss:0.0021874567676457317\n",
            "train loss:0.003131092399434939\n",
            "train loss:0.0014418380295035127\n",
            "train loss:0.0038632803328221533\n",
            "train loss:0.003594698064183186\n",
            "train loss:0.0010123394865670572\n",
            "train loss:0.0011080266432313475\n",
            "=== epoch:17, train acc:1.0, test acc:0.974 ===\n",
            "train loss:0.00042094189760687336\n",
            "train loss:0.005668006206025879\n",
            "train loss:0.004753090376138061\n",
            "train loss:0.0014755768879445958\n",
            "train loss:0.0008428532002524021\n",
            "train loss:0.0018070615636594436\n",
            "train loss:0.002882984320883115\n",
            "train loss:0.0009829723318032515\n",
            "train loss:0.0014342648099670535\n",
            "train loss:0.001035165477818032\n",
            "train loss:0.0012872298779079258\n",
            "train loss:0.0022478487966010167\n",
            "train loss:0.0011834384004193637\n",
            "train loss:0.004561770633684962\n",
            "train loss:0.0037429391042685216\n",
            "train loss:0.0028392793335463794\n",
            "train loss:0.0034163677461792963\n",
            "train loss:0.0038765790257206337\n",
            "train loss:0.0021445063152925577\n",
            "train loss:0.0018004008999850146\n",
            "train loss:0.0017290595948677902\n",
            "train loss:0.001164485292470081\n",
            "train loss:0.002648314421261286\n",
            "train loss:0.003563897289112109\n",
            "train loss:0.0023658130594881614\n",
            "train loss:0.0005523405793885915\n",
            "train loss:0.004459693707996508\n",
            "train loss:0.0030082025320392284\n",
            "train loss:0.0028067474803857117\n",
            "train loss:0.0034029883292923395\n",
            "train loss:0.0018214904654204876\n",
            "train loss:0.004929076393622598\n",
            "train loss:0.0011816375687078464\n",
            "train loss:0.0010866869931142348\n",
            "train loss:0.004113211492089436\n",
            "train loss:0.0010207749573835111\n",
            "train loss:0.001857126343052433\n",
            "train loss:0.004065130812184107\n",
            "train loss:0.0039054664605230986\n",
            "train loss:0.008561875505472346\n",
            "train loss:0.0018297843050102014\n",
            "train loss:0.004173740905913681\n",
            "train loss:0.001604652092887623\n",
            "train loss:0.0021601706819145065\n",
            "train loss:0.0007335725561411928\n",
            "train loss:0.0009907907995090226\n",
            "train loss:0.004943385614753868\n",
            "train loss:0.004114748201427491\n",
            "train loss:0.0026588679806786686\n",
            "train loss:0.004711287651344365\n",
            "=== epoch:18, train acc:0.997, test acc:0.969 ===\n",
            "train loss:0.0007967296280909348\n",
            "train loss:0.006964307213653016\n",
            "train loss:0.0031661751169107673\n",
            "train loss:0.0009039385248286534\n",
            "train loss:0.0013509501514806996\n",
            "train loss:0.0007800141761373592\n",
            "train loss:0.003207373308572554\n",
            "train loss:0.0013451594378514726\n",
            "train loss:0.0039859841069236155\n",
            "train loss:0.0009007576445879513\n",
            "train loss:0.001409913257659663\n",
            "train loss:0.000948833307677848\n",
            "train loss:0.004127100428524008\n",
            "train loss:0.005185213749807681\n",
            "train loss:0.003667540716056173\n",
            "train loss:0.001008941156154788\n",
            "train loss:0.003312167365043532\n",
            "train loss:0.002125086094531439\n",
            "train loss:0.0037395896102697434\n",
            "train loss:0.002460511812805556\n",
            "train loss:0.005007152748555505\n",
            "train loss:0.0017357353837525217\n",
            "train loss:0.004622770088652781\n",
            "train loss:0.0011802837221501624\n",
            "train loss:0.0020276552385847055\n",
            "train loss:0.0006652716240726577\n",
            "train loss:0.0019564379344021924\n",
            "train loss:0.0025981875856510082\n",
            "train loss:0.0024705214108141312\n",
            "train loss:0.0008922939789507606\n",
            "train loss:0.0018499923105802709\n",
            "train loss:0.002136667304531666\n",
            "train loss:0.0011004144623375988\n",
            "train loss:0.0027579917020902163\n",
            "train loss:0.003929749384290386\n",
            "train loss:0.0016119001178719003\n",
            "train loss:0.0008155928240428871\n",
            "train loss:0.005591432672982781\n",
            "train loss:0.002431001415260438\n",
            "train loss:0.0008523921691747798\n",
            "train loss:0.0017529538848133019\n",
            "train loss:0.002527667479125735\n",
            "train loss:0.001837101716777371\n",
            "train loss:0.0014447794016362793\n",
            "train loss:0.0006572262855354202\n",
            "train loss:0.0028789568052051628\n",
            "train loss:0.0007539525617961539\n",
            "train loss:0.0006810213339075665\n",
            "train loss:0.0008905300799433176\n",
            "train loss:0.0013008059767563463\n",
            "=== epoch:19, train acc:0.998, test acc:0.972 ===\n",
            "train loss:0.0033124342769804993\n",
            "train loss:0.0131984716434927\n",
            "train loss:0.009099839400032284\n",
            "train loss:0.003104649491914621\n",
            "train loss:0.000973711034158833\n",
            "train loss:0.0028411783823418003\n",
            "train loss:0.0026750284022172117\n",
            "train loss:0.013288595391519926\n",
            "train loss:0.0051605960335361755\n",
            "train loss:0.008144772021891209\n",
            "train loss:0.0012984890615973877\n",
            "train loss:0.0016575044656018242\n",
            "train loss:0.0002137083478558459\n",
            "train loss:0.006072545134883031\n",
            "train loss:0.0005373665150906068\n",
            "train loss:0.007411909222060322\n",
            "train loss:0.0021917061064634594\n",
            "train loss:0.0024362759318718443\n",
            "train loss:0.0016534016538428396\n",
            "train loss:0.0029944324722749093\n",
            "train loss:0.00047552657919041556\n",
            "train loss:0.0017121470055193033\n",
            "train loss:0.001788383619595095\n",
            "train loss:0.0017800743384499584\n",
            "train loss:0.002456787452434408\n",
            "train loss:0.001226109739539271\n",
            "train loss:0.0007460430009096611\n",
            "train loss:0.0028435772229896874\n",
            "train loss:0.0003018555107815471\n",
            "train loss:0.0011623463896615306\n",
            "train loss:0.0026233998826419892\n",
            "train loss:0.0016469950819403632\n",
            "train loss:0.0008307783199768365\n",
            "train loss:0.00160381428049176\n",
            "train loss:0.0009381194699425092\n",
            "train loss:0.006021956803679961\n",
            "train loss:0.000857000785111242\n",
            "train loss:0.0005794292827559169\n",
            "train loss:0.00021251316580280335\n",
            "train loss:0.0019108822568773352\n",
            "train loss:0.00038372179644175755\n",
            "train loss:0.0007222365543047036\n",
            "train loss:0.0007953484970892867\n",
            "train loss:0.001436113791918804\n",
            "train loss:0.00020396209736496204\n",
            "train loss:0.00036305383156079864\n",
            "train loss:0.000763115047488276\n",
            "train loss:0.0014206786044660936\n",
            "train loss:0.001421511919296051\n",
            "train loss:0.0009802094376554688\n",
            "=== epoch:20, train acc:1.0, test acc:0.971 ===\n",
            "train loss:0.0008158597815481947\n",
            "train loss:0.002220469152407705\n",
            "train loss:0.00032095356390729967\n",
            "train loss:0.001492201625066045\n",
            "train loss:0.0006173651080399663\n",
            "train loss:0.0006143062956939791\n",
            "train loss:0.0004802961041744705\n",
            "train loss:0.0002659959692425334\n",
            "train loss:0.00038621204765736583\n",
            "train loss:0.000368307181524884\n",
            "train loss:0.00035169625794378686\n",
            "train loss:0.0008050957735714449\n",
            "train loss:0.0007228976217718751\n",
            "train loss:0.0017938134674175968\n",
            "train loss:0.0004621696223470929\n",
            "train loss:0.0003916320918208985\n",
            "train loss:0.0004876707586618956\n",
            "train loss:0.0009686052228780363\n",
            "train loss:0.0010694825572762103\n",
            "train loss:0.0006288377009471547\n",
            "train loss:0.00046648674876446813\n",
            "train loss:0.00017522823718458362\n",
            "train loss:0.0006625418834865042\n",
            "train loss:0.0008027012819216532\n",
            "train loss:0.00024651332753178995\n",
            "train loss:0.0004951060396366793\n",
            "train loss:0.0006399620436061447\n",
            "train loss:0.0010615733414877932\n",
            "train loss:0.0007979169238619047\n",
            "train loss:0.0006019625200889497\n",
            "train loss:0.0018414914488172366\n",
            "train loss:0.00042978519699051897\n",
            "train loss:0.00012815314186789763\n",
            "train loss:0.0017135551325987452\n",
            "train loss:0.00018064223011596026\n",
            "train loss:0.00039536928179393033\n",
            "train loss:0.00018546283882354435\n",
            "train loss:0.0004639390847850865\n",
            "train loss:0.0007756387745712967\n",
            "train loss:0.0009914541664127695\n",
            "train loss:0.00040513715513625364\n",
            "train loss:0.0009179733258824802\n",
            "train loss:0.0001967366761319875\n",
            "train loss:0.00044150714097853543\n",
            "train loss:0.0006317413407791329\n",
            "train loss:0.0005279380464069894\n",
            "train loss:0.0006707972567023377\n",
            "train loss:0.0006417463306595093\n",
            "train loss:0.0026602009788853406\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.974\n",
            "Saved network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
        "                        conv_param={'filter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs,\n",
        "                  mini_batch_size=100, optimizer='Adam', optimizer_param={'lr' : 0.003},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved network Parameters!\")\n",
        "\n",
        "markers = {'train' : 'o', 'test' : 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.6486955972538544\n",
            "=== epoch:1, train acc:0.198, test acc:0.144 ===\n",
            "train loss:1.8026245552293279\n",
            "train loss:1.5766250758881686\n",
            "train loss:1.7274858715356267\n",
            "train loss:1.415697232530713\n",
            "train loss:1.3343991295913475\n",
            "train loss:0.897661276748649\n",
            "train loss:0.6248407912499206\n",
            "train loss:0.5096838195276194\n",
            "train loss:0.3898707215995438\n",
            "train loss:0.295446753107344\n",
            "train loss:0.2395963759458239\n",
            "train loss:0.13619062409087465\n",
            "train loss:0.16987907744740816\n",
            "train loss:0.29737943484197904\n",
            "train loss:0.17635632936154091\n",
            "train loss:0.15446348843451502\n",
            "train loss:0.23072727920325428\n",
            "train loss:0.20489497982885088\n",
            "train loss:0.1971915742208407\n",
            "train loss:0.31263832051918433\n",
            "train loss:0.242839902546027\n",
            "train loss:0.2643944224969541\n",
            "train loss:0.1225710255772362\n",
            "train loss:0.09340828288165844\n",
            "train loss:0.11637872393293655\n",
            "train loss:0.25396029595034425\n",
            "train loss:0.27769547672453876\n",
            "train loss:0.18154233427591962\n",
            "train loss:0.2380341810954661\n",
            "train loss:0.2291294955885368\n",
            "train loss:0.1294893456885824\n",
            "train loss:0.16024295654599555\n",
            "train loss:0.10239720381753724\n",
            "train loss:0.1635160426159097\n",
            "train loss:0.15346026366523524\n",
            "train loss:0.10136714550278769\n",
            "train loss:0.14270377344649174\n",
            "train loss:0.15294073537495362\n",
            "train loss:0.05840684869434115\n",
            "train loss:0.16787628779405034\n",
            "train loss:0.13084244951784074\n",
            "train loss:0.06838320478287574\n",
            "train loss:0.04234294049501025\n",
            "train loss:0.0609841179437354\n",
            "train loss:0.05011478004741649\n",
            "train loss:0.08266018000332684\n",
            "train loss:0.09384772922402941\n",
            "train loss:0.04527478158241567\n",
            "train loss:0.035084559481597284\n",
            "train loss:0.0491897220063052\n",
            "=== epoch:2, train acc:0.967, test acc:0.935 ===\n",
            "train loss:0.024101367475371664\n",
            "train loss:0.049210025248165784\n",
            "train loss:0.048068410477305354\n",
            "train loss:0.028055892822986955\n",
            "train loss:0.1609558598861091\n",
            "train loss:0.06418873979127969\n",
            "train loss:0.12378518790343301\n",
            "train loss:0.04099889321252179\n",
            "train loss:0.03958492746381506\n",
            "train loss:0.046829725966378195\n",
            "train loss:0.06892911844818618\n",
            "train loss:0.15497338590533954\n",
            "train loss:0.06394615480002352\n",
            "train loss:0.03719165003486421\n",
            "train loss:0.017252806612294117\n",
            "train loss:0.17105810237218455\n",
            "train loss:0.025434257110113396\n",
            "train loss:0.05761124729309215\n",
            "train loss:0.0573306076402869\n",
            "train loss:0.071762658836346\n",
            "train loss:0.026469720862827714\n",
            "train loss:0.11555140416584024\n",
            "train loss:0.15527412400655938\n",
            "train loss:0.04380599932885367\n",
            "train loss:0.04317866521367104\n",
            "train loss:0.050144106225179605\n",
            "train loss:0.035577114285615385\n",
            "train loss:0.03068903873080834\n",
            "train loss:0.04400503108885871\n",
            "train loss:0.026763199337736524\n",
            "train loss:0.03389973551382537\n",
            "train loss:0.035605023644579\n",
            "train loss:0.020134706577414233\n",
            "train loss:0.029523919196962668\n",
            "train loss:0.013841450887131504\n",
            "train loss:0.019706014628747194\n",
            "train loss:0.0390638717392413\n",
            "train loss:0.024235934296431533\n",
            "train loss:0.08038858783391921\n",
            "train loss:0.011558886866382862\n",
            "train loss:0.006668326331576339\n",
            "train loss:0.11996055424956566\n",
            "train loss:0.04696612164147485\n",
            "train loss:0.026920031238212343\n",
            "train loss:0.02227504269568425\n",
            "train loss:0.048830528316785575\n",
            "train loss:0.01613555231761742\n",
            "train loss:0.04091720007394879\n",
            "train loss:0.03384343867871623\n",
            "train loss:0.07730949257568956\n",
            "=== epoch:3, train acc:0.968, test acc:0.932 ===\n",
            "train loss:0.017968929144982573\n",
            "train loss:0.02636153274431985\n",
            "train loss:0.021484295942774744\n",
            "train loss:0.03076888785096511\n",
            "train loss:0.016566996169294187\n",
            "train loss:0.017705799631521454\n",
            "train loss:0.023487175987692423\n",
            "train loss:0.012681385654717468\n",
            "train loss:0.01338743085653827\n",
            "train loss:0.019728627445288218\n",
            "train loss:0.03280515118351841\n",
            "train loss:0.027676775709167965\n",
            "train loss:0.018853168217257816\n",
            "train loss:0.02972322192394689\n",
            "train loss:0.03678838658897774\n",
            "train loss:0.00651160891802841\n",
            "train loss:0.027250047107439425\n",
            "train loss:0.013064538283384459\n",
            "train loss:0.02455989445981973\n",
            "train loss:0.07932841755962189\n",
            "train loss:0.010801796555031875\n",
            "train loss:0.01855882305686529\n",
            "train loss:0.05959221821825434\n",
            "train loss:0.008941207465773779\n",
            "train loss:0.010274649359257277\n",
            "train loss:0.022127289441047315\n",
            "train loss:0.010199913678774163\n",
            "train loss:0.014516407371473304\n",
            "train loss:0.008959596850035384\n",
            "train loss:0.020646240597973185\n",
            "train loss:0.020160500260622712\n",
            "train loss:0.007000421956310716\n",
            "train loss:0.0070704256443049765\n",
            "train loss:0.008432949247803205\n",
            "train loss:0.01310197517875504\n",
            "train loss:0.010822799853363525\n",
            "train loss:0.025782292088254655\n",
            "train loss:0.019461206887520014\n",
            "train loss:0.00558752819638412\n",
            "train loss:0.008614816894511674\n",
            "train loss:0.005998002305180398\n",
            "train loss:0.015439282092205358\n",
            "train loss:0.007008245847368362\n",
            "train loss:0.004462394300737302\n",
            "train loss:0.0074376510501046175\n",
            "train loss:0.0038845356031275684\n",
            "train loss:0.08090504229554414\n",
            "train loss:0.0085893689663723\n",
            "train loss:0.009380622907013704\n",
            "train loss:0.020174845355418802\n",
            "=== epoch:4, train acc:0.992, test acc:0.963 ===\n",
            "train loss:0.002377775875065563\n",
            "train loss:0.011321584603833453\n",
            "train loss:0.004619008372157209\n",
            "train loss:0.013434218544731622\n",
            "train loss:0.08047681699123231\n",
            "train loss:0.03505567024619802\n",
            "train loss:0.010847352694624\n",
            "train loss:0.07747030476002459\n",
            "train loss:0.03794854863348044\n",
            "train loss:0.012156191716043693\n",
            "train loss:0.012213078435433402\n",
            "train loss:0.02597565250030791\n",
            "train loss:0.024437904705550656\n",
            "train loss:0.005689924436055074\n",
            "train loss:0.016452661334586297\n",
            "train loss:0.010578834844785728\n",
            "train loss:0.01499702906893402\n",
            "train loss:0.042975593536885626\n",
            "train loss:0.05052492456161391\n",
            "train loss:0.04234649370923578\n",
            "train loss:0.01990737626438233\n",
            "train loss:0.007055663466502892\n",
            "train loss:0.007962484336083023\n",
            "train loss:0.01339261072958626\n",
            "train loss:0.01611139664976582\n",
            "train loss:0.04297341286315474\n",
            "train loss:0.013550067326213439\n",
            "train loss:0.01677282846149243\n",
            "train loss:0.03432116088206305\n",
            "train loss:0.01753441492856139\n",
            "train loss:0.029331811960482717\n",
            "train loss:0.020508174335418718\n",
            "train loss:0.030381635655040498\n",
            "train loss:0.019284840708467654\n",
            "train loss:0.0051423850981975605\n",
            "train loss:0.017421362941860287\n",
            "train loss:0.014067580894314695\n",
            "train loss:0.032790764898237804\n",
            "train loss:0.006847298134885842\n",
            "train loss:0.04210753758906056\n",
            "train loss:0.0049364358252392635\n",
            "train loss:0.008536041964501494\n",
            "train loss:0.021269474951229047\n",
            "train loss:0.00603718386686133\n",
            "train loss:0.0074041929962831874\n",
            "train loss:0.005640345148624684\n",
            "train loss:0.005640125198485487\n",
            "train loss:0.033611941906813374\n",
            "train loss:0.0041963399857428965\n",
            "train loss:0.004361208012207771\n",
            "=== epoch:5, train acc:0.991, test acc:0.949 ===\n",
            "train loss:0.009648523621656227\n",
            "train loss:0.02063976559413765\n",
            "train loss:0.010046992986101894\n",
            "train loss:0.004179482453651977\n",
            "train loss:0.00994701253836102\n",
            "train loss:0.004033338869312567\n",
            "train loss:0.002996425657927267\n",
            "train loss:0.006914734330584705\n",
            "train loss:0.005392732359877485\n",
            "train loss:0.004772401918470606\n",
            "train loss:0.004882250675596646\n",
            "train loss:0.0038499580981093758\n",
            "train loss:0.004161585994706844\n",
            "train loss:0.008598496334360457\n",
            "train loss:0.0007846998130263766\n",
            "train loss:0.013353399533344463\n",
            "train loss:0.04519948344073444\n",
            "train loss:0.004017036738825469\n",
            "train loss:0.001429132616998809\n",
            "train loss:0.001998835306172518\n",
            "train loss:0.002023171285357593\n",
            "train loss:0.014300795222080508\n",
            "train loss:0.0016261460306176512\n",
            "train loss:0.004359878209726555\n",
            "train loss:0.0017043732927706826\n",
            "train loss:0.005133771554139101\n",
            "train loss:0.045174114062605186\n",
            "train loss:0.007819045631378883\n",
            "train loss:0.006658515280361185\n",
            "train loss:0.0038739280556946403\n",
            "train loss:0.003159806711007962\n",
            "train loss:0.0145932181007965\n",
            "train loss:0.008895900896869542\n",
            "train loss:0.008377509322944706\n",
            "train loss:0.005479898384916534\n",
            "train loss:0.00440282461887637\n",
            "train loss:0.005029910367244873\n",
            "train loss:0.0022842959409141938\n",
            "train loss:0.005599504766797656\n",
            "train loss:0.00637717364043297\n",
            "train loss:0.00923433870398812\n",
            "train loss:0.002289655038335677\n",
            "train loss:0.0016847629992484709\n",
            "train loss:0.0014713672830934598\n",
            "train loss:0.005694739908602859\n",
            "train loss:0.004339119395549997\n",
            "train loss:0.0022066442466770813\n",
            "train loss:0.009627922673035513\n",
            "train loss:0.003200833370141884\n",
            "train loss:0.0019168179728430183\n",
            "=== epoch:6, train acc:0.999, test acc:0.958 ===\n",
            "train loss:0.0035117846661250336\n",
            "train loss:0.0004050181736425116\n",
            "train loss:0.00217128644017152\n",
            "train loss:0.003070512374577829\n",
            "train loss:0.001189680291337779\n",
            "train loss:0.0019321267253939692\n",
            "train loss:0.002549243657926109\n",
            "train loss:0.004920350010353038\n",
            "train loss:0.0009845754865004056\n",
            "train loss:0.004724571546165718\n",
            "train loss:0.0008836898158883406\n",
            "train loss:0.010247351058415555\n",
            "train loss:0.004094663258091687\n",
            "train loss:0.0027353077493711066\n",
            "train loss:0.002246556010825755\n",
            "train loss:0.005436073189358171\n",
            "train loss:0.000483099110344567\n",
            "train loss:0.0011224621092922396\n",
            "train loss:0.004246363841837508\n",
            "train loss:0.004245395703713825\n",
            "train loss:0.0009014134335177516\n",
            "train loss:0.001005367844082843\n",
            "train loss:0.006843715834917855\n",
            "train loss:0.010660527660712302\n",
            "train loss:0.010704827548263103\n",
            "train loss:0.0037665549361480355\n",
            "train loss:0.0015575186979611543\n",
            "train loss:0.00464404419070035\n",
            "train loss:0.0019345287520754204\n",
            "train loss:0.0016758793810215148\n",
            "train loss:0.00527410900750352\n",
            "train loss:0.0038822046758652364\n",
            "train loss:0.004973641902162741\n",
            "train loss:0.011434229769099323\n",
            "train loss:0.001501775305905178\n",
            "train loss:0.004035503581102717\n",
            "train loss:0.0044559803939266925\n",
            "train loss:0.006409139481297735\n",
            "train loss:0.008351340099142176\n",
            "train loss:0.003758063956458404\n",
            "train loss:0.003050663346951123\n",
            "train loss:0.0052254791082362384\n",
            "train loss:0.008026676594314662\n",
            "train loss:0.003669353482325831\n",
            "train loss:0.003024095405614697\n",
            "train loss:0.0016318822076852122\n",
            "train loss:0.0006498891959124835\n",
            "train loss:0.005627467325049521\n",
            "train loss:0.006747357513521408\n",
            "train loss:0.0030318998110284118\n",
            "=== epoch:7, train acc:0.993, test acc:0.946 ===\n",
            "train loss:0.005744860195523365\n",
            "train loss:0.012116215306691483\n",
            "train loss:0.0024358890639347972\n",
            "train loss:0.0021859244591585624\n",
            "train loss:0.0024897896805088525\n",
            "train loss:0.0027064749239360097\n",
            "train loss:0.0009755626697274987\n",
            "train loss:0.00018667761215296585\n",
            "train loss:0.002168170714371488\n",
            "train loss:0.0004657525824580007\n",
            "train loss:0.00033367826853541137\n",
            "train loss:0.0037856111994998853\n",
            "train loss:0.000721031386717224\n",
            "train loss:0.00043779212725436866\n",
            "train loss:0.0025407755661231084\n",
            "train loss:0.0007678317812148048\n",
            "train loss:0.0028768930595772413\n",
            "train loss:0.003915947638464785\n",
            "train loss:0.0016877699098639227\n",
            "train loss:0.0009622876421475456\n",
            "train loss:0.0008432210513807239\n",
            "train loss:0.0017078178254660917\n",
            "train loss:0.00410464365032522\n",
            "train loss:0.0014793682410058858\n",
            "train loss:0.0014409208436601234\n",
            "train loss:0.0011941773365635\n",
            "train loss:0.0012447314154740488\n",
            "train loss:0.0007771513139474552\n",
            "train loss:0.0004480921117355765\n",
            "train loss:0.0017208637233136752\n",
            "train loss:0.0015289912853037521\n",
            "train loss:0.0024931427373480054\n",
            "train loss:0.00046995133700075383\n",
            "train loss:0.0025027381261728864\n",
            "train loss:0.00012896736770409418\n",
            "train loss:0.003473120023589923\n",
            "train loss:0.0004557440079048621\n",
            "train loss:0.00029576973694371886\n",
            "train loss:0.0035667610783650773\n",
            "train loss:0.001667321716954988\n",
            "train loss:0.0010758961793516934\n",
            "train loss:0.0020372170638646775\n",
            "train loss:0.0017695193012838082\n",
            "train loss:0.00245334272638771\n",
            "train loss:0.002218361703145791\n",
            "train loss:0.0003674424532569253\n",
            "train loss:0.00017963074399531803\n",
            "train loss:0.0005909376279989089\n",
            "train loss:0.0005985732336189039\n",
            "train loss:0.0036841940588052\n",
            "=== epoch:8, train acc:0.999, test acc:0.961 ===\n",
            "train loss:0.0008109723269375589\n",
            "train loss:0.00043830878046152743\n",
            "train loss:0.002061591704754752\n",
            "train loss:0.00180678647597301\n",
            "train loss:0.000268159363203074\n",
            "train loss:0.0005490834600861924\n",
            "train loss:0.033673532420273794\n",
            "train loss:0.0008311871340505318\n",
            "train loss:0.00010105773815584846\n",
            "train loss:0.0006392426227741703\n",
            "train loss:0.004639880232371119\n",
            "train loss:0.00027987251384464594\n",
            "train loss:0.0023770110150002854\n",
            "train loss:0.0008004064046898526\n",
            "train loss:0.0004752167592858992\n",
            "train loss:0.0013473523541858844\n",
            "train loss:0.000512947667093513\n",
            "train loss:0.004492514171415154\n",
            "train loss:0.0014557641370290153\n",
            "train loss:0.0009085496050388896\n",
            "train loss:0.0007953237443634581\n",
            "train loss:0.0012867121090492032\n",
            "train loss:0.0008895934705229541\n",
            "train loss:0.0011194716320058013\n",
            "train loss:0.0037233870811189634\n",
            "train loss:0.0010869250355298467\n",
            "train loss:0.00011470417128768014\n",
            "train loss:0.0008066404279931721\n",
            "train loss:0.0032601639259574465\n",
            "train loss:0.0005891216987658364\n",
            "train loss:0.00017124551275577576\n",
            "train loss:0.00020670623566045144\n",
            "train loss:0.0007337542506891279\n",
            "train loss:0.0025459270975848637\n",
            "train loss:0.00024217395336167721\n",
            "train loss:0.0009277607215008648\n",
            "train loss:0.0005881858152435677\n",
            "train loss:0.0016238370388718696\n",
            "train loss:0.0003157269405938561\n",
            "train loss:0.00016731812345245004\n",
            "train loss:0.0005213227297990057\n",
            "train loss:5.7744740086728034e-05\n",
            "train loss:0.0005114928291271979\n",
            "train loss:0.00018453572315941724\n",
            "train loss:3.5388265500428596e-05\n",
            "train loss:0.0007613497677402378\n",
            "train loss:0.000795286757551764\n",
            "train loss:0.0021414717387759847\n",
            "train loss:0.001109344096576738\n",
            "train loss:0.0010275456443327557\n",
            "=== epoch:9, train acc:0.998, test acc:0.957 ===\n",
            "train loss:0.0006715202859745701\n",
            "train loss:0.0009139900207489416\n",
            "train loss:0.0007579073379813842\n",
            "train loss:0.0010359213291117671\n",
            "train loss:6.341052462071188e-05\n",
            "train loss:0.00018393851429728968\n",
            "train loss:0.00013090356652153336\n",
            "train loss:4.932363797084584e-05\n",
            "train loss:0.0006561474520468299\n",
            "train loss:0.00039640241002686773\n",
            "train loss:0.0004509978055934732\n",
            "train loss:0.0005096493858117909\n",
            "train loss:0.000387114516697159\n",
            "train loss:7.711149790277678e-05\n",
            "train loss:0.0013448851603784949\n",
            "train loss:0.0004011313543938165\n",
            "train loss:2.7596867304846944e-05\n",
            "train loss:3.597835023904829e-05\n",
            "train loss:0.00014076984860607054\n",
            "train loss:0.0004410387109620906\n",
            "train loss:0.00015724344699656604\n",
            "train loss:0.00039163021872971356\n",
            "train loss:3.193288019699023e-05\n",
            "train loss:0.0003421912264809578\n",
            "train loss:0.00027528284531924967\n",
            "train loss:8.140668188576377e-05\n",
            "train loss:0.0002944605914293416\n",
            "train loss:0.0015854726559544432\n",
            "train loss:0.0002893096158223129\n",
            "train loss:0.0007736686775380701\n",
            "train loss:0.002638158317210244\n",
            "train loss:0.00027136855301438664\n",
            "train loss:0.00146188450322969\n",
            "train loss:0.0001313130061877654\n",
            "train loss:0.0002720367711529337\n",
            "train loss:0.00032539997850975237\n",
            "train loss:0.00015596244009088831\n",
            "train loss:0.0002249007483233025\n",
            "train loss:0.0005042988324097052\n",
            "train loss:0.0005268816499014812\n",
            "train loss:0.0003700849905011631\n",
            "train loss:0.0024590100795343307\n",
            "train loss:0.0011410208119529402\n",
            "train loss:0.0007750433063715447\n",
            "train loss:0.0003520104725522266\n",
            "train loss:0.0007850457387236484\n",
            "train loss:3.83914498211047e-05\n",
            "train loss:0.00013176025283238312\n",
            "train loss:0.0008441304913065253\n",
            "train loss:0.0017345881746602836\n",
            "=== epoch:10, train acc:1.0, test acc:0.964 ===\n",
            "train loss:0.00042163084960064796\n",
            "train loss:0.0009292067079392606\n",
            "train loss:0.00023773863306911347\n",
            "train loss:0.0004888175443192025\n",
            "train loss:8.707728599770868e-05\n",
            "train loss:0.0008076377329449483\n",
            "train loss:0.00023709806521180803\n",
            "train loss:0.00029559890823481055\n",
            "train loss:0.000492229531215308\n",
            "train loss:0.0003838888697789214\n",
            "train loss:0.00015435253764497416\n",
            "train loss:0.00016203323543206735\n",
            "train loss:0.00010481521775839368\n",
            "train loss:0.0002877779891283653\n",
            "train loss:0.00012446897432475716\n",
            "train loss:0.00014466249153956\n",
            "train loss:6.246762801062245e-05\n",
            "train loss:0.0008794060535548913\n",
            "train loss:0.0010006898872011532\n",
            "train loss:0.0007387305760571522\n",
            "train loss:3.3160037104607473e-05\n",
            "train loss:0.00010434983102649149\n",
            "train loss:1.6889883381314872e-05\n",
            "train loss:7.837162023238426e-05\n",
            "train loss:9.314653701851943e-05\n",
            "train loss:0.00010223648017925981\n",
            "train loss:7.275704905576987e-05\n",
            "train loss:0.001013848179042079\n",
            "train loss:0.0006933023026023587\n",
            "train loss:0.000496298867067446\n",
            "train loss:0.0001913887532100093\n",
            "train loss:7.872043786783566e-05\n",
            "train loss:0.003130100012570926\n",
            "train loss:0.001610784210278161\n",
            "train loss:0.00033937040736709196\n",
            "train loss:0.0011436689626212174\n",
            "train loss:0.0005770488998083663\n",
            "train loss:0.00018152080686690633\n",
            "train loss:0.00015085897624954507\n",
            "train loss:0.0024426443598417657\n",
            "train loss:0.0005583699831651953\n",
            "train loss:0.0015494014369004925\n",
            "train loss:0.005015282415727102\n",
            "train loss:0.0005388624201268271\n",
            "train loss:0.0002232036690527589\n",
            "train loss:0.00022368611675081535\n",
            "train loss:0.00024764874657909466\n",
            "train loss:0.008840080443609762\n",
            "train loss:0.00011555726511536338\n",
            "train loss:0.0012262341642503432\n",
            "=== epoch:11, train acc:0.992, test acc:0.958 ===\n",
            "train loss:0.0008791074507808426\n",
            "train loss:0.0002523530826431977\n",
            "train loss:0.00037398964147023866\n",
            "train loss:0.00011432362768739393\n",
            "train loss:9.969858011406182e-05\n",
            "train loss:0.0010338048248236958\n",
            "train loss:0.00027111017882465135\n",
            "train loss:0.0007798600268916356\n",
            "train loss:0.00011257360855738848\n",
            "train loss:0.0016627093850541507\n",
            "train loss:0.00098727468092217\n",
            "train loss:0.0005088358452953782\n",
            "train loss:0.0006493445662712461\n",
            "train loss:0.00021702765134578412\n",
            "train loss:0.00423429823539243\n",
            "train loss:0.0007904727625095782\n",
            "train loss:0.0004761329333653448\n",
            "train loss:0.0020723231816120215\n",
            "train loss:0.00010914965338796723\n",
            "train loss:0.000898331242386973\n",
            "train loss:0.0008680338187490892\n",
            "train loss:0.0006326433483291783\n",
            "train loss:0.0008278973543887223\n",
            "train loss:0.00030302943953202527\n",
            "train loss:0.001557937667567135\n",
            "train loss:0.00044174973685428653\n",
            "train loss:0.0013978583064247958\n",
            "train loss:0.00017111172608350736\n",
            "train loss:0.000668631103697943\n",
            "train loss:0.0066412007546412495\n",
            "train loss:0.0004519520379082261\n",
            "train loss:0.0005330880244217956\n",
            "train loss:7.062562167296049e-05\n",
            "train loss:0.000733983260508402\n",
            "train loss:0.00074856465257907\n",
            "train loss:0.00015202638055759988\n",
            "train loss:0.002723443126385424\n",
            "train loss:0.0007752148923066058\n",
            "train loss:0.009522671879503281\n",
            "train loss:0.00175120689298821\n",
            "train loss:0.0004912526017108099\n",
            "train loss:0.001506429303520609\n",
            "train loss:9.821683631199539e-05\n",
            "train loss:0.000366981978024978\n",
            "train loss:0.006955598525639919\n",
            "train loss:0.0013623384686053555\n",
            "train loss:0.00017291320214672258\n",
            "train loss:0.0033118317416419925\n",
            "train loss:0.001294381664266246\n",
            "train loss:0.0006283688646914382\n",
            "=== epoch:12, train acc:0.997, test acc:0.962 ===\n",
            "train loss:0.0007673891230500932\n",
            "train loss:0.0003610354117195947\n",
            "train loss:0.0005443793659532154\n",
            "train loss:0.0005447742025559051\n",
            "train loss:0.0003816311337645519\n",
            "train loss:0.001128211393280407\n",
            "train loss:0.0005007729857740691\n",
            "train loss:0.0016878461443331164\n",
            "train loss:0.011678493736793949\n",
            "train loss:0.001038639539267373\n",
            "train loss:0.0012952213070335286\n",
            "train loss:0.0011150648302960186\n",
            "train loss:0.0031725253276852924\n",
            "train loss:0.0006920429583050045\n",
            "train loss:0.0014228371776283761\n",
            "train loss:0.0008712043686027231\n",
            "train loss:0.0024227355989383803\n",
            "train loss:0.0021440050997569658\n",
            "train loss:0.018356744469658733\n",
            "train loss:0.0009746856110278569\n",
            "train loss:0.002410339526986107\n",
            "train loss:0.0012745160633864704\n",
            "train loss:0.0016514324774137836\n",
            "train loss:0.0015814646394441904\n",
            "train loss:0.0011037305583767062\n",
            "train loss:0.0014408211437429658\n",
            "train loss:0.00015754502093394\n",
            "train loss:0.000858096118500397\n",
            "train loss:0.0029836549779444937\n",
            "train loss:0.001908435766123253\n",
            "train loss:0.0008129861091624954\n",
            "train loss:0.0004558474490214064\n",
            "train loss:0.0020643739688720082\n",
            "train loss:0.0005701536628911169\n",
            "train loss:0.0008973166067144061\n",
            "train loss:0.0010215433989082263\n",
            "train loss:1.7498399457692338e-05\n",
            "train loss:0.0007546035300779543\n",
            "train loss:0.0023523761155218063\n",
            "train loss:0.0018549487820739216\n",
            "train loss:3.8327669893207245e-05\n",
            "train loss:0.0004386445289760048\n",
            "train loss:0.0008669719200909809\n",
            "train loss:0.00026459214007176936\n",
            "train loss:0.0014363439472220433\n",
            "train loss:0.0005591423416786711\n",
            "train loss:0.0012376928155376488\n",
            "train loss:0.0019054600661040158\n",
            "train loss:0.00274623063992389\n",
            "train loss:0.0017280995891104307\n",
            "=== epoch:13, train acc:0.996, test acc:0.966 ===\n",
            "train loss:0.0019347241186465722\n",
            "train loss:0.0004379048085766554\n",
            "train loss:0.00029578987254179356\n",
            "train loss:0.0015496939801106413\n",
            "train loss:0.001007293283044389\n",
            "train loss:0.0013912405690594094\n",
            "train loss:0.00032481155755967155\n",
            "train loss:0.0006551702464049473\n",
            "train loss:0.0018747067418404578\n",
            "train loss:0.00303031485070532\n",
            "train loss:0.001839814846154599\n",
            "train loss:0.0002358183421762417\n",
            "train loss:0.00022915293336641643\n",
            "train loss:0.00020262193260514597\n",
            "train loss:0.0012917207094700275\n",
            "train loss:0.0018707324456687638\n",
            "train loss:0.002050645997799614\n",
            "train loss:0.0004553363477036658\n",
            "train loss:0.0004101562990377217\n",
            "train loss:0.0005204969590006345\n",
            "train loss:0.001155551258127475\n",
            "train loss:0.0014526437675422674\n",
            "train loss:0.0007821270978316353\n",
            "train loss:0.0008714178940961531\n",
            "train loss:0.0008146737881191997\n",
            "train loss:0.0007753447938813519\n",
            "train loss:0.0006096879174995276\n",
            "train loss:0.0002662226927822978\n",
            "train loss:0.001242788597476484\n",
            "train loss:0.000458651259944705\n",
            "train loss:0.001230617311559501\n",
            "train loss:0.0025072620556516427\n",
            "train loss:0.010245959835472522\n",
            "train loss:0.00043657764149764277\n",
            "train loss:0.0011327495521177055\n",
            "train loss:0.000799243231464102\n",
            "train loss:0.0003979086739416828\n",
            "train loss:0.0003530100917819965\n",
            "train loss:5.567399041188628e-05\n",
            "train loss:0.0005414689072115743\n",
            "train loss:9.753445925053953e-05\n",
            "train loss:0.0003202489329699021\n",
            "train loss:0.0042331548515612775\n",
            "train loss:0.004957116117148733\n",
            "train loss:0.002841763024247441\n",
            "train loss:0.0021980873713164425\n",
            "train loss:0.0008319637384204743\n",
            "train loss:0.00063191284233439\n",
            "train loss:0.003570366001745534\n",
            "train loss:0.001090800314171666\n",
            "=== epoch:14, train acc:1.0, test acc:0.948 ===\n",
            "train loss:0.0010285638581763509\n",
            "train loss:0.0019649386742062167\n",
            "train loss:0.0003985308874349347\n",
            "train loss:0.00041644085561175545\n",
            "train loss:0.0004885593465266283\n",
            "train loss:0.0006091541418007048\n",
            "train loss:0.00020227439166073424\n",
            "train loss:4.98229895148419e-05\n",
            "train loss:0.0002691543236806217\n",
            "train loss:0.0013688004686852012\n",
            "train loss:0.0014050613146053717\n",
            "train loss:0.0002695549725487836\n",
            "train loss:0.0004670492570520911\n",
            "train loss:0.0005635924594220537\n",
            "train loss:0.0004511789553082712\n",
            "train loss:0.0003373629627902063\n",
            "train loss:7.419264291490501e-05\n",
            "train loss:6.762570056930983e-05\n",
            "train loss:0.0006020739524405793\n",
            "train loss:0.0006403043435778743\n",
            "train loss:0.00015738919448096033\n",
            "train loss:0.00010815852234921322\n",
            "train loss:0.0008673115670378381\n",
            "train loss:0.00037862465782756427\n",
            "train loss:0.0007228006907791056\n",
            "train loss:0.0008227444275877052\n",
            "train loss:0.00031824403231607936\n",
            "train loss:0.00040852280624844277\n",
            "train loss:0.000348045652987204\n",
            "train loss:0.00023453320439344606\n",
            "train loss:0.0016224993049842526\n",
            "train loss:4.410667025603266e-05\n",
            "train loss:0.0015900076300186302\n",
            "train loss:0.0008105667978335642\n",
            "train loss:0.0010743338466855262\n",
            "train loss:0.0005012976565573733\n",
            "train loss:0.00014211759385216184\n",
            "train loss:0.00025755000542484807\n",
            "train loss:0.00018461575043146844\n",
            "train loss:0.0008332743326074555\n",
            "train loss:0.00015401642830342648\n",
            "train loss:0.0015618084377038127\n",
            "train loss:0.0017928157185185584\n",
            "train loss:6.273446774610526e-05\n",
            "train loss:0.0004561983284789257\n",
            "train loss:0.0004259154853630795\n",
            "train loss:0.0007781471462947387\n",
            "train loss:0.0019057276151443847\n",
            "train loss:0.0019397770336027202\n",
            "train loss:0.00038643398966889826\n",
            "=== epoch:15, train acc:0.999, test acc:0.951 ===\n",
            "train loss:0.0003824927827626609\n",
            "train loss:0.0012620494454207104\n",
            "train loss:0.00038883825515166447\n",
            "train loss:9.280536058080834e-05\n",
            "train loss:9.25160982593481e-05\n",
            "train loss:0.0008587842338661457\n",
            "train loss:0.0004276679182451743\n",
            "train loss:0.0002827271508445649\n",
            "train loss:0.00017602669112740183\n",
            "train loss:0.0009388457243929005\n",
            "train loss:0.0002433619244706839\n",
            "train loss:0.0004049193109803549\n",
            "train loss:0.002402741892677461\n",
            "train loss:0.0003098433281148606\n",
            "train loss:0.00013896568914229944\n",
            "train loss:0.0012795081751118165\n",
            "train loss:0.0006018468856109952\n",
            "train loss:0.0002784078689868508\n",
            "train loss:0.0006018588029881919\n",
            "train loss:0.0010610509784496533\n",
            "train loss:0.00043772026262062344\n",
            "train loss:0.004293918764398113\n",
            "train loss:0.001104289687212534\n",
            "train loss:5.478575248958559e-05\n",
            "train loss:0.0009300908735913862\n",
            "train loss:4.418765525217865e-05\n",
            "train loss:0.0018648155731968033\n",
            "train loss:0.001228979580712545\n",
            "train loss:0.0010848572732346361\n",
            "train loss:0.0007300775450467896\n",
            "train loss:0.0018778769624174234\n",
            "train loss:0.0003058829311708575\n",
            "train loss:0.0007216375473330966\n",
            "train loss:0.00016736195385299554\n",
            "train loss:0.00667392645960212\n",
            "train loss:0.0011250617499202471\n",
            "train loss:0.00023127014304492305\n",
            "train loss:0.00020983026115922463\n",
            "train loss:0.00115231484119987\n",
            "train loss:0.0005207164482641808\n",
            "train loss:0.0006809454504794815\n",
            "train loss:0.00020861862135697817\n",
            "train loss:0.0002821388417841217\n",
            "train loss:0.001485136517694772\n",
            "train loss:0.004087440554507496\n",
            "train loss:0.00022963832897250738\n",
            "train loss:0.0041155114763453795\n",
            "train loss:0.002186997848888462\n",
            "train loss:0.016817826238609754\n",
            "train loss:0.002659209496365915\n",
            "=== epoch:16, train acc:0.986, test acc:0.927 ===\n",
            "train loss:0.00439180839144598\n",
            "train loss:0.00046644303312030704\n",
            "train loss:0.012382283296032883\n",
            "train loss:0.003520696696675713\n",
            "train loss:0.0009169368062883431\n",
            "train loss:0.003223188574982655\n",
            "train loss:0.003106540515283174\n",
            "train loss:0.002385289631805163\n",
            "train loss:0.0006189937954200227\n",
            "train loss:0.0006922261246806365\n",
            "train loss:0.0018579823542559481\n",
            "train loss:0.01571051145970095\n",
            "train loss:0.045203411306841204\n",
            "train loss:0.003418662925842305\n",
            "train loss:0.0029203703225309523\n",
            "train loss:0.0024553355364460515\n",
            "train loss:0.0038218111256411707\n",
            "train loss:0.00048636746856139\n",
            "train loss:0.005805250129191137\n",
            "train loss:0.0015601660341367002\n",
            "train loss:0.000852938533973886\n",
            "train loss:0.005325678613252873\n",
            "train loss:0.00169664444427456\n",
            "train loss:0.004373054929663856\n",
            "train loss:0.002191750402873117\n",
            "train loss:0.0029958199230979833\n",
            "train loss:0.0012205451709715442\n",
            "train loss:0.0009252751107783283\n",
            "train loss:0.0015503862287406989\n",
            "train loss:0.005593876569094447\n",
            "train loss:0.03737353394239994\n",
            "train loss:0.0018564570651862181\n",
            "train loss:0.0009608210367332215\n",
            "train loss:0.000443324187935792\n",
            "train loss:0.0012463133169857068\n",
            "train loss:0.0003551643880855777\n",
            "train loss:0.001604023249706643\n",
            "train loss:0.0010107029070063082\n",
            "train loss:0.024936720836164158\n",
            "train loss:0.0009250245619108603\n",
            "train loss:0.0009937277563367192\n",
            "train loss:0.0011011582324739915\n",
            "train loss:0.0005022265028173473\n",
            "train loss:0.0030880581696915814\n",
            "train loss:0.002621122608766461\n",
            "train loss:0.003160249241178101\n",
            "train loss:0.0018942183612324828\n",
            "train loss:0.001958386730467053\n",
            "train loss:0.0017137504541678305\n",
            "train loss:0.0004283467568057533\n",
            "=== epoch:17, train acc:0.994, test acc:0.945 ===\n",
            "train loss:0.0005101968183892597\n",
            "train loss:0.0009710189602486803\n",
            "train loss:0.0004851918489862423\n",
            "train loss:0.00022194031688616025\n",
            "train loss:0.0627046130574555\n",
            "train loss:0.0004878925482653053\n",
            "train loss:0.0013880440286216777\n",
            "train loss:0.00010999024549201151\n",
            "train loss:0.0015869399651454723\n",
            "train loss:0.0023337314110922188\n",
            "train loss:0.0002866834917549423\n",
            "train loss:0.0008744315474554411\n",
            "train loss:0.00468002176750343\n",
            "train loss:0.003114172338640697\n",
            "train loss:0.002316056209282924\n",
            "train loss:0.0005746618798777969\n",
            "train loss:0.000438742173691156\n",
            "train loss:0.003921309198328759\n",
            "train loss:0.000191128120120849\n",
            "train loss:0.0005742078040713507\n",
            "train loss:0.0024290861065168552\n",
            "train loss:0.002623490669827278\n",
            "train loss:0.0007401367130922909\n",
            "train loss:0.0005404890817082008\n",
            "train loss:0.00012270037804311848\n",
            "train loss:0.0001246826152859596\n",
            "train loss:0.0009537171134499775\n",
            "train loss:0.00028743921481002785\n",
            "train loss:0.000549086479494831\n",
            "train loss:0.00021509138021874374\n",
            "train loss:0.0003483483054178087\n",
            "train loss:0.0003108595675143341\n",
            "train loss:0.0002475411357320405\n",
            "train loss:0.0008213283650674455\n",
            "train loss:0.0068753343376576175\n",
            "train loss:0.0027449292227958062\n",
            "train loss:0.008166637494934011\n",
            "train loss:0.0004436718139714252\n",
            "train loss:0.0015435283990805548\n",
            "train loss:0.00175214676012704\n",
            "train loss:0.057254217323719124\n",
            "train loss:0.037025310345819025\n",
            "train loss:0.004684977543572376\n",
            "train loss:0.014530581568793565\n",
            "train loss:0.030652863549228918\n",
            "train loss:0.0020623165060963435\n",
            "train loss:0.002424856875095923\n",
            "train loss:0.008724053577154695\n",
            "train loss:0.04896104244216271\n",
            "train loss:0.02419467729489825\n",
            "=== epoch:18, train acc:0.981, test acc:0.937 ===\n",
            "train loss:0.014874239236956492\n",
            "train loss:0.0027591761659955688\n",
            "train loss:0.0045994120296293075\n",
            "train loss:0.024594561516367536\n",
            "train loss:0.01252068058046657\n",
            "train loss:0.004318299231064916\n",
            "train loss:0.002063455406163232\n",
            "train loss:0.0007852098921320987\n",
            "train loss:0.014261030972690545\n",
            "train loss:0.00034092367627096086\n",
            "train loss:0.0009465875559627364\n",
            "train loss:0.0005144630473129892\n",
            "train loss:0.0010868340361296076\n",
            "train loss:8.767053119287913e-05\n",
            "train loss:0.000876064021765024\n",
            "train loss:0.0028795362886384424\n",
            "train loss:0.01432602564494764\n",
            "train loss:0.0030130684652052565\n",
            "train loss:0.004802447745896398\n",
            "train loss:0.013376987515976957\n",
            "train loss:0.004564494399321579\n",
            "train loss:0.0026942218997900768\n",
            "train loss:0.001453590905746613\n",
            "train loss:0.005281239915991539\n",
            "train loss:0.001206393795723073\n",
            "train loss:0.003234628719622795\n",
            "train loss:0.002304276973080627\n",
            "train loss:0.0015083191835803978\n",
            "train loss:0.0056857479872648164\n",
            "train loss:0.0021089452281558667\n",
            "train loss:0.0026751857091116635\n",
            "train loss:0.0003825443026603425\n",
            "train loss:0.0005736807685805069\n",
            "train loss:0.0007680874970812058\n",
            "train loss:0.0051809524967777534\n",
            "train loss:0.0011004529785779357\n",
            "train loss:0.0008669251336906694\n",
            "train loss:0.002781078706141376\n",
            "train loss:0.0017688163706191804\n",
            "train loss:0.00038913117376301485\n",
            "train loss:0.00045133326450403076\n",
            "train loss:0.0007043046359495914\n",
            "train loss:0.0021358494192524583\n",
            "train loss:0.00047670503614266244\n",
            "train loss:0.0011433044775627957\n",
            "train loss:0.00048083586655621664\n",
            "train loss:0.0010202820769403362\n",
            "train loss:0.0012762715797212112\n",
            "train loss:0.003575416770347891\n",
            "train loss:0.0005319816053792477\n",
            "=== epoch:19, train acc:0.992, test acc:0.956 ===\n",
            "train loss:0.000524437319845531\n",
            "train loss:0.003421522396336595\n",
            "train loss:0.0004074045781431192\n",
            "train loss:0.0032622332874250757\n",
            "train loss:6.851192319071648e-05\n",
            "train loss:0.002082604204402789\n",
            "train loss:0.031033941318785487\n",
            "train loss:0.003070284417551975\n",
            "train loss:0.00038970275865613283\n",
            "train loss:0.0017740730267231236\n",
            "train loss:0.0053037063238508516\n",
            "train loss:0.0013228578392228043\n",
            "train loss:0.00031060605519353827\n",
            "train loss:0.0025458475123718454\n",
            "train loss:0.0013454989825637499\n",
            "train loss:0.0004206604850223552\n",
            "train loss:0.0002375361338629983\n",
            "train loss:0.0011332412830931398\n",
            "train loss:0.0001731963985149502\n",
            "train loss:0.0033829707395778095\n",
            "train loss:0.0005600214172766341\n",
            "train loss:0.002418668306421854\n",
            "train loss:0.0010822949087427396\n",
            "train loss:0.0005675778603375701\n",
            "train loss:0.0008493159113655653\n",
            "train loss:0.0038261248240117115\n",
            "train loss:0.0035250048679433526\n",
            "train loss:0.0019172250591137907\n",
            "train loss:0.00031515013798295995\n",
            "train loss:0.0020537015503699022\n",
            "train loss:2.662779376007266e-05\n",
            "train loss:0.0015019407924908403\n",
            "train loss:0.044002040456561996\n",
            "train loss:0.0028957914275000492\n",
            "train loss:0.011271789622710556\n",
            "train loss:0.0001944134094121154\n",
            "train loss:0.00021456298084733034\n",
            "train loss:0.0008366767837188955\n",
            "train loss:0.00033505860365582954\n",
            "train loss:0.0009986689954734442\n",
            "train loss:0.0005375219210272101\n",
            "train loss:1.3537105174267754e-05\n",
            "train loss:0.0015661800226318453\n",
            "train loss:0.0005948723495931129\n",
            "train loss:0.00012382349257696027\n",
            "train loss:5.613110855702848e-05\n",
            "train loss:0.0011758938954028189\n",
            "train loss:0.004105808398618477\n",
            "train loss:0.0016916998147657747\n",
            "train loss:0.07147428834773843\n",
            "=== epoch:20, train acc:0.997, test acc:0.961 ===\n",
            "train loss:0.0005667350702821406\n",
            "train loss:3.568102182087593e-05\n",
            "train loss:0.0004483866560589775\n",
            "train loss:0.0008319350479743274\n",
            "train loss:0.0015476614931429777\n",
            "train loss:0.042867492729609796\n",
            "train loss:0.0009395543318979185\n",
            "train loss:0.0013981321105616575\n",
            "train loss:0.00020274548746185728\n",
            "train loss:0.002239685584408201\n",
            "train loss:0.0002467386197787463\n",
            "train loss:0.001147385385954839\n",
            "train loss:0.013873887511316109\n",
            "train loss:0.005288620234970387\n",
            "train loss:0.0005318761890239107\n",
            "train loss:0.10857993653457461\n",
            "train loss:0.0016725036801831083\n",
            "train loss:0.0006849245940224324\n",
            "train loss:0.0009345069330672598\n",
            "train loss:0.0003898665330156363\n",
            "train loss:0.0009245779073991159\n",
            "train loss:0.0022579832645591193\n",
            "train loss:0.002239004840967253\n",
            "train loss:0.004407168594749209\n",
            "train loss:0.00086794917968605\n",
            "train loss:0.0005115975413831172\n",
            "train loss:0.0016422836941688305\n",
            "train loss:9.100861340606365e-05\n",
            "train loss:9.80420646688251e-05\n",
            "train loss:4.6046748571340615e-05\n",
            "train loss:0.001175458005795992\n",
            "train loss:0.0005341111370678382\n",
            "train loss:0.0008584621568015214\n",
            "train loss:0.009683936597094983\n",
            "train loss:0.007109364127162182\n",
            "train loss:0.0004323692085003302\n",
            "train loss:0.004121666459434112\n",
            "train loss:0.0011672211188666836\n",
            "train loss:0.0026611482109771168\n",
            "train loss:0.00021409505917951432\n",
            "train loss:0.0010019901954691874\n",
            "train loss:0.00024421621625913626\n",
            "train loss:0.0002548972677535262\n",
            "train loss:0.0022267192306499135\n",
            "train loss:0.0005417197185250764\n",
            "train loss:0.00040235984619627915\n",
            "train loss:0.0005537412180958598\n",
            "train loss:0.0008008256500785329\n",
            "train loss:0.0010856645246076632\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.953\n",
            "Saved network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "filter_num = 50\n",
        "hidden_size = 300\n",
        "optimizer = 'Adam'\n",
        "optimizer_param = {'lr':0.01}\n",
        "weight_init_std = 0.05\n",
        " \n",
        "test acc:0.981\n",
        "'''\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
        "                        conv_param={'filter_num' : 50, 'filter_size' : 5, 'pad' : 0, 'stride' : 1},\n",
        "                        hidden_size=300, output_size=10, weight_init_std=0.05)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs,\n",
        "                  mini_batch_size=100, optimizer='Adam', optimizer_param={'lr' : 0.01},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved network Parameters!\")\n",
        "\n",
        "markers = {'train' : 'o', 'test' : 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
